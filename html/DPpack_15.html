<div class="container">

<table style="width: 100%;"><tr>
<td>LogisticRegressionDP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Privacy-preserving Logistic Regression</h2>

<h3>Description</h3>

<p>This class implements differentially private logistic regression
(Chaudhuri et al. 2011). Either the output or the objective
perturbation method can be used.
</p>


<h3>Details</h3>

<p>To use this class for logistic regression, first use the <code>new</code>
method to construct an object of this class with the desired function
values and hyperparameters. After constructing the object, the <code>fit</code>
method can be applied with a provided dataset and data bounds to fit the
model. In fitting, the model stores a vector of coefficients <code>coeff</code>
which satisfy differential privacy. These can be released directly, or used
in conjunction with the <code>predict</code> method to privately predict the
outcomes of new datapoints.
</p>
<p>Note that in order to guarantee differential privacy for logistic
regression, certain constraints must be satisfied for the values used to
construct the object, as well as for the data used to fit. These conditions
depend on the chosen perturbation method. The regularizer must be
1-strongly convex and differentiable. It also must be doubly differentiable
if objective perturbation is chosen. Additionally, it is assumed that if x
represents a single row of the dataset X, then the l2-norm of x is at most
1 for all x. In order to ensure this constraint is satisfied, the dataset
is preprocessed and scaled, and the resulting coefficients are
postprocessed and un-scaled so that the stored coefficients correspond to
the original data. Due to this constraint on x, it is best to avoid using a
bias term in the model whenever possible. If a bias term must be used, the
issue can be partially circumvented by adding a constant column to X before
fitting the model, which will be scaled along with the rest of X. The
<code>fit</code> method contains functionality to add a column of constant 1s to
X before scaling, if desired.
</p>


<h3>Super class</h3>

<p><code>DPpack::EmpiricalRiskMinimizationDP.CMS</code> -&gt; <code>LogisticRegressionDP</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LogisticRegressionDP-new"><code>LogisticRegressionDP$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LogisticRegressionDP-fit"><code>LogisticRegressionDP$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-LogisticRegressionDP-predict"><code>LogisticRegressionDP$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-LogisticRegressionDP-clone"><code>LogisticRegressionDP$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-LogisticRegressionDP-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new <code>LogisticRegressionDP</code> object.
</p>


<h5>Usage</h5>

<div class="r"><pre>LogisticRegressionDP$new(
  regularizer,
  eps,
  gamma,
  perturbation.method = "objective",
  regularizer.gr = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>regularizer</code></dt>
<dd>
<p>String or regularization function. If a string, must be
'l2', indicating to use l2 regularization. If a function, must have form
<code>regularizer(coeff)</code>, where <code>coeff</code> is a vector or matrix, and
return the value of the regularizer at <code>coeff</code>. See
<code>regularizer.l2</code> for an example. Additionally, in order to
ensure differential privacy, the function must be 1-strongly convex and
doubly differentiable.</p>
</dd>
<dt><code>eps</code></dt>
<dd>
<p>Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.</p>
</dd>
<dt><code>gamma</code></dt>
<dd>
<p>Nonnegative real number representing the regularization
constant.</p>
</dd>
<dt><code>perturbation.method</code></dt>
<dd>
<p>String indicating whether to use the 'output' or
the 'objective' perturbation methods (Chaudhuri et al. 2011).
Defaults to 'objective'.</p>
</dd>
<dt><code>regularizer.gr</code></dt>
<dd>
<p>Optional function representing the gradient of the
regularization function with respect to <code>coeff</code> and of the form
<code>regularizer.gr(coeff)</code>. Should return a vector. See
<code>regularizer.gr.l2</code> for an example. If <code>regularizer</code> is
given as a string, this value is ignored. If not given and
<code>regularizer</code> is a function, non-gradient based optimization methods
are used to compute the coefficient values in fitting the model.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>A new <code>LogisticRegressionDP</code> object.
</p>


<hr>
<a id="method-LogisticRegressionDP-fit"></a>



<h4>Method <code>fit()</code>
</h4>

<p>Fit the differentially private logistic regression model. This
method runs either the output perturbation or the objective perturbation
algorithm (Chaudhuri et al. 2011), depending on the value of
perturbation.method used to construct the object, to generate an
objective function. A numerical optimization method is then run to find
optimal coefficients for fitting the model given the training data and
hyperparameters. The built-in <code>optim</code> function using the
"BFGS" optimization method is used. If <code>regularizer</code> is given as
'l2' or if <code>regularizer.gr</code> is given in the construction of the
object, the gradient of the objective function is utilized by
<code>optim</code> as well. Otherwise, non-gradient based optimization methods
are used. The resulting privacy-preserving coefficients are stored in
<code>coeff</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LogisticRegressionDP$fit(X, y, upper.bounds, lower.bounds, add.bias = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt>
<dd>
<p>Dataframe of data to be fit.</p>
</dd>
<dt><code>y</code></dt>
<dd>
<p>Vector or matrix of true labels for each row of <code>X</code>.</p>
</dd>
<dt><code>upper.bounds</code></dt>
<dd>
<p>Numeric vector of length <code>ncol(X)</code> giving upper
bounds on the values in each column of X. The <code>ncol(X)</code> values are
assumed to be in the same order as the corresponding columns of <code>X</code>.
Any value in the columns of <code>X</code> larger than the corresponding upper
bound is clipped at the bound.</p>
</dd>
<dt><code>lower.bounds</code></dt>
<dd>
<p>Numeric vector of length <code>ncol(X)</code> giving lower
bounds on the values in each column of <code>X</code>. The <code>ncol(X)</code>
values are assumed to be in the same order as the corresponding columns
of <code>X</code>. Any value in the columns of <code>X</code> larger than the
corresponding upper bound is clipped at the bound.</p>
</dd>
<dt><code>add.bias</code></dt>
<dd>
<p>Boolean indicating whether to add a bias term to <code>X</code>.
Defaults to FALSE.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-LogisticRegressionDP-predict"></a>



<h4>Method <code>predict()</code>
</h4>

<p>Predict label(s) for given <code>X</code> using the fitted
coefficients.
</p>


<h5>Usage</h5>

<div class="r"><pre>LogisticRegressionDP$predict(X, add.bias = FALSE, raw.value = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt>
<dd>
<p>Dataframe of data on which to make predictions. Must be of same
form as <code>X</code> used to fit coefficients.</p>
</dd>
<dt><code>add.bias</code></dt>
<dd>
<p>Boolean indicating whether to add a bias term to <code>X</code>.
Defaults to FALSE. If add.bias was set to TRUE when fitting the
coefficients, add.bias should be set to TRUE for predictions.</p>
</dd>
<dt><code>raw.value</code></dt>
<dd>
<p>Boolean indicating whether to return the raw predicted
value or the rounded class label. If FALSE (default), outputs the
predicted labels 0 or 1. If TRUE, returns the raw score from the logistic
regression.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Matrix of predicted labels or scores corresponding to each row of
<code>X</code>.
</p>


<hr>
<a id="method-LogisticRegressionDP-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LogisticRegressionDP$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>References</h3>

<p>Chaudhuri K, Monteleoni C, Sarwate AD (2011).
“Differentially Private Empirical Risk Minimization.”
<em>Journal of Machine Learning Research</em>, <b>12</b>(29), 1069-1109.
<a href="https://jmlr.org/papers/v12/chaudhuri11a.html">https://jmlr.org/papers/v12/chaudhuri11a.html</a>.
</p>
<p>Chaudhuri K, Monteleoni C (2009).
“Privacy-preserving logistic regression.”
In Koller D, Schuurmans D, Bengio Y, Bottou L (eds.), <em>Advances in Neural Information Processing Systems</em>, volume 21.
<a href="https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf">https://proceedings.neurips.cc/paper/2008/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Build train dataset X and y, and test dataset Xtest and ytest
N &lt;- 200
K &lt;- 2
X &lt;- data.frame()
y &lt;- data.frame()
for (j in (1:K)){
  t &lt;- seq(-.25, .25, length.out = N)
  if (j==1) m &lt;- stats::rnorm(N,-.2, .1)
  if (j==2) m &lt;- stats::rnorm(N, .2, .1)
  Xtemp &lt;- data.frame(x1 = 3*t , x2 = m - t)
  ytemp &lt;- data.frame(matrix(j-1, N, 1))
  X &lt;- rbind(X, Xtemp)
  y &lt;- rbind(y, ytemp)
}
Xtest &lt;- X[seq(1,(N*K),10),]
ytest &lt;- y[seq(1,(N*K),10),,drop=FALSE]
X &lt;- X[-seq(1,(N*K),10),]
y &lt;- y[-seq(1,(N*K),10),,drop=FALSE]

# Construct object for logistic regression
regularizer &lt;- 'l2' # Alternatively, function(coeff) coeff%*%coeff/2
eps &lt;- 1
gamma &lt;- 1
lrdp &lt;- LogisticRegressionDP$new(regularizer, eps, gamma)

# Fit with data
# Bounds for X based on construction
upper.bounds &lt;- c( 1, 1)
lower.bounds &lt;- c(-1,-1)
lrdp$fit(X, y, upper.bounds, lower.bounds) # No bias term
lrdp$coeff # Gets private coefficients

# Predict new data points
predicted.y &lt;- lrdp$predict(Xtest)
n.errors &lt;- sum(predicted.y!=ytest)

</code></pre>


</div>