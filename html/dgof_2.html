<div class="container">

<table style="width: 100%;"><tr>
<td>ks.test</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kolmogorov-Smirnov Tests</h2>

<h3>Description</h3>

<p>Performs one or two sample Kolmogorov-Smirnov tests.
</p>


<h3>Usage</h3>

<pre><code class="language-R">






ks.test(x, y, ...,
        alternative = c("two.sided", "less", "greater"),
        exact = NULL, tol=1e-8, simulate.p.value=FALSE, B=2000)




</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric vector of data values.</p>
</td>
</tr></table>
<table><tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a numeric vector of data values, or a character string
naming a cumulative distribution function or an actual cumulative
distribution function such as <code>pnorm</code>. Alternatively, <code>y</code>
can be an <code>ecdf</code> function (or an object of class
<code>stepfun</code>) for specifying a discrete distribution.</p>
</td>
</tr></table>
<table>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>parameters of the distribution specified (as a character
string) by <code>y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alternative</code></td>
<td>
<p>indicates the alternative hypothesis and must be
one of <code>"two.sided"</code> (default), <code>"less"</code>, or
<code>"greater"</code>.  You can specify just the initial letter of the
value, but the argument name must be give in full.
See ‘Details’ for the meanings of the possible values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exact</code></td>
<td>
<p><code>NULL</code> or a logical indicating whether an exact
p-value should be computed.  See ‘Details’ for the meaning of
<code>NULL</code>.  Not used for the one-sided two-sample case.</p>
</td>
</tr>
</table>
<table>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>used as an upper bound for possible rounding error in values
(say, <code>a</code> and <code>b</code>) when needing to check for equality
(<code>a==b</code>); value of <code>NA</code> or <code>0</code> does exact comparisons
but risks making errors due to numerical imprecisions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>simulate.p.value</code></td>
<td>
<p>a logical indicating whether to compute
p-values by Monte Carlo simulation, for discrete goodness-of-fit
tests only.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>an integer specifying the number of replicates used in the
Monte Carlo test (for discrete goodness-of-fit tests only).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If <code>y</code> is numeric, a two-sample test of the null hypothesis
that <code>x</code> and <code>y</code> were drawn from the same <em>continuous</em>
distribution is performed.
</p>


<p>Alternatively, <code>y</code> can be a character string naming a continuous
(cumulative) distribution function (or such a function),
or an <code>ecdf</code> function
(or object of class <code>stepfun</code>) giving a discrete distribution.  In
these cases, a one-sample test is carried  out of the null that the
distribution function which generated <code>x</code> is distribution <code>y</code>
with parameters specified by <code>...</code>.
</p>
<p>The presence of ties generates a warning unless <code>y</code> describes a discrete
distribution (see above), since continuous distributions do not generate them.


</p>
<p>The possible values <code>"two.sided"</code>, <code>"less"</code> and
<code>"greater"</code> of <code>alternative</code> specify the null hypothesis
that the true distribution function of <code>x</code> is equal to, not less
than or not greater than the hypothesized distribution function
(one-sample case) or the distribution function of <code>y</code> (two-sample
case), respectively.  This is a comparison of cumulative distribution
functions, and the test statistic is the maximum difference in value,
with the statistic in the <code>"greater"</code> alternative being
<code class="reqn">D^+ = \max_u [ F_x(u) - F_y(u) ]</code>.
Thus in the two-sample case
<code>alternative="greater"</code> includes distributions for which <code>x</code>
is stochastically <em>smaller</em> than <code>y</code> (the CDF of <code>x</code> lies
above and hence to the left of that for <code>y</code>), in contrast to
<code>t.test</code> or <code>wilcox.test</code>.
</p>


<p>Exact p-values are not available for the one-sided two-sample case,
or in the case of ties if <code>y</code> is continuous.  If <code>exact = NULL</code>
(the default), an exact p-value is computed if the sample size is less
than 100 in the one-sample case with <code>y</code> continuous or if the sample
size is less than or equal to 30 with <code>y</code>
discrete; or if the product of the
sample sizes is less than 10000 in the two-sample case for continuous
<code>y</code>.  Otherwise,
asymptotic distributions are used whose approximations may be inaccurate
in small samples.  With <code>y</code> continuous,
the one-sample two-sided case, exact p-values are
obtained as described in Marsaglia, Tsang &amp; Wang (2003); the formula of
Birnbaum &amp; Tingey (1951) is used for the one-sample one-sided case.
</p>
<p>In the one-sample case with <code>y</code> discrete, the methods presented in
Conover (1972) and Gleser (1985) are used when <code>exact=TRUE</code> (or when
<code>exact=NULL</code>) and <code>length(x)&lt;=30</code> as described above.
When <code>exact=FALSE</code> or <code>exact=NULL</code> with
<code>length(x)&gt;30</code>, the test is not exact and the resulting p-values
are known to be conservative.  Usage of <code>exact=TRUE</code> with
sample sizes greater than 30 is not advised due to numerical instabilities;
in such cases, simulated p-values may be desirable.
</p>
<p>If a single-sample test is used with <code>y</code> continuous,
the parameters specified in
<code>...</code> must be pre-specified and not estimated from the data.
There is some more refined distribution theory for the KS test with
estimated parameters (see Durbin, 1973), but that is not implemented
in <code>ks.test</code>.


</p>


<h3>Value</h3>

<p>A list with class <code>"htest"</code> containing the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alternative</code></td>
<td>
<p>a character string describing the alternative
hypothesis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a character string indicating what type of test was
performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data.name</code></td>
<td>
<p>a character string giving the name(s) of the data.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Modified by Taylor B. Arnold and John W. Emerson to include
one-sample testing with a discrete distribution (as presented
in Conover's 1972 paper – see references).
</p>


<h3>References</h3>

<p>Z. W. Birnbaum and Fred H. Tingey (1951),
One-sided confidence contours for probability distribution functions.
<em>The Annals of Mathematical Statistics</em>, <b>22</b>/4, 592–596.
</p>
<p>William J. Conover (1971),
<em>Practical Nonparametric Statistics</em>.
New York: John Wiley &amp; Sons.
Pages 295–301 (one-sample Kolmogorov test),
309–314 (two-sample Smirnov test).
</p>


<p>William J. Conover (1972),
A Kolmogorov Goodness-of-Fit Test for Discontinuous Distributions.
<em>Journal of American Statistical Association</em>, Vol. 67, No. 339, 591–596.
</p>
<p>Leon Jay Gleser (1985),
Exact Power of Goodness-of-Fit Tests of Kolmogorov Type for Discontinuous
Distributions.
<em>Journal of American Statistical Association</em>, Vol. 80, No. 392,  954–958.


</p>
<p>Durbin, J. (1973)
<em>Distribution theory for tests based on the sample distribution
function</em>.  SIAM.
</p>
<p>George Marsaglia, Wai Wan Tsang and Jingbo Wang (2003),
Evaluating Kolmogorov's distribution.
<em>Journal of Statistical Software</em>, <b>8</b>/18.
<a href="https://www.jstatsoft.org/v08/i18/">https://www.jstatsoft.org/v08/i18/</a>.
</p>


<h3>See Also</h3>

<p><code>shapiro.test</code> which performs the Shapiro-Wilk test for
normality; <code>cvm.test</code> for Cramer-von Mises type tests.
</p>


<h3>Examples</h3>

<pre><code class="language-R">require(graphics)
require(dgof)

set.seed(1)

x &lt;- rnorm(50)
y &lt;- runif(30)
# Do x and y come from the same distribution?
ks.test(x, y)
# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")

# test if x is stochastically larger than x2
x2 &lt;- rnorm(50, -1)
plot(ecdf(x), xlim=range(c(x, x2)))
plot(ecdf(x2), add=TRUE, lty="dashed")
t.test(x, x2, alternative="g")
wilcox.test(x, x2, alternative="g")
ks.test(x, x2, alternative="l")

#########################################################
# TBA, JWE new examples added for discrete distributions:

x3 &lt;- sample(1:10, 25, replace=TRUE)

# Using ecdf() to specify a discrete distribution:
ks.test(x3, ecdf(1:10))

# Using step() to specify the same discrete distribution:
myfun &lt;- stepfun(1:10, cumsum(c(0, rep(0.1, 10))))
ks.test(x3, myfun)

# The previous R ks.test() does not correctly calculate the
# test statistic for discrete distributions (gives warning):
# stats::ks.test(c(0, 1), ecdf(c(0, 1)))
# ks.test(c(0, 1), ecdf(c(0, 1)))

# Even when the correct test statistic is given, the
# previous R ks.test() gives conservative p-values:
stats::ks.test(rep(1, 3), ecdf(1:3))
ks.test(rep(1, 3), ecdf(1:3))
ks.test(rep(1, 3), ecdf(1:3), simulate=TRUE, B=10000)

</code></pre>


</div>