<div class="container">

<table style="width: 100%;"><tr>
<td>ddst-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Data Driven Smooth Tests
</h2>

<h3>Description</h3>

<p>Set of Data Driven Smooth Tests for Goodness of Fit
</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Package: </td>
<td style="text-align: left;"> ddst</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
Version: </td>
<td style="text-align: left;"> 1.3</td>
</tr>
<tr>
<td style="text-align: left;">
Date: </td>
<td style="text-align: left;"> 2008-07-01</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<h3>General Description</h3>

<p>Smooth test was introduced by Neyman (1937) to verify simple null hypothesis asserting that observations obey completely known continuous distribution function <em>F</em>. Smooth test statistic (with <em>k</em> components) can be interpreted as score statistic in an appropriate class of auxiliary models indexed by a vector of parameters <em>$theta in R^k, k &gt;= 1$.</em> 
</p>
<p>Pertaining auxilary null hypothesis asserts <em>$theta=theta_0=0$</em>. Therefore, in this case, the smooth test statistic based on <em>n</em> i.i.d. observations <em>$Z_1,...,Z_n$</em> has the form
<em>$W_k=[1/sqrt(n) sum_i=1^n l(Z_i)]I^-1[1/sqrt(n) sum_i=1^n l(Z_i)]'$</em>,
</p>
<p>where <em>$l(Z_i)$</em>, i=1,...,n, is <em>k</em>-dimensional (row) score vector, the symbol <em>'</em> denotes transposition while <em>$I=Cov_theta_0[l(Z_1)]'[l(Z_1)]$</em>. Following Neyman's idea of modelling underlying distributions one gets <em>$l(Z_i)=(phi_1(F(Z_i)),...,phi_k(F(Z_i)))$</em> and <em>I</em> being the identity matrix, where <em>$phi_j$</em>'s, j &gt;= 1,  are zero mean orthonormal functions on [0,1], while <em>F</em> is the completely specified null distribution function.
</p>
<p>In case of composite null hypothesis there is also unspecified vector of nuisance parameters <em>$gamma$</em> defining the distribution of observations. Smooth statistic (with <em>k</em> components) in such applications is understood as efficient score statistic for some class of models indexed by an auxiliary parmeter <em>$theta in R^k$</em>, k &gt;= 1. Pertaining efficient score vector <em>$l^*(Z_i;gamma)$</em> is defined as the residual from projection the score vector for <em>$theta$</em> onto the space spanned by score vector for <em>$gamma$</em>. As such, smooth test is alternative name for <em>$C(alpha)$</em> Neyman's test. See Neyman (1959), Buhler and Puri (1966) as well as Javitz (1975) for details. Hence, smooth test, based on <em>n</em> i.i.d. variables <em>$Z_1,...,Z_n$</em> rejects hypothesis <em>$theta=theta_0=0$</em> for large values of 
</p>
<p><em>$W_k^*(tilde gamma)=[1/sqrt(n) sum_i=1^n l^*(Z_i;tilde gamma)][I^*(tilde gamma)]^-1[1/sqrt(n) sum_i=1^n l^*(Z_i;tilde gamma)]'$</em>,
where <em>$tilde gamma$</em> is an appropriate estimator of <em>$gamma$</em> while <em>$I^*(gamma)=Cov_theta_0[l^*(Z_1;gamma)]'[l^*(Z_1;gamma)]$</em>. More details can be found in Janic and Ledwina (2008), Kallenberg and Ledwina (1997 a,b) as well as Inglot and Ledwina (2006 a,b). 
</p>
<p>Auxiliary models, mentioned above, aim to mimic the unknown underlying model for the data at hand. To choose the dimension <em>k</em> of the auxilary model we apply some model selection criteria. Among several solutions already considered, we decided to implement two following ones, pertaining to the two above described problems and resulting <em>$W_k$</em> and <em>$W_k^*(tilde gamma)$</em>. The selection rules in the two cases are briefly denoted by <em>T</em> and <em>$T^*$</em>, respectively, and given by
</p>
<p><em>$T = min1 &lt;= k &lt;= d: W_k-pi(k,n,c) &gt;= W_j-pi(j,n,c), j=1,...,d$</em>
</p>
<p>and
</p>
<p><em>
$T^* = min1 &lt;= k &lt;= d: W_k^*(tilde gamma)-pi^*(k,n,c) &gt;= W_j^*(tilde gamma)-pi^*(j,n,c), j=1,...,d$</em>.
</p>
<p>Both criteria are based on approximations of penalized loglikelihoods, where loglikelihoods are replaced by <em>$W_k$</em> and <em>$W_k^*(tilde gamma)$</em>, respectively.  The penalties for the dimension <em>j</em> in case of simple and composite null hypothesis are defined as follows
</p>
<p><em>$pi(j,n,c)=jlog n,  if  max1 &lt;= k &lt;= d|Y_k| &lt;= sqrt(c log(n)), 2j,  if max1 &lt;= k &lt;= d|Y_k|&gt;sqrt(c log(n)). $</em>
</p>
<p>and
</p>
<p><em>
$pi^*(j,n,c)=jlog n,  if max1 &lt;= k &lt;= d|Y_k^*| &lt;= sqrt(c log(n)),2j  if max(1 &lt;= k &lt;= d)|Y_k^*| &gt; sqrt(c log(n))$</em>.
</p>
<p>respectively, where <em>c</em> is some calibrating constant, <em>d</em> is maximal dimension taken into account, 
</p>
<p><em>$(Y_1,...,Y_k)=[1/sqrt(n) sum_i=1^n l(Z_i)]I^-1/2$</em> 
</p>
<p>while  
</p>
<p><em>$(Y_1^*,...,Y_k^*)=[1/sqrt(n) sum_i=1^n l^*(Z_i; tilde gamma)][I^*(tilde gamma)]^-1/2$</em>.
</p>
<p>In consequence, data driven smooth tests for the simple and composite null hypothesis reject for large values of <em>$W_T$</em>
and <em>$W_T^* = W_T^*(tilde gamma)$</em>, respectively. For details see Inglot and Ledwina (2006 a,b,c). 
</p>
<p>The choice of <em>c</em> in <em>T</em> and <em>$T^*$</em> is decisive to finite sample behaviour of the selection rules and pertaining statistics <em>$W_T$</em> and <em>$W_T^*(tilde gamma)$</em>. In particular, under large <em>c</em>'s the rules behave similarly as Schwarz's (1978) BIC while for <em>c=0</em> they mimic Akaike's (1973) AIC. For moderate sample sizes, values <em>c in (2,2.5)</em> guarantee, under ‘smooth’ departures, only slightly smaller power as in case BIC were used and simultaneously give much higher power than BIC under multimodal alternatives. In genral, large <em>c's</em> are recommended if changes in location, scale, skewness and kurtosis are in principle aimed to be detected. For evidence and discussion see Inglot and Ledwina (2006 c). 
</p>
<p>It <em>c&gt;0</em> then the limiting null distribution of <em>$W_T$</em> and <em>$W_T^*(tilde gamma)$</em> is central chi-squared with one degree of freedom. In our implementation, for given <em>n</em>, both critical values and <em>p</em>-values are computed by MC method.
</p>
<p>Empirical distributions of <em>T</em> and <em>$T^*$</em> as well as <em>$W_T$</em> and <em>$W_T^*(tilde gamma)$</em> are not essentially influenced by the choice of reasonably large <em>d</em>'s, provided that sample size is at least moderate.
</p>
<p>For more details see: <a href="http://www.biecek.pl/R/ddst/description.pdf">http://www.biecek.pl/R/ddst/description.pdf</a>.
</p>


<h3>Author(s)</h3>

<p>Przemyslaw Biecek and Teresa Ledwina
</p>
<p>Maintainer: You should complain to Przemyslaw Biecek  &lt;przemyslaw.biecek@gmail.com&gt;
</p>


<h3>References</h3>

<p>Akaike, H. (1973). Information theory and the maximum likelihood principle. In: <em> 2nd International Symposium on Information Theory</em>, (eds. B. N. Petrov and F. Csaki), 267-281. Akademiai Kiado, Budapest.
</p>
<p>Buhler, W.J., Puri, P.S. (1966). On optimal asymptotic tests of composite hypotheses with several constraints. <em> Z. Wahrsch. verw. Geb.</em> <b> 5</b>, 71–88.
</p>
<p>Inglot, T., Ledwina, T. (2006 a). Data-driven score tests for homoscedastic linear regression model: asymptotic results. <em> Probab. Math. Statist.</em> <b> 26</b>, 41–61.
</p>
<p>Inglot, T., Ledwina, T. (2006 b). Data-driven score tests for homoscedastic linear regression model: the construction and simulations. In <em> Prague Stochastics 2006. Proceedings</em>, (eds. M. Huskova, M. Janzura), 124–137. Matfyzpress, Prague.
</p>
<p>Inglot, T., Ledwina, T. (2006 c). Towards data driven selection of a penalty function for data driven Neyman tests. <em> Linear Algebra and its Appl.</em> <b> 417</b>, 579–590. 
</p>
<p>Javitz, H.S. (1975). Generalized smooth tests of goodness of fit, independence and equality of distributions. Ph.D. thesis at University of California, Berkeley.
</p>
<p>Janic, A. and Ledwina, T. (2008). Data-driven tests for a location-scale family revisited. <em> J. Statist. Theory. Pract. Special issue on Modern Goodness of Fit Methods. accepted.</em>.
</p>
<p>Kallenberg, W.C.M., Ledwina, T. (1997 a). Data driven smooth tests for composite hypotheses: Comparison of powers. <em> J. Statist. Comput. Simul.</em> <b> 59</b>, 101–121.
</p>
<p>Kallenberg, W.C.M.,  Ledwina, T. (1997 b). Data driven smooth tests when the hypothesis is composite. <em> J. Amer. Statist. Assoc.</em> <b> 92</b>, 1094–1104.
</p>
<p>Neyman, J. (1937). ‘Smooth test’ for goodness of fit. <em> Skand. Aktuarietidskr.</em> <b> 20</b>, 149-199.
</p>
<p>Neyman, J. (1959). Optimal asymptotic tests of composite statistical hypotheses. In <em> Probability and Statistics</em>, (ed. U. Grenander), Harald Cramer Volume, 212–234. Wiley, New York.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Data Driven Smooth Test for Uniformity
#
# H0 is true
z = runif(80)
ddst.uniform.test(z, compute.p=TRUE)

# H0 is false
z = rbeta(80,4,2)
(t = ddst.uniform.test(z, compute.p=TRUE))
t$p.value

# Data Driven Smooth Test for Normality
#
# H0 is true
z = rnorm(80)
ddst.norm.test(z, compute.p=TRUE)

# H0 is false
z = rexp(80,4)
ddst.norm.test(z, B=5000, compute.p=TRUE)

# Data Driven Smooth Test for Extreme Value Distribution
#
# H0 is true
#library(evd)
#z = -qgumbel(runif(100),-1,1)
#ddst.extr.test (z, compute.p = TRUE)

# H0 is false
z = rexp(80,4)
ddst.extr.test (z, compute.p = TRUE)

# Data Driven Smooth Test for Exponentiality
#
# H0 is true
z = rexp(80,4)
ddst.exp.test (z, compute.p = TRUE)

# H0 is false
z = rchisq(80,4)
ddst.exp.test (z, compute.p = TRUE)

</code></pre>


</div>