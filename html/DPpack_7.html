<div class="container">

<table style="width: 100%;"><tr>
<td>GaussianMechanism</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Gaussian Mechanism</h2>

<h3>Description</h3>

<p>This function implements the Gaussian mechanism for differential privacy by
adding noise to the true value(s) of a function according to specified values
of epsilon, delta, and l2-global sensitivity(-ies). Global sensitivity
calculated based either on bounded or unbounded differential privacy can be
used (Kifer and Machanavajjhala 2011). If true.values is a vector, the provided
epsilon and delta are divided such that (epsilon, delta)-level differential
privacy is satisfied across all function values. In the case that each
element of true.values comes from its own function with different
corresponding sensitivities, a vector of sensitivities may be provided. In
this case, if desired, the user can specify how to divide epsilon and delta
among the function values using alloc.proportions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">GaussianMechanism(
  true.values,
  eps,
  delta,
  sensitivities,
  type.DP = "aDP",
  alloc.proportions = NULL,
  analytic = FALSE,
  tol = 1e-12
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>true.values</code></td>
<td>
<p>Real number or numeric vector corresponding to the true
value(s) of the desired function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Positive real number defining the epsilon privacy parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>delta</code></td>
<td>
<p>Positive real number defining the delta privacy parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sensitivities</code></td>
<td>
<p>Real number or numeric vector corresponding to the
l2-global sensitivity(-ies) of the function(s) generating true.values. This
value must be of length 1 or of the same length as true.values. If it is of
length 1 and true.values is a vector, this indicates that the given
sensitivity applies simultaneously to all elements of true.values and that
the privacy budget need not be allocated (alloc.proportions is unused in
this case). If it is of the same length as true.values, this indicates that
each element of true.values comes from its own function with different
corresponding sensitivities. In this case, the l2-norm of the provided
sensitivities is used to generate the Gaussian noise.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type.DP</code></td>
<td>
<p>String indicating the type of differential privacy desired for
the Gaussian mechanism. Can be either 'pDP' for probabilistic DP
(Liu 2019) or 'aDP' for approximate DP
(Dwork et al. 2006). Note that if 'aDP' is chosen, epsilon must
be strictly less than 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alloc.proportions</code></td>
<td>
<p>Optional numeric vector giving the allocation
proportions of epsilon and delta to the function values in the case of
vector-valued sensitivities. For example, if sensitivities is of length two
and alloc.proportions = c(.75, .25), then 75% of the privacy budget eps
(and 75% of delta) is allocated to the noise computation for the first
element of true.values, and the remaining 25% is allocated to the noise
computation for the second element of true.values. This ensures (eps,
delta)-level privacy across all computations. Input does not need to be
normalized, meaning alloc.proportions = c(3,1) produces the same result as
the example above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>analytic</code></td>
<td>
<p>Indicates whether to use the analytic Gaussian mechanism to
compute the noise scale (Balle and Wang 2018). Defaults to FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Error tolerance for binary search used in determining the noise
parameter for the analytic Gaussian mechanism. Unused if analytic is FALSE.
Defaults to 1e-12.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Sanitized function values based on the bounded and/or unbounded
definitions of differential privacy, sanitized via the Gaussian mechanism.
</p>


<h3>References</h3>

<p>Dwork C, McSherry F, Nissim K, Smith A (2006).
“Calibrating Noise to Sensitivity in Private Data Analysis.”
In Halevi S, Rabin T (eds.), <em>Theory of Cryptography</em>, 265–284.
ISBN 978-3-540-32732-5, <a href="https://doi.org/10.1007/11681878_14">https://doi.org/10.1007/11681878_14</a>.
</p>
<p>Kifer D, Machanavajjhala A (2011).
“No Free Lunch in Data Privacy.”
In <em>Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data</em>,  SIGMOD '11, 193–204.
ISBN 9781450306614, <a href="https://doi.org/10.1145/1989323.1989345">doi:10.1145/1989323.1989345</a>.
</p>
<p>Balle B, Wang Y (2018).
“Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising.”
In Dy J, Krause A (eds.), <em>Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of <em>Proceedings of Machine Learning Research</em>, 394–403.
<a href="https://proceedings.mlr.press/v80/balle18a.html">https://proceedings.mlr.press/v80/balle18a.html</a>.
</p>
<p>Liu F (2019).
“Generalized Gaussian Mechanism for Differential Privacy.”
<em>IEEE Transactions on Knowledge and Data Engineering</em>, <b>31</b>(4), 747-756.
<a href="https://doi.org/10.1109/TKDE.2018.2845388">https://doi.org/10.1109/TKDE.2018.2845388</a>.
</p>
<p>Dwork C, Kenthapadi K, McSherry F, Mironov I, Naor M (2006).
“Our Data, Ourselves: Privacy Via Distributed Noise Generation.”
In Vaudenay S (ed.), <em>Advances in Cryptology - EUROCRYPT 2006</em>, 486–503.
ISBN 978-3-540-34547-3, <a href="https://doi.org/10.1007/11761679_29">doi:10.1007/11761679_29</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Simulate dataset
n &lt;- 100
c0 &lt;- 5 # Lower bound
c1 &lt;- 10 # Upper bound
D1 &lt;- stats::runif(n, c0, c1)

# Privacy budget
epsilon &lt;- 0.9 # eps must be in (0, 1) for approximate differential privacy
delta &lt;- 0.01
sensitivity &lt;- (c1-c0)/n

# Approximate differential privacy
private.mean.approx &lt;- GaussianMechanism(mean(D1), epsilon, delta,
                                         sensitivity)
private.mean.approx

# Probabilistic differential privacy
private.mean.prob &lt;- GaussianMechanism(mean(D1), epsilon, delta, sensitivity,
                                       type.DP = 'pDP')
private.mean.prob

# Analytic Gaussian mechanism
epsilon &lt;- 1.1 # epsilon can be &gt; 1 for analytic Gaussian mechanism
private.mean.analytic &lt;- GaussianMechanism(mean(D1), epsilon, delta,
                                           sensitivity, analytic=TRUE)
private.mean.analytic

# Simulate second dataset
d0 &lt;- 3 # Lower bound
d1 &lt;- 6 # Upper bound
D2 &lt;- stats::runif(n, d0, d1)
D &lt;- matrix(c(D1,D2),ncol=2)
sensitivities &lt;- c((c1-c0)/n, (d1-d0)/n)
epsilon &lt;- 0.9 # Total privacy budget for all means
delta &lt;- 0.01

# Here, sensitivities are summed and the result is used to generate Laplace
# noise. This is essentially the same as allocating epsilon proportional to
# the corresponding sensitivity. The results satisfy (0.9,0.01)-approximate
# differential privacy.
private.means &lt;- GaussianMechanism(apply(D, 2, mean), epsilon, delta,
                                   sensitivities)
private.means

# Here, privacy budget is explicitly split so that 75% is given to the first
# vector element and 25% is given to the second.
private.means &lt;- GaussianMechanism(apply(D, 2, mean), epsilon, delta,
                                   sensitivities,
                                   alloc.proportions = c(0.75, 0.25))
private.means

</code></pre>


</div>