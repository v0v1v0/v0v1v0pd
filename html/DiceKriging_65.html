<div class="container">

<table style="width: 100%;"><tr>
<td>leaveOneOut.km</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Leave-one-out for a km object </h2>

<h3>Description</h3>

<p>Cross validation by leave-one-out for a <code>km</code> object without noisy observations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">leaveOneOut.km(model, type, trend.reestim=FALSE) 
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p> an object of class "km" without noisy observations.</p>
</td>
</tr></table>
<table>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p> a character string corresponding to the kriging family, to be chosen between simple kriging ("SK"), or universal kriging ("UK").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trend.reestim</code></td>
<td>
<p> should the trend be reestimated when removing an observation? Default to FALSE.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Leave-one-out (LOO) consists of computing the prediction at a design point when the corresponding observation is removed from the learning set (and this, for all design points). A quick version of LOO based on Dubrule formula is also implemented; It is limited to 2 cases: <code>type=="SK" &amp; (!trend.reestim)</code> and <code>type=="UK" &amp; trend.reestim</code>. Leave-one-out is not implemented yet for noisy observations.</p>


<h3>Value</h3>

<p> A list composed of
</p>
<table>
<tr style="vertical-align: top;">
<td><code>mean </code></td>
<td>
<p> a vector of length <em>n</em>. The ith coordinate is equal to the kriging mean (including the trend) at the ith observation number when removing it from the learning set, </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sd </code></td>
<td>
<p> a vector of length <em>n</em>. The ith coordinate is equal to the kriging standard deviation at the ith observation number when removing it from the learning set,</p>
</td>
</tr>
</table>
<p>where <em>n</em> is the total number of observations.
</p>


<h3>Warning</h3>

<p>Kriging parameters are not re-estimated when removing one observation. With few points, the re-estimated values can be far from those obtained with the entire learning set. One option is to reestimate the trend coefficients, by setting <code>trend.reestim=TRUE</code>.
</p>


<h3>Author(s)</h3>

<p> O. Roustant, D. Ginsbourger, Ecole des Mines de St-Etienne. </p>


<h3>References</h3>

 
<p>F. Bachoc (2013), Cross Validation and Maximum Likelihood estimations of hyper-parameters of Gaussian processes with model misspecification. <em>Computational Statistics and Data Analysis</em>, <b>66</b>, 55-69. <a href="http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html">http://www.lpma.math.upmc.fr/pageperso/bachoc/publications.html</a>
</p>
<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>, Wiley series in probability and mathematical statistics.
</p>
<p>O. Dubrule (1983), Cross validation of Kriging in a unique neighborhood. <em>Mathematical Geology</em>, <b>15</b>, 687-699.
</p>
<p>J.D. Martin and T.W. Simpson (2005), Use of kriging models to approximate deterministic computer models, <em>AIAA Journal</em>, <b>43</b> no. 4, 853-863.
</p>
<p>M. Schonlau (1997), <em>Computer experiments and global optimization</em>, Ph.D. thesis, University of Waterloo.	
</p>


<h3>See Also</h3>

 <p><code>predict,km-method</code>,  <code>plot,km-method</code>,
<code>cv</code></p>


</div>