<div class="container">

<table style="width: 100%;"><tr>
<td>deepgp-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Package deepgp</h2>

<h3>Description</h3>

<p>Performs Bayesian posterior inference for deep Gaussian 
processes following Sauer, Gramacy, and Higdon (2023).  
See Sauer (2023) for comprehensive 
methodological details and <a href="https://bitbucket.org/gramacylab/deepgp-ex/">https://bitbucket.org/gramacylab/deepgp-ex/</a> for 
a variety of coding examples. Models are trained through MCMC including 
elliptical slice sampling of latent Gaussian layers and Metropolis-Hastings 
sampling of kernel hyperparameters.  Vecchia-approximation for faster 
computation is implemented following Sauer, Cooper, and Gramacy 
(2023).  Optional monotonic warpings are implemented following 
Barnett et al. (2024).  Downstream tasks include sequential design 
through active learning Cohn/integrated mean squared error (ALC/IMSE; Sauer, 
Gramacy, and Higdon, 2023), optimization through expected improvement 
(EI; Gramacy, Sauer, and Wycoff, 2022), and contour 
location through entropy (Booth, Renganathan, and Gramacy, 
2024).  Models extend up to three layers deep; a one 
layer model is equivalent to typical Gaussian process regression.  
Incorporates OpenMP and SNOW parallelization and utilizes C/C++ under 
the hood.
</p>


<h3>Important Functions</h3>


<ul>
<li> <p><code>fit_one_layer</code>: conducts MCMC sampling of 
hyperparameters for a one layer GP
</p>
</li>
<li> <p><code>fit_two_layer</code>: conducts MCMC sampling of 
hyperparameters and hidden layer for a two layer deep GP
</p>
</li>
<li> <p><code>fit_three_layer</code>: conducts MCMC sampling of 
hyperparameters and hidden layers for a three layer deep GP
</p>
</li>
<li> <p><code>continue</code>: collects additional MCMC samples
</p>
</li>
<li> <p><code>trim</code>: cuts off burn-in and optionally thins 
samples
</p>
</li>
<li> <p><code>predict</code>: calculates posterior mean and 
variance over a set of input locations (optionally calculates EI or entropy)
</p>
</li>
<li> <p><code>plot</code>: produces trace plots, hidden layer 
plots, and posterior predictive plots
</p>
</li>
<li> <p><code>ALC</code>: calculates active learning Cohn over 
set of input locations using reference grid
</p>
</li>
<li> <p><code>IMSE</code>: calculates integrated mean-squared error
over set of input locations
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Annie S. Booth <a href="mailto:annie_booth@ncsu.edu">annie_booth@ncsu.edu</a>
</p>


<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<a href="http://hdl.handle.net/10919/114845">http://hdl.handle.net/10919/114845</a>
<br><br>
Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br><br>
Sauer, A., Cooper, A., &amp; Gramacy, R. B. (2023). Vecchia-approximated deep Gaussian 
processes for computer experiments. 
*Journal of Computational and Graphical Statistics, 32*(3), 824-837.  arXiv:2204.02904
<br><br>
Gramacy, R. B., Sauer, A. &amp; Wycoff, N. (2022). Triangulation candidates for Bayesian 
optimization.  *Advances in Neural Information Processing Systems (NeurIPS), 35,* 
35933-35945.  arXiv:2112.07457
<br><br>
Booth, A., Renganathan, S. A. &amp; Gramacy, R. B. (2024). Contour location for 
reliability in airfoil simulation experiments using deep Gaussian 
processes. *In Review.* arXiv:2308.04420
</p>
<p>Barnett, S., Beesley, L. J., Booth, A. S., Gramacy, R. B., &amp; Osthus D. (2024). 
Monotonic warpings for additive and deep Gaussian processes. *In Review.* arXiv:2408.01540
</p>


<h3>Examples</h3>

<pre><code class="language-R"># See vignette, ?fit_one_layer, ?fit_two_layer, ?fit_three_layer, 
# ?ALC, or ?IMSE for examples
# Many more examples including real-world computer experiments are available at: 
# https://bitbucket.org/gramacylab/deepgp-ex/

</code></pre>


</div>