<div class="container">

<table style="width: 100%;"><tr>
<td>logistic.dma</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Dynamic model averaging for binary outcomes
</h2>

<h3>Description</h3>

<p>Implements dynamic model averaging for continuous outcomes as described in 
McCormick et al. (2011, Biometrics).  It can be either performed for all data at once (using <code>logistic.dma</code>), or dynamically for one observation at a time (combining the remaining functions, see Example).
Along with the values described below, plot() creates a plot of the posterior model probabilities over time and 
model-averaged fitted values (with smooth curve overlay) and print() returns model matrix and posterior
model probabilities.  There are K candidate
models, T time points, and d total covariates (including the intercept).
</p>


<h3>Usage</h3>

<pre><code class="language-R">logistic.dma(x, y, models.which, lambda = 0.99, alpha = 0.99,autotune = TRUE, 
    initmodelprobs = NULL, initialsamp = NULL)
 
logdma.init(x, y, models.which)

logdma.predict(fit, newx)

logdma.update(fit, newx, newy, lambda = 0.99, autotune = TRUE)

logdma.average(fit, alpha = 0.99, initmodelprobs = NULL)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>T by (d-1) matrix of observed covariates.  Note that a column of 1's is added
automatically for the intercept. In <code>logdma.init</code>, this matrix contains only the training set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>T vector of binary responses. In <code>logdma.init</code>, these correspond to the training set only.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>models.which</code></td>
<td>
<p>K by (d-1) matrix defining models.  A 1 indicates a covariate is included
in a particular model, a 0 if it is excluded.  Model averaging is done over all
modeld specified in models.which.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>scalar forgetting factor with each model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>scalar forgetting factor for model averaging</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>autotune</code></td>
<td>
<p> T/F indicates whether or not the automatic tuning procedure desribed in 
McCormick et al. should be applied.  Default is true.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initmodelprobs</code></td>
<td>
<p>K vector of starting probabilities for model averaging.  If null (default),
then use 1/K for each model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initialsamp</code></td>
<td>
<p>scalar indicating how many observations to use for generating initial 
values.  If null (default), then use the first 10 percent of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newx, newy</code></td>
<td>
<p>Subset of <code>x</code> and <code>y</code> corresponding to new observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fit</code></td>
<td>
<p>List with estimation results that are outputs of functions <code>logdma.init</code>, <code>logdma.update</code> and <code>logdma.average</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function <code>logistic.dma</code> is composed of three parts, which can be also used separately: First, the model is trained with a subset of the data  (function <code>logdma.init</code>), where the size of the training set is determined by <code>initialsamp</code>. Note that arguments <code>x</code> and <code>y</code> in <code>logdma.init</code> should contain the training subset only. Then, the estimation is updated with new observations (function <code>logdma.update</code>). Lastly, a dynamic model averaging is performed on the final estimates (function <code>logdma.average</code>). The updating, averaging and in addition predicting (<code>logdma.predict</code>) can be performed dynamically for one observation at a time, see Example below.
</p>


<h3>Value</h3>

<p>Functions <code>logistic.dma</code> and <code>logdma.average</code> return an object of class <code>logistic.dma</code>. Functions <code>logdma.init</code> and <code>logdma.update</code> return a list with estimation results which is a subset of the <code>logistic.dma</code> object. It has the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>T by (d-1) matrix of covariates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>T by 1 vector of binary responses</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>models.which</code></td>
<td>
<p>K by (d-1) matrix of candidate models</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>scalar, tuning factor within models</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>scalar, tuning factor for model averaging</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>autotune</code></td>
<td>
<p>T/F, indicator of whether or not to use autotuning algorithm</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha.used</code></td>
<td>
<p>T vector of alpha values used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>K by T by d array of dynamic logistic regression estimates for each model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vartheta</code></td>
<td>
<p>K by T by d array of dynamic logistic regression variances for each model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmp</code></td>
<td>
<p>K by T array of posterior model probabilities</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhatdma</code></td>
<td>
<p>T vector of model-averaged predictions</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>yhatmodel</code></td>
<td>
<p>K by T vector of fitted values for each model</p>
</td>
</tr>
</table>
<p>Function <code>logdma.predict</code> returns a matrix with predictions corresponding to the <code>newx</code> covariates.
</p>


<h3>Author(s)</h3>

<p>Tyler H. McCormick, David Madigan, Adrian Raftery
</p>
<p>Sevvandi Kandanaarachchi and Hana Sevcikova implemented the "streaming" functionality, i.e. the original function was decomposed into standalone  parts that can be used separately for one observation at a time.
</p>


<h3>References</h3>

<p>McCormick, T.M., Raftery, A.E., Madigan, D. and Burd, R.S. (2011) "Dynamic Logistic Regression and 
Dynamic Model Averaging for Binary Classification." Biometrics, 66:1162-1173.</p>


<h3>Examples</h3>

<pre><code class="language-R"># simulate some data to test
# first, static coefficients
coef &lt;- c(.08,-.4,-.1)
coefmat &lt;- cbind(rep(coef[1],200),rep(coef[2],200),rep(coef[3],200))
# then, dynamic ones
coefmat &lt;- cbind(coefmat,seq(1,.45,length.out=nrow(coefmat)),
            seq(-.75,-.15,length.out=nrow(coefmat)),
            c(rep(-1.5,nrow(coefmat)/2),rep(-.5,nrow(coefmat)/2)))
npar &lt;- ncol(coefmat)-1

# simulate data
set.seed(1234)
dat &lt;- matrix(rnorm(200*(npar),0,1),200,(npar))
ydat &lt;- exp(rowSums((cbind(rep(1,nrow(dat)),dat))[1:100,]*coefmat[1:100,]))/
          (1+exp(rowSums(cbind(rep(1,nrow(dat)),dat)[1:100,]*coefmat[1:100,])))
y &lt;- c(ydat,exp(rowSums(cbind(rep(1,nrow(dat)),dat)[-c(1:100),c(1,5,6)]*
               coefmat[-c(1:100),c(1,5,6)]))/
          (1+exp(rowSums(cbind(rep(1,nrow(dat)),dat)[-c(1:100),c(1,5,6)]*
               coefmat[-c(1:100),c(1,5,6)]))))
u &lt;- runif (length(y))
y &lt;- as.numeric (u &lt; y)

# Consider three candidate models
mmat &lt;- matrix(c(1,1,1,1,1,0,0,0,1,1,1,0,1,0,1),3,5, byrow = TRUE)

# Fit model and plot
# autotuning is turned off for this demonstration example
ldma.test &lt;- logistic.dma(dat, y, mmat, lambda = .99, alpha = .99, 
    autotune = FALSE, initialsamp = 20)
plot(ldma.test)

# Using DMA in a "streaming" mode
modl &lt;- logdma.init(dat[1:20,], y[1:20], mmat)
yhat &lt;- matrix(0, ncol=3, nrow=200)
for(i in 21:200){
  # if prediction is desired, use logdma.predict
  yhat[i,] &lt;- logdma.predict(modl, dat[i,])
  # update
  modl &lt;- logdma.update(modl, dat[i,], y[i], 
                lambda = .99, autotune = FALSE)
}
# the averaging step could be also done within the loop above
ldma.stream &lt;- logdma.average(modl, alpha = .99)
plot(ldma.stream)
</code></pre>


</div>