<div class="container">

<table style="width: 100%;"><tr>
<td>dnnFit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Fitting a Deep Learning model with a given loss function
</h2>

<h3>Description</h3>


<p>dnnFit is used to train a deep learning neural network model based on a specified loss function. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">dnnFit(x, y, model, control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>covariates for the neural network model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>output (target) value for neural network model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>the neural network model, see below for details</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>a list of control values, in the format produced by
'dnnControl'. The default value is dnnControl(loss='mse')</p>
</td>
</tr>
</table>
<h3>Details</h3>


<p>The 'dnnFit' function takes the input data, the target values, the network architecture, and the loss function as arguments, and returns a trained model that minimizes the loss function. The function also supports various options for regularization and optimization of the model.
</p>
<p>See <code>dNNmodel</code> for details on how to specify a deep learning model. 
</p>
<p>Parameters in <code>dnnControl</code> will be used to control the model fit process. The loss function can be specified as dnnControl(loss = "lossFunction"). Currently, the following loss functions are supported: 
</p>
<p>'mse': Mean square error loss = 0.5*sum(dy^2)
</p>
<p>'cox': Cox partial likelihood loss = -sum(delta*(yhat - log(S0)))
</p>
<p>'bin': Cross-entropy = -sum(y*log(p) + (1-y)*log(1-p))
</p>
<p>'log': Log linear cost = -sum(y*log(lambda)-lambda)
</p>
<p>'mae': Mean absolute error loss = sum(abs(dy))
</p>
<p>Additional loss functions will be added to the library in the future.
</p>
<p>{ dnnFit2 } is a C++ version of dnnFit, which runs about 20% faster, however, only loss = 'mse' and 'cox' are currently supported. 
</p>
<p>When the variance for covariance matrix X is too large, please use xbar = scale(x) to standardize X.
</p>


<h3>Value</h3>


<p>An object of class "dnnFit" is returned. The dnnFit object contains the following list components:
</p>

<table>
<tr style="vertical-align: top;">
<td><code>cost</code></td>
<td>
<p>cost at the final epoch.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dW</code></td>
<td>
<p>the gradient at the final epoch dW = dL/dW.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted.values</code></td>
<td>
<p>predictor value mu = f(x).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>history</code></td>
<td>
<p>a cost history at each epoch.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lp</code></td>
<td>
<p>predictor value mu = f(x).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logLik</code></td>
<td>
<p>-2*log Likelihood = cost.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a dNNmodel object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>residuals</code></td>
<td>
<p>raw residual dy = d log(L)/dmu</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dvi</code></td>
<td>
<p>deviance dvi = dy*dy</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Chen, B. E. and Norman P.
</p>


<h3>References</h3>


<p>Buckley, J. and James, I. (1979). Linear regression with censored data. Biometrika, 66, page 429-436.
</p>
<p>Norman, P. and Chen, B. E. (2019). DeepAFAT: A nonparametric accelerated failure time model with artificial neural network. Manuscript to be submitted. 
</p>
<p>Chollet, F. and Allaire J. J. (2017). Deep learning with R. Manning.
</p>


<h3>See Also</h3>


<p><code>deepAFT</code>, <code>deepGlm</code>, <code>deepSurv</code>,   <code>dnnControl</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Example for dnnFit with MSE loss function to do a non-linear regression
  set.seed(101)
### define model layers
  model = dNNmodel(units = c(4, 3, 1), activation = c("elu", "sigmoid", "sigmoid"), 
                   input_shape = 3)
  x = matrix(runif(15), nrow = 5, ncol = 3)
  y = exp(x[, 1])
  control = dnnControl(loss='mse')
  fit = dnnFit(x, y, model, control) 
</code></pre>


</div>