<div class="container">

<table style="width: 100%;"><tr>
<td>at_from_os</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Asymptotic Test from Outlier Scores</h2>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training set, <code>x_train</code> as the
as the reference. The method checks whether the test set, <code>x_test</code>, is
worse off relative to this reference set. The function <code>scorer</code> assigns
an outlier score to each instance/observation in both training and test set.
</p>


<h3>Usage</h3>

<pre><code class="language-R">at_from_os(os_train, os_test)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Li and Fine (2010) derives the asymptotic null distribution for the weighted
AUC (WAUC), the test statistic. This approach does not use permutations
and can, as a result, be much faster because it sidesteps the need to refit
the scoring function <code>scorer</code>. This works well for large samples. The
prefix <em>at</em> stands for asymptotic test to tell it apart from the
prefix <em>pt</em>, the permutation test.
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li>
</ul>
<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch – in-sample versus out-of-sample
scores – voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[at_oob()] for variant requiring a scoring function.
[pt_from_os()] for permutation test with the outlier scores.
</p>
<p>Other asymptotic-test: 
<code>at_oob()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
test_result &lt;- at_from_os(os_train, os_test)
test_result


</code></pre>


</div>