<div class="container">

<table style="width: 100%;"><tr>
<td>deepnet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Build and train an Artificial Neural Network of any size</h2>

<h3>Description</h3>

<p>Build and train Artifical Neural Network of any depth in a single line code. Choose the hyperparameters to improve the accuracy or generalisation of model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">deepnet(
  x,
  y,
  hiddenLayerUnits = c(2, 2),
  activation = c("sigmoid", "relu"),
  reluLeak = 0,
  modelType = c("regress"),
  iterations = 500,
  eta = 10^-2,
  seed = 2,
  gradientClip = 0.8,
  regularisePar = 0,
  optimiser = "adam",
  parMomentum = 0.9,
  inputSizeImpact = 1,
  parRmsPropZeroAdjust = 10^-8,
  parRmsProp = 0.9999,
  printItrSize = 100,
  showProgress = TRUE,
  stopError = 0.01,
  miniBatchSize = NA,
  useBatchProgress = FALSE,
  ignoreNAerror = FALSE,
  normalise = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a data frame with input variables</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a data frame with ouptut variable</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hiddenLayerUnits</code></td>
<td>
<p>a numeric vector, length of vector indicates number of hidden layers and each element in vector indicates corresponding hidden units Eg: c(6,4) for two layers, one with 6 hiiden units and other with 4 hidden units. Note: Output layer is automatically created.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>one of "sigmoid","relu","sin","cos","none". The default is "sigmoid". Choose a activation per hidden layer</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reluLeak</code></td>
<td>
<p>numeric. Applicable when activation is "relu". Specify value between 0 any number close to zero below 1. Eg: 0.01,0.001 etc</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modelType</code></td>
<td>
<p>one of "regress","binary","multiClass". "regress" for regression will create a linear single unit output layer. "binary" will create a single unit sigmoid activated layer. "multiClass" will create layer with units corresponding to number of output classes with softmax activation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iterations</code></td>
<td>
<p>integer. This indicates number of iteratios or epochs in backpropagtion .The default value is 500.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>numeric.Hyperparameter,sets the Learning rate for backpropagation. Eta determines the convergence ability and speed of convergence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>numeric. Set seed with this parameter. Incase of sin activation sometimes changing seed can yeild better results. Default is 2</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gradientClip</code></td>
<td>
<p>numeric. Hyperparameter numeric value which limits gradient size for weight update operation in backpropagation. Default is 0.8 . It can take any postive value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>regularisePar</code></td>
<td>
<p>numeric. L2 Regularisation Parameter .</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimiser</code></td>
<td>
<p>one of "gradientDescent","momentum","rmsProp","adam". Default value "adam"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parMomentum</code></td>
<td>
<p>numeric. Applicable for optimiser "mometum" and "adam"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inputSizeImpact</code></td>
<td>
<p>numeric. Adjusts the gradient size by factor of percentage of rows in input. For very small data set setting this to 0 could yeild faster result. Default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parRmsPropZeroAdjust</code></td>
<td>
<p>numeric. Applicable for optimiser "rmsProp" and "adam"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parRmsProp</code></td>
<td>
<p>numeric.Applicable for optimiser "rmsProp" and "adam"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>printItrSize</code></td>
<td>
<p>numeric. Number of iterations after which progress message should be shown. Default value 100 and for iterations below 100 atleast 5 messages will be seen</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>showProgress</code></td>
<td>
<p>logical. True will show progress and F will not show progress</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stopError</code></td>
<td>
<p>Numeric. Rmse at which iterations can be stopped. Default is 0.01, can be set as NA in case all iterations needs to run.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>miniBatchSize</code></td>
<td>
<p>integer. Set the mini batch size for mini batch gradient</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>useBatchProgress</code></td>
<td>
<p>logical. Applicable for miniBatch , setting T will use show rmse in Batch and F will show error on full dataset. For large dataset set T</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ignoreNAerror</code></td>
<td>
<p>logical. Set T if iteration needs to be stopped when predictions become NA</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalise</code></td>
<td>
<p>logical. Set F if normalisation not required.Default T</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>returns model object which can be passed into <code>predict.deepnet</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">require(deepdive)

x &lt;- data.frame(x1 = runif(10),x2 = runif(10))
y&lt;- data.frame(y=20*x$x1 +30*x$x2+10)

#train
modelnet&lt;-deepnet(x,y,c(2,2),
activation = c('relu',"sigmoid"),
reluLeak = 0.01,
modelType = "regress",
iterations =5,
eta=0.8,
optimiser="adam")

#predict
predDeepNet&lt;-predict.deepnet(modelnet,newData=x)

#evaluate
sqrt(mean((predDeepNet$ypred-y$y)^2))


</code></pre>


</div>