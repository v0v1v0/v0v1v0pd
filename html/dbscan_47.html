<div class="container">

<table style="width: 100%;"><tr>
<td>jpclust</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Jarvis-Patrick Clustering</h2>

<h3>Description</h3>

<p>Fast C++ implementation of the Jarvis-Patrick clustering which first builds
a shared nearest neighbor graph (k nearest neighbor sparsification) and then
places two points in the same cluster if they are in each others nearest
neighbor list and they share at least kt nearest neighbors.
</p>


<h3>Usage</h3>

<pre><code class="language-R">jpclust(x, k, kt, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a data matrix/data.frame (Euclidean distance is used), a
precomputed dist object or a kNN object created with <code>kNN()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>Neighborhood size for nearest neighbor sparsification. If <code>x</code>
is a kNN object then <code>k</code> may be missing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kt</code></td>
<td>
<p>threshold on the number of shared nearest neighbors (including the
points themselves) to form clusters. Range: <code class="reqn">[1, k]</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments are passed on to the k nearest neighbor
search algorithm. See <code>kNN()</code> for details on how to control the
search strategy.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Following the original paper, the shared nearest neighbor list is
constructed as the k neighbors plus the point itself (as neighbor zero).
Therefore, the threshold <code>kt</code> needs to be in the range <code class="reqn">[1, k]</code>.
</p>
<p>Fast nearest neighbors search with <code>kNN()</code> is only used if <code>x</code> is
a matrix. In this case Euclidean distance is used.
</p>


<h3>Value</h3>

<p>A object of class <code>general_clustering</code> with the following
components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>cluster </code></td>
<td>
<p>A integer vector with cluster assignments. Zero
indicates noise points.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type </code></td>
<td>
<p> name of used clustering algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric </code></td>
<td>
<p> the distance metric used for clustering.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>param </code></td>
<td>
<p> list of used clustering parameters. </p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>R. A. Jarvis and E. A. Patrick. 1973. Clustering Using a
Similarity Measure Based on Shared Near Neighbors. <em>IEEE Trans. Comput.
22,</em> 11 (November 1973), 1025-1034.
<a href="https://doi.org/10.1109/T-C.1973.223640">doi:10.1109/T-C.1973.223640</a>
</p>


<h3>See Also</h3>

<p>Other clustering functions: 
<code>dbscan()</code>,
<code>extractFOSC()</code>,
<code>hdbscan()</code>,
<code>optics()</code>,
<code>sNNclust()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("DS3")

# use a shared neighborhood of 20 points and require 12 shared neighbors
cl &lt;- jpclust(DS3, k = 20, kt = 12)
cl

plot(DS3, col = cl$cluster+1L, cex = .5)
# Note: JP clustering does not consider noise and thus,
# the sine wave points chain clusters together.

# use a precomputed kNN object instead of the original data.
nn &lt;- kNN(DS3, k = 30)
nn

cl &lt;- jpclust(nn, k = 20, kt = 12)
cl

# cluster with noise removed (use low pointdensity to identify noise)
d &lt;- pointdensity(DS3, eps = 25)
hist(d, breaks = 20)
DS3_noiseless &lt;- DS3[d &gt; 110,]

cl &lt;- jpclust(DS3_noiseless, k = 20, kt = 10)
cl

plot(DS3_noiseless, col = cl$cluster+1L, cex = .5)
</code></pre>


</div>