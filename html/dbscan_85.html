<div class="container">

<table style="width: 100%;"><tr>
<td>sNN</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Find Shared Nearest Neighbors</h2>

<h3>Description</h3>

<p>Calculates the number of shared nearest neighbors
and creates a shared nearest neighbors graph.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sNN(
  x,
  k,
  kt = NULL,
  jp = FALSE,
  sort = TRUE,
  search = "kdtree",
  bucketSize = 10,
  splitRule = "suggest",
  approx = 0
)

## S3 method for class 'sNN'
sort(x, decreasing = TRUE, ...)

## S3 method for class 'sNN'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a data matrix, a dist object or a kNN object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>number of neighbors to consider to calculate the shared nearest
neighbors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kt</code></td>
<td>
<p>minimum threshold on the number of shared nearest neighbors to
build the shared nearest neighbor graph. Edges are only preserved if
<code>kt</code> or more neighbors are shared.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>jp</code></td>
<td>
<p>In regular sNN graphs, two points that are not neighbors
can have shared neighbors.
Javis and Patrick (1973) requires the two points to be neighbors, otherwise
the count is zeroed out. <code>TRUE</code> uses this behavior.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sort</code></td>
<td>
<p>sort by the number of shared nearest neighbors? Note that this
is expensive and <code>sort = FALSE</code> is much faster. sNN objects can be
sorted using <code>sort()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>search</code></td>
<td>
<p>nearest neighbor search strategy (one of <code>"kdtree"</code>, <code>"linear"</code> or
<code>"dist"</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bucketSize</code></td>
<td>
<p>max size of the kd-tree leafs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>splitRule</code></td>
<td>
<p>rule to split the kd-tree. One of <code>"STD"</code>, <code>"MIDPT"</code>, <code>"FAIR"</code>,
<code>"SL_MIDPT"</code>, <code>"SL_FAIR"</code> or <code>"SUGGEST"</code> (SL stands for sliding). <code>"SUGGEST"</code> uses
ANNs best guess.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>approx</code></td>
<td>
<p>use approximate nearest neighbors. All NN up to a distance of
a factor of <code style="white-space: pre;">⁠(1 + approx) eps⁠</code> may be used. Some actual NN may be omitted
leading to spurious clusters and noise points.  However, the algorithm will
enjoy a significant speedup.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>decreasing</code></td>
<td>
<p>logical; sort in decreasing order?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters are passed on.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The number of shared nearest neighbors of two points p and q is the
intersection of the kNN neighborhood of two points.
Note: that each point is considered to be part
of its own kNN neighborhood.
The range for the shared nearest neighbors is
<code class="reqn">[0, k]</code>. The result is a n-by-k matrix called <code>shared</code>.
Each row is a point and the columns are the point's k nearest neighbors.
The value is the count of the shared neighbors.
</p>
<p>The shared nearest neighbor graph connects a point with all its nearest neighbors
if they have at least one shared neighbor. The number of shared neighbors can be used
as an edge weight.
Javis and Patrick (1973) use a slightly
modified (see parameter <code>jp</code>) shared nearest neighbor graph for
clustering.
</p>


<h3>Value</h3>

<p>An object of class <code>sNN</code> (subclass of kNN and NN) containing a list
with the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>id </code></td>
<td>
<p>a matrix with ids. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist</code></td>
<td>
<p>a matrix with the distances. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shared </code></td>
<td>
<p>a matrix with the number of shared nearest neighbors. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k </code></td>
<td>
<p>number of <code>k</code> used. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric </code></td>
<td>
<p>the used distance metric. </p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michael Hahsler
</p>


<h3>References</h3>

<p>R. A. Jarvis and E. A. Patrick. 1973. Clustering Using a
Similarity Measure Based on Shared Near Neighbors. <em>IEEE Trans. Comput.</em>
22, 11 (November 1973), 1025-1034.
<a href="https://doi.org/10.1109/T-C.1973.223640">doi:10.1109/T-C.1973.223640</a>
</p>


<h3>See Also</h3>

<p>Other NN functions: 
<code>NN</code>,
<code>comps()</code>,
<code>frNN()</code>,
<code>kNN()</code>,
<code>kNNdist()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(iris)
x &lt;- iris[, -5]

# finding kNN and add the number of shared nearest neighbors.
k &lt;- 5
nn &lt;- sNN(x, k = k)
nn

# shared nearest neighbor distribution
table(as.vector(nn$shared))

# explore number of shared points for the k-neighborhood of point 10
i &lt;- 10
nn$shared[i,]

plot(nn, x)

# apply a threshold to create a sNN graph with edges
# if more than 3 neighbors are shared.
nn_3 &lt;- sNN(nn, kt = 3)
plot(nn_3, x)

# get an adjacency list for the shared nearest neighbor graph
adjacencylist(nn_3)
</code></pre>


</div>