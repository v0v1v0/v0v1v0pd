<div class="container">

<table style="width: 100%;"><tr>
<td>bf_compare</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian and Frequentist Test from Outlier Scores</h2>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training (outlier) scores,
<code>os_train</code>, as the reference. The method checks whether the test
scores, <code>os_test</code>, are worse off relative to the training set.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bf_compare(os_train, os_test, threshold = 1/12, n_pt = 4000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>Threshold for adverse shift. Defaults to 1 / 12,
the asymptotic value of the test statistic when the two samples are drawn
from the same distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This compares the Bayesian to the frequentist approach for convenience.
The Bayesian test mimics 'bf_from_os()' and the frequentist one,
'pt_from_os()'. The Bayesian test computes Bayes factors based on the
asymptotic (defaults to 1/12) and the exchangeable threshold. The latter
calculates the threshold as the median weighted AUC (WAUC) after <code>n_pt</code>
permutations assuming outlier scores are exchangeable. This is recommended
for small samples. The frequentist test converts the one-sided (one-tailed)
p-value to the Bayes factor - see <code>as_bf</code> function.
</p>


<h3>Value</h3>

<p>A list of factors (BF) for 3 different test specifications:
</p>

<ul>
<li> <p><code>frequentist</code>: Frequentist BF.
</p>
</li>
<li> <p><code>bayes_noperm</code>: Bayestion BF test with asymptotic threshold.
</p>
</li>
<li> <p><code>bayes_perm</code>: Bayestion BF with exchangeable threshold.
</p>
</li>
</ul>
<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch – in-sample versus out-of-sample
scores – voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>See Also</h3>

<p>[bf_from_os()] for bayes factor, the Bayesian test.
[pt_from_os()] for p-value, the frequentist test.
</p>
<p>Other bayesian-test: 
<code>as_bf()</code>,
<code>as_pvalue()</code>,
<code>bf_from_os()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
bayes_test &lt;- bf_compare(os_train, os_test)
bayes_test
# To run in parallel on local cluster, uncomment the next two lines.
# library(future)
# future::plan(future::multisession)
parallel_test &lt;- bf_compare(os_train, os_test)
parallel_test


</code></pre>


</div>