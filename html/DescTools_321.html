<div class="container">

<table style="width: 100%;"><tr>
<td>KappaM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kappa for m Raters</h2>

<h3>Description</h3>

<p>Computes kappa as an index of interrater agreement between m raters on categorical data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KappaM(x, method = c("Fleiss", "Conger", "Light"), conf.level = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p><code class="reqn">n \times m</code> matrix or dataframe, n subjects m raters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>a logical indicating whether the exact Kappa (Conger, 1980), the Kappa described by Fleiss (1971) or Light's Kappa (1971) should be computed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf.level</code></td>
<td>
<p>confidence level of the interval. If set to <code>NA</code> (which is the default) no confidence intervals will be calculated.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br>
The coefficient described by Fleiss (1971) does not reduce to Cohen's Kappa (unweighted) for m=2 raters. Therefore, the exact Kappa coefficient, which is slightly higher in most cases, was proposed by Conger (1980).<br>
Light's Kappa equals the average of all possible combinations of bivariate Kappas between raters.<br>
The confidence levels can only be reported using Fleiss' formulation of Kappa.
</p>


<h3>Value</h3>

<p>a single numeric value if no confidence intervals are requested,<br>
and otherwise a numeric vector with 3 elements for the estimate, the lower and the upper confidence interval
</p>


<h3>Note</h3>

<p> This function was previously published as <code>kappam.fleiss()</code> in the  <span class="pkg">irr</span> package and has been integrated here with some changes in the interface.
</p>


<h3>Author(s)</h3>

<p>Matthias Gamer, with some modifications by Andri Signorell &lt;andri@signorell.net&gt;</p>


<h3>References</h3>

<p>Conger, A.J. (1980): Integration and generalisation of Kappas for multiple raters. <em>Psychological Bulletin</em>, 88, 322-328
</p>
<p>Fleiss, J.L. (1971): Measuring nominal scale agreement among many raters <em>Psychological Bulletin</em>, 76, 378-382
</p>
<p>Fleiss, J.L., Levin, B., &amp; Paik, M.C. (2003): <em>Statistical Methods for Rates and Proportions</em>, 3rd Edition. New York: John Wiley &amp; Sons
</p>
<p>Light, R.J. (1971): Measures of response agreement for qualitative data: Some generalizations and alternatives. <em>Psychological Bulletin</em>, 76, 365-377.
</p>


<h3>See Also</h3>

<p><code>CohenKappa</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">statement &lt;- data.frame(
  A=c(2,3,1,3,1,2,1,2,3,3,3,3,3,2,1,3,3,2,2,1,
      2,1,3,3,2,2,1,2,1,1,2,3,3,3,3,3,1,2,1,1),
  B=c(2,2,2,1,1,2,1,2,3,3,2,3,1,3,1,1,3,2,1,2,
      2,1,3,2,2,2,3,2,1,1,2,2,3,3,3,3,2,2,2,3),
  C=c(2,2,2,1,1,2,1,2,3,3,2,3,3,3,3,2,2,2,2,3,
      2,2,3,3,2,2,3,2,2,2,2,3,3,3,3,3,3,2,2,2),
  D=c(2,2,2,1,1,2,1,2,3,3,2,3,3,3,3,3,2,2,2,2,
      3,1,3,2,2,2,1,2,2,1,2,3,3,3,3,3,3,2,2,1),
  E=c(2,2,2,3,3,2,3,1,3,3,2,3,3,3,3,3,2,2,2,3,
      2,3,3,2,2,2,3,2,1,3,2,3,3,1,3,3,3,2,2,1)
)

KappaM(statement)

KappaM(statement, method="Conger")   # Exact Kappa
KappaM(statement, conf.level=0.95)   # Fleiss' Kappa and confidence intervals

KappaM(statement, method="Light")   # Exact Kappa
</code></pre>


</div>