<div class="container">

<table style="width: 100%;"><tr>
<td>bf_from_os</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian Test from Outlier Scores</h2>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training (outlier) scores,
<code>os_train</code>, as the reference. The method checks whether the test
scores, <code>os_test</code>, are worse off relative to the training set.
</p>


<h3>Usage</h3>

<pre><code class="language-R">bf_from_os(os_train, os_test, n_pt = 4000, threshold = 1/12)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>Threshold for adverse shift. Defaults to 1 / 12,
the asymptotic value of the test statistic when the two samples are drawn
from the same distribution.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The posterior distribution of the test statistic is based on <code>n_pt</code>
(boostrap) permutations. The method uses the Bayesian bootstrap as a
resampling procedure as in Gu et al (2008). Johnson (2005) shows to
leverage (turn) a test statistic into a Bayes factor. The test statistic
is the weighted AUC (WAUC).
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.bayes</code> containing:
</p>

<ul>
<li> <p><code>posterior</code>: Posterior distribution of WAUC test statistic
</p>
</li>
<li> <p><code>threshold</code>: WAUC threshold for adverse shift
</p>
</li>
<li> <p><code>adverse_probability</code>: probability of adverse shift
</p>
</li>
<li> <p><code>bayes_factor</code>: Bayes factor
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li>
</ul>
<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch – in-sample versus out-of-sample
scores – voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2023).
<em>Are you OK? A Bayesian test for adverse shift</em>.
Manuscript in preparation.
</p>
<p>Johnson, V. E. (2005).
<em>Bayes factors based on test statistics</em>.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(5), 689-701.
</p>
<p>Gu, J., Ghosal, S., &amp; Roy, A. (2008).
<em>Bayesian bootstrap estimation of ROC curve</em>.
Statistics in medicine, 27(26), 5407-5420.
</p>


<h3>See Also</h3>

<p>Other bayesian-test: 
<code>as_bf()</code>,
<code>as_pvalue()</code>,
<code>bf_compare()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
bayes_test &lt;- bf_from_os(os_train, os_test)
bayes_test
# To run in parallel on local cluster, uncomment the next two lines.
# library(future)
# future::plan(future::multisession)
parallel_test &lt;- bf_from_os(os_train, os_test)
parallel_test


</code></pre>


</div>