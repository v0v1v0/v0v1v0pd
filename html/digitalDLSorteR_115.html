<div class="container">

<table style="width: 100%;"><tr>
<td>trainDDLSModel</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train Deep Neural Network model</h2>

<h3>Description</h3>

<p>Train a Deep Neural Network model using the training data from
<code>DigitalDLSorter</code> object. In addition, the trained model
is evaluated with test data and prediction results are computed to determine
its performance (see <code>?calculateEvalMetrics</code>). Training and
evaluation can be performed using simulated profiles stored in the
<code>DigitalDLSorter</code> object or 'on the fly' by simulating the
pseudo-bulk profiles at the same time as the training/evaluation is performed
(see Details).
</p>


<h3>Usage</h3>

<pre><code class="language-R">trainDDLSModel(
  object,
  type.data.train = "bulk",
  type.data.test = "bulk",
  batch.size = 64,
  num.epochs = 60,
  num.hidden.layers = 2,
  num.units = c(200, 200),
  activation.fun = "relu",
  dropout.rate = 0.25,
  loss = "kullback_leibler_divergence",
  metrics = c("accuracy", "mean_absolute_error", "categorical_accuracy"),
  normalize = TRUE,
  scaling = "standardize",
  norm.batch.layers = TRUE,
  custom.model = NULL,
  shuffle = TRUE,
  use.generator = FALSE,
  on.the.fly = FALSE,
  pseudobulk.function = "AddRawCount",
  threads = 1,
  view.metrics.plot = TRUE,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p><code>DigitalDLSorter</code> object with
<code>single.cell.real</code>/<code>single.cell.simul</code>, <code>prob.cell.matrix</code>
and <code>bulk.simul</code> slots.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type.data.train</code></td>
<td>
<p>Type of profiles to be used for training. It can be
<code>'both'</code>, <code>'single-cell'</code> or <code>'bulk'</code> (<code>'bulk'</code> by
default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type.data.test</code></td>
<td>
<p>Type of profiles to be used for evaluation. It can be
<code>'both'</code>, <code>'single-cell'</code> or <code>'bulk'</code> (<code>'bulk'</code> by
default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch.size</code></td>
<td>
<p>Number of samples per gradient update. If not specified,
<code>batch.size</code> will default to 64.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.epochs</code></td>
<td>
<p>Number of epochs to train the model (10 by default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.hidden.layers</code></td>
<td>
<p>Number of hidden layers of the neural network (2 by
default). This number must be equal to the length of <code>num.units</code>
argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num.units</code></td>
<td>
<p>Vector indicating the number of neurons per hidden layer
(<code>c(200, 200)</code> by default). The length of this vector must be equal to
<code>num.hidden.layers</code> argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation.fun</code></td>
<td>
<p>Activation function to use (<code>'relu'</code> by default).
See the
<a href="https://tensorflow.rstudio.com/reference/keras/activation_relu.html">keras
documentation</a> to know available activation functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout.rate</code></td>
<td>
<p>Float between 0 and 1 indicating the fraction of the
input neurons to drop in layer dropouts (0.25 by default). By default,
<span class="pkg">digitalDLSorteR</span> implements 1 dropout layer per hidden layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>Character indicating loss function selected for model training
(<code>'kullback_leibler_divergence'</code> by default). See the
<a href="https://tensorflow.rstudio.com/reference/keras/loss-functions.html">keras
documentation</a> to know available loss functions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p>Vector of metrics used to assess model performance during
training and evaluation (<code>c("accuracy", "mean_absolute_error",
  "categorical_accuracy")</code> by default). See the
<a href="https://tensorflow.rstudio.com/reference/keras/metric_binary_accuracy.html">keras
documentation</a> to know available performance metrics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>Whether to normalize data using logCPM (<code>TRUE</code> by
default). This parameter is only considered when the method used to
simulate mixed transcriptional profiles (<code>simMixedProfiles</code>
function) was <code>"AddRawCount"</code>. Otherwise, data were already
normalized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scaling</code></td>
<td>
<p>How to scale data before training. It may be:
<code>"standardize"</code> (values are centered around the mean with a unit
standard deviation) or <code>"rescale"</code> (values are shifted and rescaled so
that they end up ranging between 0 and 1).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>norm.batch.layers</code></td>
<td>
<p>Whether to include batch normalization layers
between each hidden dense layer (<code>TRUE</code> by default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>custom.model</code></td>
<td>
<p>It allows to use a custom neural network. It must be a
<code>keras.engine.sequential.Sequential</code> object in which the number of
input neurons is equal to the number of considered features/genes, and the
number of output neurons is equal to the number of cell types considered
(<code>NULL</code> by default). If provided, the arguments related to the neural
network architecture will be ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shuffle</code></td>
<td>
<p>Boolean indicating whether data will be shuffled (<code>TRUE</code>
by default). Note that if <code>bulk.simul</code> is not <code>NULL</code>, the data
already has been shuffled and <code>shuffle</code> will be ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use.generator</code></td>
<td>
<p>Boolean indicating whether to use generators during
training and test. Generators are automatically used when <code>on.the.fly
  = TRUE</code> or HDF5 files are used, but it can be activated by the user on
demand (<code>FALSE</code> by default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>on.the.fly</code></td>
<td>
<p>Boolean indicating whether data will be generated 'on the
fly' during training (<code>FALSE</code> by default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pseudobulk.function</code></td>
<td>
<p>Function used to build pseudo-bulk samples. It may
be: </p>
 <ul>
<li> <p><code>"MeanCPM"</code>: single-cell profiles (raw counts) are
transformed into CPMs and cross-cell averages are calculated. Then,
<code>log2(CPM + 1)</code> is calculated. </p>
</li>
<li> <p><code>"AddCPM"</code>: single-cell
profiles (raw counts) are transformed into CPMs and are added up across
cells. Then, log-CPMs are calculated. </p>
</li>
<li> <p><code>"AddRawCount"</code>:
single-cell profiles (raw counts) are added up across cells. Then, log-CPMs
are calculated.</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threads</code></td>
<td>
<p>Number of threads used during simulation of pseudo-bulk
samples if <code>on.the.fly = TRUE</code> (1 by default).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>view.metrics.plot</code></td>
<td>
<p>Boolean indicating whether to show plots of loss and
metrics progression during training (<code>TRUE</code> by default). <span class="pkg">keras</span>
for R allows to see the progression of the model during training if you are
working in RStudio.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Boolean indicating whether to display model progression during
training and model architecture information (<code>TRUE</code> by default).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><strong>Keras/Tensorflow environment</strong>
</p>
<p>All Deep Learning related steps in the <span class="pkg">digitalDLSorteR</span> package are
performed by using the <span class="pkg">keras</span> package, an API in R for <span class="pkg">keras</span> in
Python available on CRAN. We recommend using the <code>installTFpython</code>
function included in the package.
</p>
<p><strong>Simulation of bulk RNA-Seq profiles 'on the fly'</strong>
</p>
<p><code>trainDDLSModel</code> allows to avoid storing bulk RNA-Seq
profiles by using <code>on.the.fly</code> argument. This functionality aims to
avoid exexcution times and memory usage of the <code>simBulkProfiles</code>
function, as the simulated pseudo-bulk profiles are built in each batch
during training/evaluation.
</p>
<p><strong>Neural network architecture</strong>
</p>
<p>By default, <code>trainDDLSModel</code> implements the
architecture selected in Torroja and Sánchez-Cabo, 2019. However, as the
default architecture may not produce good results depending on the dataset,
it is possible to change its parameters by using the corresponding argument:
number of hidden layers, number of neurons for each hidden layer, dropout
rate, activation function and loss function. For more customized models, it
is possible to provide a pre-built model in the <code>custom.model</code> argument
(a <code>keras.engine.sequential.Sequential</code> object) where it is necessary
that the number of input neurons is equal to the number of considered
features/genes and the number of output neurons is equal to the number of
considered cell types.
</p>


<h3>Value</h3>

<p>A <code>DigitalDLSorter</code> object with
<code>trained.model</code> slot containing a
<code>DigitalDLSorterDNN</code> object. For more information about
the structure of this class, see <code>?DigitalDLSorterDNN</code>.
</p>


<h3>References</h3>

<p>Torroja, C. and Sánchez-Cabo, F. (2019). digitalDLSorter: A Deep
Learning algorithm to quantify immune cell populations based on scRNA-Seq
data. Frontiers in Genetics 10, 978. doi: <a href="https://doi.org/10.3389/fgene.2019.00978">doi:10.3389/fgene.2019.00978</a>
</p>


<h3>See Also</h3>

<p><code>plotTrainingHistory</code>
<code>deconvDigitalDLSorter</code> <code>deconvDDLSObj</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
set.seed(123) # reproducibility
sce &lt;- SingleCellExperiment::SingleCellExperiment(
  assays = list(
    counts = matrix(
      rpois(30, lambda = 5), nrow = 15, ncol = 10,
      dimnames = list(paste0("Gene", seq(15)), paste0("RHC", seq(10)))
    )
  ),
  colData = data.frame(
    Cell_ID = paste0("RHC", seq(10)),
    Cell_Type = sample(x = paste0("CellType", seq(2)), size = 10,
                       replace = TRUE)
  ),
  rowData = data.frame(
    Gene_ID = paste0("Gene", seq(15))
  )
)
DDLS &lt;- createDDLSobject(
  sc.data = sce,
  sc.cell.ID.column = "Cell_ID",
  sc.gene.ID.column = "Gene_ID",
  sc.filt.genes.cluster = FALSE, 
  sc.log.FC = FALSE
)
probMatrixValid &lt;- data.frame(
  Cell_Type = paste0("CellType", seq(2)),
  from = c(1, 30),
  to = c(15, 70)
)
DDLS &lt;- generateBulkCellMatrix(
  object = DDLS,
  cell.ID.column = "Cell_ID",
  cell.type.column = "Cell_Type",
  prob.design = probMatrixValid,
  num.bulk.samples = 30,
  verbose = TRUE
)
# training of DDLS model
tensorflow::tf$compat$v1$disable_eager_execution()
DDLS &lt;- trainDDLSModel(
  object = DDLS,
  on.the.fly = TRUE,
  batch.size = 12,
  num.epochs = 5
)

## End(Not run)

</code></pre>


</div>