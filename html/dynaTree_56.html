<div class="container">

<table style="width: 100%;"><tr>
<td>relevance.dynaTree</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Calculate relevance statistics for input coordinates
</h2>

<h3>Description</h3>

<p>Computes relevance statistics for each input coordinate by
calculating their particle-averaged mean reduction in variance
each time that coordinate is used as a splitting variable in
(an internal node of) the tree(s)
</p>


<h3>Usage</h3>

<pre><code class="language-R">relevance.dynaTree(object, rect = NULL, categ = NULL,
     approx = FALSE, verb = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>

<p>a <code>"dynaTree"</code>-class object built by <code>dynaTree</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rect</code></td>
<td>

<p>an optional <code>matrix</code> with two columns and
<code>ncol(object$X)</code> rows describing the bounding rectangle
for the ALC integration; the
default that is used when <code>rect = NULL</code> is the bounding
rectangle obtained by applying <code>range</code> to each
column of <code>object$X</code> (taking care to remove the
first/intercept column of <code>object$X</code> if <code>icept =
      "augmented"</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>categ</code></td>
<td>

<p>A vector of logicals of length <code>ncol(object$X)</code> indicating
which, if any, dimensions of the input space should be treated
as categorical; the default <code>categ</code>
argument is <code>NULL</code> meaning that the categorical inputs
are derived from <code>object$X</code> in a sensible way</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>approx</code></td>
<td>

<p>a scalar logical indicating if the count of the number of
data points in the leaf should be used in place of its area;
this can help with numerical accuracy in high dimensional input
spaces
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>

<p>a positive scalar integer indicating how many particles should
be processed (iterations) before a progress statement should be
printed to the console; a (default) value of <code>verb = 0</code> is quiet
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Each binary split in the tree (in each particle) emits a reduction
in variance (for regression models) or a reduction in entropy
(for classification).  This function calculates these reductions
and attributes them to the variable(s) involved in the split(s).
Those with the largest relevances are the most useful for prediction.
A sensible variable selection rule based on these relevances is to
discard those variables whose median relevance is not positive.  See
the Gramacy, Taddy, &amp; Wild (2011) reference below for more details.
</p>
<p>The new set of particles is appended to the old set.  However
after a subsequent <code>update.dynaTree</code> call the total
number of particles reverts to the original amount.
</p>
<p>Note that this does not work well with <code>dynaTree</code> objects
which were built with <code>model="linear"</code>.  Rather, a full
sensitivity analysis (<code>sens.dynaTree</code>) is needed.  Usually
it is best to first do <code>model="constant"</code> and then use
<code>relevance.dynaTree</code>.  Bayes factors (<code>getBF</code>)
can be used to back up any variable selections implied by the
relevance.  Then, if desired, one can re-fit on the new (possibly
reduced) set of predictors with <code>model="linear"</code>.
</p>
<p>There are no caveats with <code>model="class"</code>
</p>


<h3>Value</h3>

<p>The entire <code>object</code> is returned with a new entry called
<code>relevance</code> containing a <code>matrix</code> with <code>ncol(X)</code>
columns.  Each row contains the sample from the relevance of
each input, and there is a row for each particle
</p>


<h3>Author(s)</h3>

<p>Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a>, <br>
Matt Taddy and Christoforos Anagnostopoulos</p>


<h3>References</h3>

<p>Gramacy, R.B., Taddy, M.A., and S. Wild (2011).
“Variable Selection and Sensitivity Analysis via
Dynamic Trees with an Application to Computer Code Performance Tuning”
arXiv:1108.4739
</p>
<p><a href="https://bobby.gramacy.com/r_packages/dynaTree/">https://bobby.gramacy.com/r_packages/dynaTree/</a>
</p>


<h3>See Also</h3>

<p><code>dynaTree</code>, <code>sens.dynaTree</code>,
<code>predict.dynaTree</code>
<code>varpropuse</code>, <code>varproptotal</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## see the examples in sens.dynaTree for the relevances;
## Also see varpropuse and the class2d demo via
## demo("class2d")
</code></pre>


</div>