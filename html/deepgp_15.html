<div class="container">

<table style="width: 100%;"><tr>
<td>fit_three_layer</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>MCMC sampling for three layer deep GP</h2>

<h3>Description</h3>

<p>Conducts MCMC sampling of hyperparameters, hidden layer 
<code>z</code>, and hidden layer <code>w</code> for a three layer deep GP.  
Separate length scale parameters <code>theta_z</code>, 
<code>theta_w</code>, and <code>theta_y</code> govern the correlation 
strength of the inner layer, middle layer, and outer layer respectively.  
Nugget parameter <code>g</code> governs noise on the outer layer.  In Matern 
covariance, <code>v</code> governs smoothness.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fit_three_layer(
  x,
  y,
  nmcmc = 10000,
  D = ifelse(is.matrix(x), ncol(x), 1),
  verb = TRUE,
  w_0 = NULL,
  z_0 = NULL,
  g_0 = 0.001,
  theta_y_0 = 0.1,
  theta_w_0 = 0.1,
  theta_z_0 = 0.1,
  true_g = NULL,
  settings = NULL,
  cov = c("matern", "exp2"),
  v = 2.5,
  vecchia = FALSE,
  m = min(25, length(y) - 1),
  ordering = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>vector or matrix of input locations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of response values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nmcmc</code></td>
<td>
<p>number of MCMC iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>D</code></td>
<td>
<p>integer designating dimension of hidden layers, defaults to 
dimension of <code>x</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>
<p>logical indicating whether to print iteration progress</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w_0</code></td>
<td>
<p>initial value for hidden layer <code>w</code> (must be matrix 
of dimension <code>nrow(x)</code> by <code>D</code> or  dimension 
<code>nrow(x) - 1</code> by <code>D</code>).  Defaults to the identity mapping.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>z_0</code></td>
<td>
<p>initial value for hidden layer <code>z</code> (must be matrix 
of dimension <code>nrow(x)</code> by <code>D</code> or  dimension 
<code>nrow(x) - 1</code> by <code>D</code>).  Defaults to the identity mapping.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g_0</code></td>
<td>
<p>initial value for <code>g</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta_y_0</code></td>
<td>
<p>initial value for <code>theta_y</code> (length scale of outer 
layer)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta_w_0</code></td>
<td>
<p>initial value for <code>theta_w</code> (length scale of middle 
layer), may be single value or vector of length <code>D</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta_z_0</code></td>
<td>
<p>initial value for <code>theta_z</code> (length scale of inner 
layer), may be single value or vector of length <code>D</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>true_g</code></td>
<td>
<p>if true nugget is known it may be specified here (set to a 
small value to make fit deterministic).  Note - values that are too 
small may cause numerical issues in matrix inversions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>settings</code></td>
<td>
<p>hyperparameters for proposals and priors (see details)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cov</code></td>
<td>
<p>covariance kernel, either Matern or squared exponential 
(<code>"exp2"</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>Matern smoothness parameter (only used if <code>cov = "matern"</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vecchia</code></td>
<td>
<p>logical indicating whether to use Vecchia approximation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>size of Vecchia conditioning sets (only used if 
<code>vecchia = TRUE</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ordering</code></td>
<td>
<p>optional ordering for Vecchia approximation, must correspond
to rows of <code>x</code>, defaults to random, is applied to <code>x</code>,
<code>w</code>, and <code>z</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>pmx = TRUE</code> option not yet implemented for three-layer DGP.
</p>
<p>Maps inputs <code>x</code> through hidden layer <code>z</code> then hidden
layer <code>w</code> to outputs <code>y</code>.  Conducts sampling of the hidden 
layers using Elliptical Slice sampling.  Utilizes Metropolis Hastings 
sampling of the length scale and nugget parameters with proposals and 
priors controlled by <code>settings</code>.  When <code>true_g</code> is set to a 
specific value, the nugget is not estimated.  When 
<code>vecchia = TRUE</code>, all calculations leverage the Vecchia 
approximation with specified conditioning set size <code>m</code>.  Vecchia 
approximation is only implemented for <code>cov = "matern"</code>.
</p>
<p>NOTE on OpenMP: The Vecchia implementation relies on OpenMP parallelization
for efficient computation.  This function will produce a warning message 
if the package was installed without OpenMP (this is the default for 
CRAN packages installed on Apple machines).  To set up OpenMP 
parallelization, download the package source code and install 
using the gcc/g++ compiler.  
</p>
<p>Proposals for <code>g</code>, 
<code>theta_y</code>, <code>theta_w</code>, and <code>theta_z</code> follow a uniform 
sliding window scheme, e.g.
</p>
<p><code>g_star &lt;- runif(1, l * g_t / u, u * g_t / l)</code>,
</p>
<p>with defaults <code>l = 1</code> and <code>u = 2</code> provided in <code>settings</code>.
To adjust these, set <code>settings = list(l = new_l, u = new_u)</code>.  
Priors on <code>g</code>, <code>theta_y</code>, <code>theta_w</code>, and <code>theta_z</code> 
follow Gamma distributions with shape parameters (<code>alpha</code>) and rate 
parameters (<code>beta</code>) controlled within the <code>settings</code> list 
object.  Defaults have been updated with package version 1.1.3.  
Default priors differ for noisy/deterministic settings and 
depend on whether <code>monowarp = TRUE</code>.  All default values are 
visible in the internal <code>deepgp:::check_settings</code> function.
These priors are designed for <code>x</code> scaled to [0, 1] and <code>y</code> 
scaled to have mean 0 and variance 1.  These may be adjusted using the 
<code>settings</code> input.
</p>
<p>In the current version, the three-layer does not have any equivalent
setting for <code>monowarp = TRUE</code> or <code>pmx = TRUE</code> as in 
<code>fit_two_layer</code>.
</p>
<p>When <code>w_0 = NULL</code> and/or <code>z_0 = NULL</code>, the hidden layers are 
initialized at <code>x</code> (i.e. the identity mapping).  The default prior 
mean of the inner hidden layer <code>z</code> is zero, but may be adjusted to <code>x</code> 
using <code>settings = list(z_prior_mean = x)</code>.  The prior mean of the
middle hidden layer <code>w</code> is set at zero is is not user adjustable.
If <code>w_0</code> and/or <code>z_0</code> is of dimension <code>nrow(x) - 1</code> by 
<code>D</code>, the final row is predicted using kriging. This is helpful in 
sequential design when adding a new input location and starting the MCMC 
at the place where the previous MCMC left off.
</p>
<p>The output object of class <code>dgp3</code> or <code>dgp3vec</code> is designed for 
use with <code>continue</code>, <code>trim</code>, and <code>predict</code>.
</p>


<h3>Value</h3>

<p>a list of the S3 class <code>dgp3</code> or <code>dgp3vec</code> with elements:
</p>

<ul>
<li> <p><code>x</code>: copy of input matrix
</p>
</li>
<li> <p><code>y</code>: copy of response vector
</p>
</li>
<li> <p><code>nmcmc</code>: number of MCMC iterations
</p>
</li>
<li> <p><code>settings</code>: copy of proposal/prior settings
</p>
</li>
<li> <p><code>v</code>: copy of Matern smoothness parameter (<code>v = 999</code> 
indicates <code>cov = "exp2"</code>) 
</p>
</li>
<li> <p><code>g</code>: vector of MCMC samples for <code>g</code>
</p>
</li>
<li> <p><code>theta_y</code>: vector of MCMC samples for <code>theta_y</code> (length 
scale of outer layer)
</p>
</li>
<li> <p><code>theta_w</code>: matrix of MCMC samples for <code>theta_w</code> (length 
scale of middle layer)
</p>
</li>
<li> <p><code>theta_z</code>: matrix of MCMC samples for <code>theta_z</code> (length 
scale of inner layer)
</p>
</li>
<li> <p><code>tau2</code>: vector of MLE estimates for <code>tau2</code> (scale 
parameter of outer layer)
</p>
</li>
<li> <p><code>w</code>: list of MCMC samples for middle hidden layer <code>w</code>
</p>
</li>
<li> <p><code>z</code>: list of MCMC samples for inner hidden layer <code>z</code>
</p>
</li>
<li> <p><code>ll</code>: vector of MVN log likelihood of the outer layer 
for reach Gibbs iteration
</p>
</li>
<li> <p><code>time</code>: computation time in seconds
</p>
</li>
</ul>
<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<br><br>
Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br><br>
Sauer, A., Cooper, A., &amp; Gramacy, R. B. (2023). Vecchia-approximated deep Gaussian 
processes for computer experiments. 
*Journal of Computational and Graphical Statistics, 32*(3), 824-837.  arXiv:2204.02904
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Additional examples including real-world computer experiments are available at: 
# https://bitbucket.org/gramacylab/deepgp-ex/

# G function in 2 dimensions (https://www.sfu.ca/~ssurjano/gfunc.html)
f &lt;- function(xx, a = (c(1:length(xx)) - 1) / 2) { 
  new1 &lt;- abs(4 * xx - 2) + a
  new2 &lt;- 1 + a
  prod &lt;- prod(new1 / new2)
  return((prod - 1) / 0.86)
}

# Training data
d &lt;- 2
n &lt;- 30
x &lt;- matrix(runif(n * d), ncol = d)
y &lt;- apply(x, 1, f)

# Testing data
n_test &lt;- 500
xx &lt;- matrix(runif(n_test * d), ncol = d)
yy &lt;- apply(xx, 1, f)

i &lt;- interp::interp(xx[, 1], xx[, 2], yy)
image(i, col = heat.colors(128))
contour(i, add = TRUE)
contour(i, level = -0.5, col = 4, add = TRUE) # potential failure limit
points(x)

# Example 1: full model (nugget estimated, entropy calculated)
fit &lt;- fit_three_layer(x, y, nmcmc = 2000)
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, entropy_limit = -0.5, cores = 1)
plot(fit)
i &lt;- interp::interp(xx[, 1], xx[, 2], fit$entropy)
image(i, col = heat.colors(128), main = "Entropy")

# Example 2: Vecchia approximated model (nugget fixed)
# (Vecchia approximation is faster for larger data sizes)
fit &lt;- fit_three_layer(x, y, nmcmc = 2000, vecchia = TRUE, 
                       m = 10, true_g = 1e-6)
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit)


</code></pre>


</div>