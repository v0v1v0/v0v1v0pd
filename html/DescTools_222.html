<div class="container">

<table style="width: 100%;"><tr>
<td>Entropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Shannon Entropy and Mutual Information

</h2>

<h3>Description</h3>

<p>Computes Shannon entropy and the mutual information of two variables. The entropy quantifies the expected value of the information contained in a vector. The mutual information is a quantity that measures the mutual dependence of the two random variables. 

</p>


<h3>Usage</h3>

<pre><code class="language-R">Entropy(x, y = NULL, base = 2, ...)

MutInf(x, y, base = 2, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a vector or a matrix of numerical or categorical type. If only x is supplied it will be interpreted as 
contingency table.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>a vector with the same type and dimension as x. If y is not <code>NULL</code> then the entropy of <code>table(x, y, ...)</code> 
will be calculated.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>base</code></td>
<td>
<p>base of the logarithm to be used, defaults to 2.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments are passed to the function <code>table</code>, allowing i.e. to set <code>useNA</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The Shannon entropy equation provides a way to estimate the average minimum number of bits needed to encode a string of symbols, based on the frequency of the symbols.<br> 
It is given by the formula <code class="reqn">H = - \sum(\pi log(\pi))</code> where <code class="reqn">\pi</code> is the 
probability of character number i showing up in a stream of characters of the given "script".<br>
The entropy is ranging from 0 to Inf.

</p>


<h3>Value</h3>

<p>a numeric value.





</p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;

</p>


<h3>References</h3>

<p>Shannon, Claude E. (July/October 1948). A Mathematical Theory of Communication, <em>Bell System Technical Journal</em> 27 (3): 379-423.
</p>
<p>Ihara, Shunsuke (1993) <em>Information theory for continuous systems</em>, World Scientific. p. 2. ISBN 978-981-02-0985-8. 
</p>



<h3>See Also</h3>

<p>package <span class="pkg">entropy</span> which implements various estimators of entropy

</p>


<h3>Examples</h3>

<pre><code class="language-R">
Entropy(as.matrix(rep(1/8, 8)))

# http://r.789695.n4.nabble.com/entropy-package-how-to-compute-mutual-information-td4385339.html
x &lt;- as.factor(c("a","b","a","c","b","c")) 
y &lt;- as.factor(c("b","a","a","c","c","b")) 

Entropy(table(x), base=exp(1))
Entropy(table(y), base=exp(1))
Entropy(x, y, base=exp(1))

# Mutual information is 
Entropy(table(x), base=exp(1)) + Entropy(table(y), base=exp(1)) - Entropy(x, y, base=exp(1))
MutInf(x, y, base=exp(1))

Entropy(table(x)) + Entropy(table(y)) - Entropy(x, y)
MutInf(x, y, base=2)

# http://en.wikipedia.org/wiki/Cluster_labeling
tab &lt;- matrix(c(60,10000,200,500000), nrow=2, byrow=TRUE)
MutInf(tab, base=2) 

d.frm &lt;- Untable(as.table(tab))
str(d.frm)
MutInf(d.frm[,1], d.frm[,2])

table(d.frm[,1], d.frm[,2])

MutInf(table(d.frm[,1], d.frm[,2]))


# Ranking mutual information can help to describe clusters
#
#   r.mi &lt;- MutInf(x, grp)
#   attributes(r.mi)$dimnames &lt;- attributes(tab)$dimnames
# 
#   # calculating ranks of mutual information
#   r.mi_r &lt;- apply( -r.mi, 2, rank, na.last=TRUE )
#   # show only first 6 ranks
#   r.mi_r6 &lt;- ifelse( r.mi_r &lt; 7, r.mi_r, NA) 
#   attributes(r.mi_r6)$dimnames &lt;- attributes(tab)$dimnames
#   r.mi_r6
</code></pre>


</div>