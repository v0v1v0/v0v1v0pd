<div class="container">

<table style="width: 100%;"><tr>
<td>diffpriv</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
<code>diffpriv</code>: practical differential privacy in R.</h2>

<h3>Description</h3>

<p>The <code>diffpriv</code> package is a collection of generic tools for privacy-aware
data science, under the formal framework of differential privacy. A
differentially-private mechanism can release responses to untrusted third
parties, models fit on privacy-sensitive data. Due to the formal worst-case
nature of the framework, however, mechanism development typically requires
theoretical analysis. <code>diffpriv</code> offers a turn-key approach to
differential privacy.
</p>


<h3>General-purpose mechanisms</h3>

<p>Differential privacy's popularity is owed in part to a number of generic
mechanisms for privatizing non-private target functions. Virtual S4
class <code>DPMech-class</code> captures common features of these mechanisms and
is superclass to:
</p>

<ul>
<li> <p><code>DPMechLaplace</code>: the Laplace mechanism of Dwork et al.
(2006) for releasing numeric vectors;
</p>
</li>
<li> <p><code>DPMechExponential</code>: the exponential mechanism of McSherry
and Talwar (2007) for releasing solutions to optimizations, over numeric or
non-numeric sets; and
</p>
</li>
<li>
<p> More mechanisms coming soon. Users can also develop new mechanisms by
subclassing <code>DPMech-class</code>.
</p>
</li>
</ul>
<p><code>DPMech-class</code>-derived objects are initialized with a problem-specific
non-private <code>target</code> function. Subsequently, the
<code>releaseResponse</code> method can privatize responses of <code>target</code>
on input datasets. The level of corresponding privatization depends on given
privacy parameters <code>DPParamsEps</code> or derived parameters object.
</p>


<h3>Privatize anything with sensitivity measurement</h3>

<p><code>diffpriv</code> mechanisms have in common a reliance on the 'sensitivity' of
<code>target</code> function to small changes to input datasets. This sensitivity
must be provably bounded for an application's <code>target</code> in order for
differential privacy to be proved, and is used to calibrate privacy-preserving
randomization. Unfortunately bounding sensitivity is often prohibitively
complex, for example if <code>target</code> is an arbitrary computer program. All
<code>DPMech-class</code> mechanisms offer a <code>sensitivitySampler</code>
method due to Rubinstein and Aldà (2017) that repeatedly probes <code>target</code>
to estimate sensitivity automatically. Mechanisms with estimated sensitivities
achieve a slightly weaker form of random differential privacy due to
Hall et al. (2013), but without any theoretical analysis necessary.
</p>


<h3>References</h3>

<p>Benjamin I. P. Rubinstein and Francesco Aldà. "Pain-Free Random Differential
Privacy with Sensitivity Sampling", accepted into the 34th International
Conference on Machine Learning (ICML'2017), May 2017.
</p>
<p>Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
"Calibrating noise to sensitivity in private data analysis." In Theory of
Cryptography Conference, pp. 265-284. Springer Berlin Heidelberg, 2006.
</p>
<p>Frank McSherry and Kunal Talwar. "Mechanism design via differential privacy."
In the 48th Annual IEEE Symposium on Foundations of Computer Science
(FOCS'07), pp. 94-103. IEEE, 2007.
</p>
<p>Rob Hall, Alessandro Rinaldo, and Larry Wasserman. "Random Differential
Privacy." Journal of Privacy and Confidentiality, 4(2), pp. 43-59, 2012.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## for full examples see the diffpriv vignette
vignette("diffpriv")

## End(Not run)

</code></pre>


</div>