<div class="container">

<table style="width: 100%;"><tr>
<td>pt_from_os</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Permutation Test from Outlier Scores</h2>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training (outlier) scores,
<code>os_train</code>, as the reference. The method checks whether the test
scores, <code>os_test</code>, are worse off relative to the training set.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pt_from_os(os_train, os_test, n_pt = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>os_train</code></td>
<td>
<p>Outlier scores in training (reference) set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>os_test</code></td>
<td>
<p>Outlier scores in test set.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The null distribution of the test statistic is based on <code>n_pt</code>
permutations. For speed, this is implemented as a sequential Monte Carlo test
with the <span class="pkg">simctest</span> package. See Gandy (2009) for details. The prefix
<em>pt</em> refers to permutation test. This approach does not use the
asymptotic null distribution for the test statistic. This is the recommended
approach for small samples. The test statistic is the weighted AUC (WAUC).
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li>
</ul>
<h3>Notes</h3>

<p>The outlier scores should all mimic out-of-sample behaviour. Mind that the
training scores are not in-sample and thus, biased (overfitted) while the
test scores are out-of-sample. The mismatch – in-sample versus out-of-sample
scores – voids the test validity. A simple fix for this is to get the
training scores from an indepedent (fresh) validation set; this follows
the train/validation/test sample splitting convention and the validation set
is effectively the reference set or distribution in this case.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[pt_oob()] for variant requiring a scoring function.
[at_from_os()] for asymptotic test with the outlier scores.
</p>
<p>Other permutation-test: 
<code>pt_oob()</code>,
<code>pt_refit()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(dsos)
set.seed(12345)
os_train &lt;- rnorm(n = 100)
os_test &lt;- rnorm(n = 100)
null_test &lt;- pt_from_os(os_train, os_test)
null_test


</code></pre>


</div>