<div class="container">

<table style="width: 100%;"><tr>
<td>dfr_adap_sgl</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit a DFR-aSGL model.</h2>

<h3>Description</h3>

<p>Adaptive Sparse-group lasso (aSGL) with DFR main fitting function. Supports both linear and logistic regression, both with dense and sparse matrix implementations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">dfr_adap_sgl(
  X,
  y,
  groups,
  type = "linear",
  lambda = "path",
  alpha = 0.95,
  gamma_1 = 0.1,
  gamma_2 = 0.1,
  max_iter = 5000,
  backtracking = 0.7,
  max_iter_backtracking = 100,
  tol = 1e-05,
  standardise = "l2",
  intercept = TRUE,
  path_length = 20,
  min_frac = 0.05,
  screen = TRUE,
  verbose = FALSE,
  v_weights = NULL,
  w_weights = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Input matrix of dimensions <code class="reqn">n \times p</code>. Can be a sparse matrix (using class <code>"sparseMatrix"</code> from the <code>Matrix</code> package).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Output vector of dimension <code class="reqn">n</code>. For <code>type="linear"</code> should be continuous and for <code>type="logistic"</code> should be a binary variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>groups</code></td>
<td>
<p>A grouping structure for the input data. Should take the form of a vector of group indices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>The type of regression to perform. Supported values are: <code>"linear"</code> and <code>"logistic"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The regularisation parameter. Defines the level of sparsity in the model. A higher value leads to sparser models:
</p>

<ul>
<li> <p><code>"path"</code> computes a path of regularisation parameters of length <code>"path_length"</code>. The path will begin just above the value at which the first predictor enters the model and will terminate at the value determined by <code>"min_frac"</code>.
</p>
</li>
<li>
<p> User-specified single value or sequence. Internal scaling is applied based on the type of standardisation. The returned <code>"lambda"</code> value will be the original unscaled value(s).
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The value of <code class="reqn">\alpha</code>, which defines the convex balance between the lasso and group lasso. Must be between 0 and 1. Recommended value is 0.95.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_1</code></td>
<td>
<p>Hyperparameter which determines the shape of the variable penalties.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_2</code></td>
<td>
<p>Hyperparameter which determines the shape of the group penalties.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>Maximum number of ATOS iterations to perform.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>backtracking</code></td>
<td>
<p>The backtracking parameter, <code class="reqn">\tau</code>, as defined in Pedregosa and Gidel (2018).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter_backtracking</code></td>
<td>
<p>Maximum number of backtracking line search iterations to perform per global iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Convergence tolerance for the stopping criteria.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardise</code></td>
<td>
<p>Type of standardisation to perform on <code>X</code>:
</p>

<ul>
<li> <p><code>"l2"</code> standardises the input data to have <code class="reqn">\ell_2</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/\sqrt{n}</code>.
</p>
</li>
<li> <p><code>"l1"</code> standardises the input data to have <code class="reqn">\ell_1</code> norms of one. When using this <code>"lambda"</code> is scaled internally by <code class="reqn">1/n</code>.
</p>
</li>
<li> <p><code>"sd"</code> standardises the input data to have standard deviation of one.
</p>
</li>
<li> <p><code>"none"</code> no standardisation applied.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Logical flag for whether to fit an intercept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>path_length</code></td>
<td>
<p>The number of <code class="reqn">\lambda</code> values to fit the model for. If <code>"lambda"</code> is user-specified, this is ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_frac</code></td>
<td>
<p>Smallest value of <code class="reqn">\lambda</code> as a fraction of the maximum value. That is, the final <code class="reqn">\lambda</code> will be <code>"min_frac"</code> of the first <code class="reqn">\lambda</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>screen</code></td>
<td>
<p>Logical flag for whether to apply the DFR screening rules (see Feser and Evangelou (2024)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical flag for whether to print fitting information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v_weights</code></td>
<td>
<p>Optional vector for the variable penalty weights. Overrides the adaptive SGL penalties if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w_weights</code></td>
<td>
<p>Optional vector for the group penalty weights. Overrides the adaptive SGL penalties if specified. When entering custom weights, these are multiplied internally by <code class="reqn">\lambda</code> and <code class="reqn">1-\alpha</code>. To void this behaviour, set <code class="reqn">\lambda = 2</code> and <code class="reqn">\alpha = 0.5</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>dfr_adap_sgl()</code> fits a DFR-aSGL model (Feser and Evangelou (2024)) using Adaptive Three Operator Splitting (ATOS) (Pedregosa and Gidel (2018)).
It solves the convex optimisation problem given by (Poignard (2020) and Mendez-Civieta et al. (2020))
</p>
<p style="text-align: center;"><code class="reqn">
  \frac{1}{2n} f(b ; y, \mathbf{X}) + \lambda \alpha \sum_{i=1}^{p}v_i |b_i| + \lambda (1-\alpha)\sum_{g=1}^{m} w_g \sqrt{p_g} \|b^{(g)}\|_2,
</code>
</p>

<p>where <code class="reqn">f(\cdot)</code> is the loss function, <code class="reqn">p_g</code> are the group sizes, and <code class="reqn">(v,w)</code> are adaptive weights. In the case of the linear model, the loss function is given by the mean-squared error loss:
</p>
<p style="text-align: center;"><code class="reqn">
 f(b; y, \mathbf{X}) = \left\|y-\mathbf{X}b \right\|_2^2.
</code>
</p>

<p>In the logistic model, the loss function is given by
</p>
<p style="text-align: center;"><code class="reqn">
f(b;y,\mathbf{X})=-1/n \log(\mathcal{L}(b; y, \mathbf{X})).
</code>
</p>

<p>where the log-likelihood is given by
</p>
<p style="text-align: center;"><code class="reqn">
 \mathcal{L}(b; y, \mathbf{X}) = \sum_{i=1}^{n}\left\{y_i b^\intercal x_i - \log(1+\exp(b^\intercal x_i)) \right\}.
</code>
</p>

<p>The adaptive weights are chosen as, for a group <code class="reqn">g</code> and variable <code class="reqn">i</code> (Mendez-Civieta et al. (2020))
</p>
<p style="text-align: center;"><code class="reqn">
   v_i = \frac{1}{|q_{1i}|^{\gamma_1}}, \; w_g = \frac{1}{\|q_1^{(g)}\|_2^{\gamma_2}},
</code>
</p>

<p>DFR uses the dual norm (the <code class="reqn">\epsilon</code>-norm) and the KKT conditions to discard features at <code class="reqn">\lambda_k</code> that would have been inactive at <code class="reqn">\lambda_{k+1}</code>.
It applies two layers of screening, so that it first screens out any groups that satisfy
</p>
<p style="text-align: center;"><code class="reqn">
\|\nabla_g f(\hat{\beta}(\lambda_{k}))\|_{\epsilon_g'} \leq \gamma_g(2\lambda_{k+1} - \lambda_k)
</code>
</p>

<p>and then screens out any variables that satisfy
</p>
<p style="text-align: center;"><code class="reqn">
|\nabla_i f(\hat{\beta}(\lambda_{k}))| \leq \alpha v_i (2\lambda_{k+1} - \lambda_k)
</code>
</p>

<p>leading to effective input dimensionality reduction. See Feser and Evangelou (2024) for full details.
</p>


<h3>Value</h3>

<p>A list containing:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>The fitted values from the regression. Taken to be the more stable fit between <code>x</code> and <code>z</code>, which is usually the former. A filter is applied to remove very small values, where ATOS has not been able to shrink exactly to zero. Check this against <code>x</code> and <code>z</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>The solution to the original problem (see Pedregosa and Gidel (2018)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>u</code></td>
<td>
<p>The solution to the dual problem (see Pedregosa and Gidel (2018)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>z</code></td>
<td>
<p>The updated values from applying the first proximal operator (see Pedregosa and Gidel (2018)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>Indicates which type of regression was performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Value(s) of <code class="reqn">\lambda</code> used to fit the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>success</code></td>
<td>
<p>Logical flag indicating whether ATOS converged, according to <code>tol</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_it</code></td>
<td>
<p>Number of iterations performed. If convergence is not reached, this will be <code>max_iter</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>certificate</code></td>
<td>
<p>Final value of convergence criteria.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>Logical flag indicating whether an intercept was fit.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Feser, F., Evangelou, M. (2024). <em>Dual feature reduction for the sparse-group lasso and its adaptive variant</em>, <a href="https://arxiv.org/abs/2405.17094">https://arxiv.org/abs/2405.17094</a>
</p>
<p>Mendez-Civieta, A., Carmen Aguilera-Morillo, M., Lillo, R. (2020). <em>Adaptive sparse group LASSO in quantile regression</em>, <a href="https://link.springer.com/article/10.1007/s11634-020-00413-8">https://link.springer.com/article/10.1007/s11634-020-00413-8</a>
</p>
<p>Pedregosa, F., Gidel, G. (2018). <em>Adaptive Three Operator Splitting</em>, <a href="https://proceedings.mlr.press/v80/pedregosa18a.html">https://proceedings.mlr.press/v80/pedregosa18a.html</a>
</p>
<p>Poignard, B. (2020). <em>Asymptotic theory of the adaptive Sparse Group Lasso</em>, <a href="https://link.springer.com/article/10.1007/s10463-018-0692-7">https://link.springer.com/article/10.1007/s10463-018-0692-7</a>
</p>


<h3>See Also</h3>

<p>Other SGL-methods: 
<code>dfr_adap_sgl.cv()</code>,
<code>dfr_sgl()</code>,
<code>dfr_sgl.cv()</code>,
<code>plot.sgl()</code>,
<code>predict.sgl()</code>,
<code>print.sgl()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># specify a grouping structure
groups = c(1,1,1,2,2,3,3,3,4,4)
# generate data
data = sgs::gen_toy_data(p=10, n=5, groups = groups, seed_id=3,group_sparsity=1)
# run DFR-aSGL 
model = dfr_adap_sgl(X = data$X, y = data$y, groups = groups, type="linear", path_length = 5, 
alpha=0.95, standardise = "l2", intercept = TRUE, verbose=FALSE)
</code></pre>


</div>