<div class="container">

<table style="width: 100%;"><tr>
<td>activation</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Activation function
</h2>

<h3>Description</h3>

<p>Different type of activation functions and the corresponding derivatives 
</p>


<h3>Usage</h3>

<pre><code class="language-R">  sigmoid(x)
  elu(x)
  relu(x)
  lrelu(x)
  idu(x)
  dsigmoid(y)
  delu(y)
  drelu(y)
  dlrelu(y)
  dtanh(y)   #activation function tanh(x) is already available in R
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>input of the activation function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>input of the derivative of the activation function</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Each function returns either the activation function (e.g. sigmoid, relu) or its derivative (e.g. dsigmoid, drelu).
</p>


<h3>Value</h3>


<p>An activation function is applied to x and returns a matrix the same size as x. 
The detail formula for each activation function is:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>sigmoid</code></td>
<td>
<p>return 1/(1+exp(-x))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>elu</code></td>
<td>
<p>return x for x&gt;0 and exp(x)-1 for x&lt;0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>relu</code></td>
<td>
<p>return x for x&gt;0 and 0 for x&lt;0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lrelu</code></td>
<td>
<p>return x for x&gt;0 and 0.1*x for x&lt;0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tanh</code></td>
<td>
<p>return tanh(x)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>idu</code></td>
<td>
<p>return (x)</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p><code>bwdNN</code>, 
<code>fwdNN</code>,
<code>dNNmodel</code>, 
<code>optimizerSGD</code>,
<code>optimizerNAG</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">  # Specify a dnn nodel with user define activation function in layer 2.
  softmax  = function(x) {log(1+exp(x))}    # y = log(1+exp(x))
  dsoftmax = function(y) {sigmoid(y)}       # x = exp(y)/(1+exp(y))
  model = dNNmodel(units=c(8, 6, 1), activation= c('relu', 'softmax', 'sigmoid'), 
          input_shape = c(3))
  print(model)
</code></pre>


</div>