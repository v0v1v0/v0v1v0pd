<div class="container">

<table style="width: 100%;"><tr>
<td>DebiasProg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The proposed debiasing (primal) program.</h2>

<h3>Description</h3>

<p>This function implements our proposed debiasing (primal) program that solves for
the weights for correcting the Lasso pilot estimate.
</p>


<h3>Usage</h3>

<pre><code class="language-R">DebiasProg(X, x, Pi, gamma_n = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>The input design n*d matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>The current query point, which is a 1*d array.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Pi</code></td>
<td>
<p>An n*n diagonal matrix with (estimated) propensity scores as its diagonal entries.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma_n</code></td>
<td>
<p>The regularization parameter "<code class="reqn">\gamma/n</code>". (Default: gamma_n=0.1.)</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The estimated weights by our debiasing program, which is a n-dim vector.
</p>


<h3>Author(s)</h3>

<p>Yikun Zhang, <a href="mailto:yikunzhang@foxmail.com">yikunzhang@foxmail.com</a>
</p>


<h3>References</h3>

<p>Zhang, Y., Giessing, A. and Chen, Y.-C. (2023)
<em>Efficient Inference on High-Dimensional Linear Model with Missing Outcomes.</em>
<a href="https://arxiv.org/abs/2309.06429">https://arxiv.org/abs/2309.06429</a>.
</p>
<p>Fu, A., Narasimhan, B. and Boyd, S. (2017) <em>CVXR: An R Package for Disciplined Convex Optimization.
Journal of Statistical Software 94 (14): 1â€“34.</em> doi: <a href="https://doi.org/10.18637/jss.v094.i14">10.18637/jss.v094.i14</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
  require(MASS)
  require(glmnet)
  d = 1000
  n = 900

  Sigma = array(0, dim = c(d,d)) + diag(d)
  rho = 0.1
  for(i in 1:(d-1)){
    for(j in (i+1):d){
      if ((j &lt; i+6) | (j &gt; i+d-6)){
        Sigma[i,j] = rho
        Sigma[j,i] = rho
      }
    }
  }
  sig = 1

  ## Current query point
  x_cur = rep(0, d)
  x_cur[c(1, 2, 3, 7, 8)] = c(1, 1/2, 1/4, 1/2, 1/8)
  x_cur = array(x_cur, dim = c(1,d))

  ## True regression coefficient
  s_beta = 5
  beta_0 = rep(0, d)
  beta_0[1:s_beta] = sqrt(5)

  ## Generate the design matrix and outcomes
  X_sim = mvrnorm(n, mu = rep(0, d), Sigma)
  eps_err_sim = sig * rnorm(n)
  Y_sim = drop(X_sim %*% beta_0) + eps_err_sim

  obs_prob = 1 / (1 + exp(-1 + X_sim[, 7] - X_sim[, 8]))
  R_sim = rep(1, n)
  R_sim[runif(n) &gt;= obs_prob] = 0

  ## Estimate the propensity scores via the Lasso-type generalized linear model
  zeta = 5*sqrt(log(d)/n)/n
  lr1 = glmnet(X_sim, R_sim, family = "binomial", alpha = 1, lambda = zeta,
               standardize = TRUE, thresh=1e-6)
  prop_score = drop(predict(lr1, newx = X_sim, type = "response"))

  ## Estimate the debiasing weights
  w_obs = DebiasProg(X_sim, x_cur, Pi=diag(prop_score), gamma_n = 0.1)


</code></pre>


</div>