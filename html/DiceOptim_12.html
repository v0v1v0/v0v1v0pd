<div class="container">

<table style="width: 100%;"><tr>
<td>DiceOptim-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kriging-based optimization methods for computer experiments</h2>

<h3>Description</h3>

<p>Sequential and parallel Kriging-based optimization methods relying on
expected improvement criteria.
</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;"> Package: </td>
<td style="text-align: left;"> DiceOptim</td>
</tr>
<tr>
<td style="text-align: left;"> Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;"> Version:
</td>
<td style="text-align: left;"> 2.0 </td>
</tr>
<tr>
<td style="text-align: left;"> Date: </td>
<td style="text-align: left;"> July 2016</td>
</tr>
<tr>
<td style="text-align: left;"> License: </td>
<td style="text-align: left;"> GPL-2 | GPL-3</td>
</tr>
<tr>
<td style="text-align: left;"> </td>
</tr>
</table>
<h3>Note</h3>

<p>This work is a follow-up of DiceOptim 1.0, which was produced within
the frame of the DICE (Deep Inside Computer Experiments) Consortium between
ARMINES, Renault, EDF, IRSN, ONERA and TOTAL S.A.
</p>
<p>The authors would like to thank Yves Deville for his precious advice in R
programming and packaging, as well as the DICE members for useful
feedbacks, and especially Yann Richet (IRSN) for numerous discussions
concerning the user-friendliness of this package.
</p>
<p>Package <code>rgenoud</code> &gt;=5.3.3. is recommended.
</p>
<p>Important functions or methods: </p>

<table>
<tr>
<td style="text-align: left;"> <code>EGO.nsteps</code> </td>
<td style="text-align: left;"> Standard Efficient Global Optimization algorithm with a fixed number of iterations (nsteps) </td>
</tr>
<tr>
<td style="text-align: left;"> 
</td>
<td style="text-align: left;">---with model updates including re-estimation of covariance hyperparameters </td>
</tr>
<tr>
<td style="text-align: left;">

<code>EI</code> </td>
<td style="text-align: left;"> Expected Improvement criterion (single infill point, noise-free, constraint free problems)</td>
</tr>
<tr>
<td style="text-align: left;">

<code>max_EI</code> </td>
<td style="text-align: left;"> Maximization of the EI criterion. No need to specify any objective function </td>
</tr>
<tr>
<td style="text-align: left;">

<code>qEI.nsteps</code> </td>
<td style="text-align: left;"> EGO algorithm with batch-sequential (parallel) infill strategy </td>
</tr>
<tr>
<td style="text-align: left;">

<code>noisy.optimizer</code> </td>
<td style="text-align: left;"> EGO algorithm for noisy objective functions </td>
</tr>
<tr>
<td style="text-align: left;">

<code>EGO.cst</code> </td>
<td style="text-align: left;"> EGO algorithm for (non-linear) constrained problems </td>
</tr>
<tr>
<td style="text-align: left;">

<code>easyEGO.cst</code> </td>
<td style="text-align: left;"> User-friendly wrapper for <code>EGO.cst</code>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Victor Picheny (INRA, Castanet-Tolosan, France)
</p>
<p>David Ginsbourger (Idiap Research Institute and University of Bern, Switzerland)
</p>
<p>Olivier Roustant (Mines Saint-Etienne, France).
</p>
<p>with contributions by M. Binois, C. Chevalier, S. Marmin and T. Wagner
</p>


<h3>References</h3>

<p>N.A.C. Cressie (1993), <em>Statistics for spatial data</em>,
Wiley series in probability and mathematical statistics.
</p>
<p>D. Ginsbourger (2009), <em>Multiples metamodeles pour l'approximation et
l'optimisation de fonctions numeriques multivariables</em>, Ph.D. thesis, Ecole
Nationale Superieure des Mines de Saint-Etienne, 2009.
<a href="https://tel.archives-ouvertes.fr/tel-00772384">https://tel.archives-ouvertes.fr/tel-00772384</a>
</p>
<p>D. Ginsbourger, R. Le Riche, and L. Carraro (2010), chapter "Kriging is
well-suited to parallelize optimization", in <em>Computational
Intelligence in Expensive Optimization Problems</em>, Studies in Evolutionary
Learning and Optimization, Springer.
</p>
<p>D.R. Jones (2001), A taxonomy of global optimization methods based on
response surfaces, <em>Journal of Global Optimization</em>, 21, 345-383.
</p>
<p>D.R. Jones, M. Schonlau, and W.J. Welch (1998), Efficient global
optimization of expensive black-box functions, <em>Journal of Global
Optimization</em>, 13, 455-492.
</p>
<p>W.R. Jr. Mebane and J.S. Sekhon (2011), Genetic optimization
using derivatives: The rgenoud package for R, <em>Journal of Statistical
Software</em>, <b>51</b>(1), 1-55, <a href="https://www.jstatsoft.org/v51/i01/">https://www.jstatsoft.org/v51/i01/</a>.
</p>
<p>J. Mockus (1988), <em>Bayesian Approach to Global Optimization</em>. Kluwer
academic publishers.
</p>
<p>V. Picheny and D. Ginsbourger (2013), Noisy kriging-based optimization
methods: A unified implementation within the DiceOptim package,
<em>Computational Statistics &amp; Data Analysis</em>, 71, 1035-1053. 
</p>
<p>C.E. Rasmussen and C.K.I. Williams (2006), <em>Gaussian Processes for
Machine Learning</em>, the MIT Press, <a href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a>
</p>
<p>B.D. Ripley (1987), <em>Stochastic Simulation</em>, Wiley.
</p>
<p>O. Roustant, D. Ginsbourger and Yves Deville (2012), DiceKriging,
DiceOptim: Two R Packages for the Analysis of Computer Experiments by
Kriging-Based Metamodeling and Optimization, <em>Journal of Statistical
Software</em>, <b>42</b>(11), 1â€“26, <a href="https://www.jstatsoft.org/article/view/v042i11">https://www.jstatsoft.org/article/view/v042i11</a>.
</p>
<p>T.J. Santner, B.J. Williams, and W.J. Notz (2003), <em>The design and
analysis of computer experiments</em>, Springer.
</p>
<p>M. Schonlau (1997), <em>Computer experiments and global optimization</em>,
Ph.D. thesis, University of Waterloo.
</p>


<h3>Examples</h3>

<pre><code class="language-R"> 
set.seed(123)

###############################################################
###	2D optimization USING EGO.nsteps and qEGO.nsteps   ########
###############################################################

# a 9-points factorial design, and the corresponding response
d &lt;- 2
n &lt;- 9
design.fact &lt;- expand.grid(seq(0,1,length=3), seq(0,1,length=3)) 
names(design.fact)&lt;-c("x1", "x2")
design.fact &lt;- data.frame(design.fact) 
names(design.fact)&lt;-c("x1", "x2")
response.branin &lt;- data.frame(apply(design.fact, 1, branin))
names(response.branin) &lt;- "y" 

# model identification
fitted.model1 &lt;- km(~1, design=design.fact, response=response.branin, 
covtype="gauss", control=list(pop.size=50,trace=FALSE), parinit=c(0.5, 0.5))

### EGO, 5 steps ##################
library(rgenoud)
nsteps &lt;- 5
lower &lt;- rep(0,d) 
upper &lt;- rep(1,d)     
oEGO &lt;- EGO.nsteps(model=fitted.model1, fun=branin, nsteps=nsteps, 
lower=lower, upper=upper, control=list(pop.size=20, BFGSburnin=2))
print(oEGO$par)
print(oEGO$value)

# graphics
n.grid &lt;- 15
x.grid &lt;- y.grid &lt;- seq(0,1,length=n.grid)
design.grid &lt;- expand.grid(x.grid, y.grid)
response.grid &lt;- apply(design.grid, 1, branin)
z.grid &lt;- matrix(response.grid, n.grid, n.grid)
contour(x.grid, y.grid, z.grid, 40)
title("EGO")
points(design.fact[,1], design.fact[,2], pch=17, col="blue")
points(oEGO$par, pch=19, col="red")
text(oEGO$par[,1], oEGO$par[,2], labels=1:nsteps, pos=3)

### Parallel EGO, 3 steps with batches of 3 ##############
nsteps &lt;- 3
lower &lt;- rep(0,d) 
upper &lt;- rep(1,d)
npoints &lt;- 3 # The batchsize
oEGO &lt;- qEGO.nsteps(model = fitted.model1, branin, npoints = npoints, nsteps = nsteps,
crit="exact", lower, upper, optimcontrol = NULL)
print(oEGO$par)
print(oEGO$value)

# graphics
contour(x.grid, y.grid, z.grid, 40)
title("qEGO")
points(design.fact[,1], design.fact[,2], pch=17, col="blue")
points(oEGO$par, pch=19, col="red")
text(oEGO$par[,1], oEGO$par[,2], labels=c(tcrossprod(rep(1,npoints),1:nsteps)), pos=3)

##########################################################################
### 2D OPTIMIZATION, NOISY OBJECTIVE                                   ###
##########################################################################

set.seed(10)
library(DiceDesign)
# Set test problem parameters
doe.size &lt;- 9
dim &lt;- 2
test.function &lt;- get("branin2")
lower &lt;- rep(0,1,dim)
upper &lt;- rep(1,1,dim)
noise.var &lt;- 0.1

# Build noisy simulator
funnoise &lt;- function(x)
{     f.new &lt;- test.function(x) + sqrt(noise.var)*rnorm(n=1)
      return(f.new)}

# Generate DOE and response
doe &lt;- as.data.frame(lhsDesign(doe.size, dim)$design)
y.tilde &lt;- funnoise(doe)

# Create kriging model
model &lt;- km(y~1, design=doe, response=data.frame(y=y.tilde),
     covtype="gauss", noise.var=rep(noise.var,1,doe.size), 
     lower=rep(.1,dim), upper=rep(1,dim), control=list(trace=FALSE))

# Optimisation with noisy.optimizer
optim.param &lt;- list()
optim.param$quantile &lt;- .7
optim.result &lt;- noisy.optimizer(optim.crit="EQI", optim.param=optim.param, model=model,
		n.ite=5, noise.var=noise.var, funnoise=funnoise, lower=lower, upper=upper,
		NoiseReEstimate=FALSE, CovReEstimate=FALSE)

print(optim.result$best.x)

##########################################################################
### 2D OPTIMIZATION, 2 INEQUALITY CONSTRAINTS                          ###
##########################################################################
set.seed(25468)
library(DiceDesign)

fun &lt;- goldsteinprice
fun1.cst &lt;- function(x){return(-branin(x) + 25)}
fun2.cst &lt;- function(x){return(3/2 - x[1] - 2*x[2] - .5*sin(2*pi*(x[1]^2 - 2*x[2])))}
constraint &lt;- function(x){return(c(fun1.cst(x), fun2.cst(x)))}

lower &lt;- rep(0, 2)
upper &lt;- rep(1, 2)

## Optimization using the Expected Feasible Improvement criterion
res &lt;- easyEGO.cst(fun=fun, constraint=constraint, n.cst=2, lower=lower, upper=upper, budget=10, 
                   control=list(method="EFI", inneroptim="genoud", maxit=20))

cat("best design found:", res$par, "\n")
cat("corresponding objective and constraints:", res$value, "\n")

# Objective function in colour, constraint boundaries in red
# Initial DoE: white circles, added points: blue crosses, best solution: red cross

n.grid &lt;- 15
test.grid &lt;- expand.grid(X1 = seq(0, 1, length.out = n.grid), X2 = seq(0, 1, length.out = n.grid))
obj.grid &lt;- apply(test.grid, 1, fun)
cst1.grid &lt;- apply(test.grid, 1, fun1.cst)
cst2.grid &lt;- apply(test.grid, 1, fun2.cst)
filled.contour(seq(0, 1, length.out = n.grid), seq(0, 1, length.out = n.grid), nlevels = 50,
               matrix(obj.grid, n.grid), main = "Two inequality constraints",
               xlab = expression(x[1]), ylab = expression(x[2]), color = terrain.colors, 
               plot.axes = {axis(1); axis(2);
                            contour(seq(0, 1, length.out = n.grid), seq(0, 1, length.out = n.grid), 
                                    matrix(cst1.grid, n.grid), level = 0, add=TRUE,
                                    drawlabels=FALSE, lwd=1.5, col = "red")
                            contour(seq(0, 1, length.out = n.grid), seq(0, 1, length.out = n.grid), 
                                    matrix(cst2.grid, n.grid), level = 0, add=TRUE,drawlabels=FALSE,
                                    lwd=1.5, col = "red")
                            points(res$history$X, col = "blue", pch = 4, lwd = 2)       
                            points(res$par[1], res$par[2], col = "red", pch = 4, lwd = 2, cex=2) 
               }
)
</code></pre>


</div>