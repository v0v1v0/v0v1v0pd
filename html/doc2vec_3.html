<div class="container">

<table style="width: 100%;"><tr>
<td>paragraph2vec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Train a paragraph2vec also known as doc2vec model on text</h2>

<h3>Description</h3>

<p>Construct a paragraph2vec model on text. 
The algorithm is explained at <a href="https://arxiv.org/pdf/1405.4053.pdf">https://arxiv.org/pdf/1405.4053.pdf</a>.
People also refer to this model as doc2vec.<br>
The model is an extension to the word2vec algorithm, 
where an additional vector for every paragraph is added directly in the training.
</p>


<h3>Usage</h3>

<pre><code class="language-R">paragraph2vec(
  x,
  type = c("PV-DBOW", "PV-DM"),
  dim = 50,
  window = ifelse(type == "PV-DM", 5L, 10L),
  iter = 5L,
  lr = 0.05,
  hs = FALSE,
  negative = 5L,
  sample = 0.001,
  min_count = 5L,
  threads = 1L,
  encoding = "UTF-8",
  embeddings = matrix(nrow = 0, ncol = dim),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a data.frame with columns doc_id and text or the path to the file on disk containing training data.<br>
Note that the text column should be of type character, should contain less than 1000 words where space or tab is 
used as a word separator and that the text should not contain newline characters as these are considered document delimiters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>character string with the type of algorithm to use, either one of
</p>

<ul>
<li>
<p>'PV-DM': Distributed Memory paragraph vectors
</p>
</li>
<li>
<p>'PV-DBOW': Distributed Bag Of Words paragraph vectors
</p>
</li>
</ul>
<p>Defaults to 'PV-DBOW'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>dimension of the word and paragraph vectors. Defaults to 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>window</code></td>
<td>
<p>skip length between words. Defaults to 10 for PV-DM and 5 for PV-DBOW</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>number of training iterations. Defaults to 20.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>initial learning rate also known as alpha. Defaults to 0.05</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hs</code></td>
<td>
<p>logical indicating to use hierarchical softmax instead of negative sampling. Defaults to FALSE indicating to do negative sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>negative</code></td>
<td>
<p>integer with the number of negative samples. Only used in case hs is set to FALSE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample</code></td>
<td>
<p>threshold for occurrence of words. Defaults to 0.001</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_count</code></td>
<td>
<p>integer indicating the number of time a word should occur to be considered as part of the training vocabulary. Defaults to 5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threads</code></td>
<td>
<p>number of CPU threads to use. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>encoding</code></td>
<td>
<p>the encoding of <code>x</code> and <code>stopwords</code>. Defaults to 'UTF-8'. 
Calculating the model always starts from files allowing to build a model on large corpora. The encoding argument 
is passed on to <code>file</code> when writing <code>x</code> to hard disk in case you provided it as a data.frame.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>embeddings</code></td>
<td>
<p>optionally a matrix with pretrained word embeddings which will be used to initialise the word embedding space with (transfer learning). 
The rownames of this matrix should consist of words. Only words overlapping with the vocabulary extracted from <code>x</code> will be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments passed on to the C++ function <code>paragraph2vec_train</code> - for expert use only</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>an object of class <code>paragraph2vec_trained</code> which is a list with elements 
</p>

<ul>
<li>
<p>model: a Rcpp pointer to the model
</p>
</li>
<li>
<p>data: a list with elements file: the training data used, n (the number of words in the training data), n_vocabulary (number of words in the vocabulary) and n_docs (number of documents)
</p>
</li>
<li>
<p>control: a list of the training arguments used, namely min_count, dim, window, iter, lr, skipgram, hs, negative, sample
</p>
</li>
</ul>
<h3>References</h3>

<p><a href="https://arxiv.org/pdf/1405.4053.pdf">https://arxiv.org/pdf/1405.4053.pdf</a>, <a href="https://groups.google.com/g/word2vec-toolkit/c/Q49FIrNOQRo/m/J6KG8mUj45sJ">https://groups.google.com/g/word2vec-toolkit/c/Q49FIrNOQRo/m/J6KG8mUj45sJ</a>
</p>


<h3>See Also</h3>

<p><code>predict.paragraph2vec</code>, <code>as.matrix.paragraph2vec</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(tokenizers.bpe)
## Take data and standardise it a bit
data(belgium_parliament, package = "tokenizers.bpe")
str(belgium_parliament)
x &lt;- subset(belgium_parliament, language %in% "french")
x$text   &lt;- tolower(x$text)
x$text   &lt;- gsub("[^[:alpha:]]", " ", x$text)
x$text   &lt;- gsub("[[:space:]]+", " ", x$text)
x$text   &lt;- trimws(x$text)
x$nwords &lt;- txt_count_words(x$text)
x &lt;- subset(x, nwords &lt; 1000 &amp; nchar(text) &gt; 0)

## Build the model
model &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 15,  iter = 5)

model &lt;- paragraph2vec(x = x, type = "PV-DBOW", dim = 100, iter = 20)

str(model)
embedding &lt;- as.matrix(model, which = "words")
embedding &lt;- as.matrix(model, which = "docs")
head(embedding)

## Get vocabulary
vocab &lt;- summary(model, type = "vocabulary",  which = "docs")
vocab &lt;- summary(model, type = "vocabulary",  which = "words")


## Transfer learning using existing word embeddings
library(word2vec)
w2v   &lt;- word2vec(x$text, dim = 50, type = "cbow", iter = 20, min_count = 5)
emb   &lt;- as.matrix(w2v)
model &lt;- paragraph2vec(x = x, dim = 50, type = "PV-DM", iter = 20, min_count = 5, 
                       embeddings = emb)


## Transfer learning - proof of concept without learning (iter=0, set to higher to learn)
emb       &lt;- matrix(rnorm(30), nrow = 2, dimnames = list(c("en", "met")))
model     &lt;- paragraph2vec(x = x, type = "PV-DM",   dim = 15,  iter = 0, embeddings = emb)
embedding &lt;- as.matrix(model, which = "words", normalize = FALSE)
embedding[c("en", "met"), ]
emb

</code></pre>


</div>