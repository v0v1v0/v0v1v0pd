<div class="container">

<table style="width: 100%;"><tr>
<td>DoubleML</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Abstract class DoubleML</h2>

<h3>Description</h3>

<p>Abstract base class that can't be initialized.
</p>


<h3>Format</h3>

<p>R6::R6Class object.
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>all_coef</code></dt>
<dd>
<p>(<code>matrix()</code>) <br>
Estimates of the causal parameter(s) for the <code>n_rep</code> different sample
splits after calling <code>fit()</code>.</p>
</dd>
<dt><code>all_dml1_coef</code></dt>
<dd>
<p>(<code>array()</code>) <br>
Estimates of the causal parameter(s) for the <code>n_rep</code> different sample
splits after calling <code>fit()</code> with <code>dml_procedure = "dml1"</code>.</p>
</dd>
<dt><code>all_se</code></dt>
<dd>
<p>(<code>matrix()</code>) <br>
Standard errors of the causal parameter(s) for the <code>n_rep</code> different
sample splits after calling <code>fit()</code>.</p>
</dd>
<dt><code>apply_cross_fitting</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates whether cross-fitting should be applied. Default is <code>TRUE</code>.</p>
</dd>
<dt><code>boot_coef</code></dt>
<dd>
<p>(<code>matrix()</code>) <br>
Bootstrapped coefficients for the causal parameter(s) after calling
<code>fit()</code> and <code>bootstrap()</code>.</p>
</dd>
<dt><code>boot_t_stat</code></dt>
<dd>
<p>(<code>matrix()</code>) <br>
Bootstrapped t-statistics for the causal parameter(s) after calling
<code>fit()</code> and <code>bootstrap()</code>.</p>
</dd>
<dt><code>coef</code></dt>
<dd>
<p>(<code>numeric()</code>) <br>
Estimates for the causal parameter(s) after calling <code>fit()</code>.</p>
</dd>
<dt><code>data</code></dt>
<dd>
<p>(<code>data.table</code>)<br>
Data object.</p>
</dd>
<dt><code>dml_procedure</code></dt>
<dd>
<p>(<code>character(1)</code>) <br>
A <code>character()</code> (<code>"dml1"</code> or <code>"dml2"</code>) specifying the double machine
learning algorithm. Default is <code>"dml2"</code>.</p>
</dd>
<dt><code>draw_sample_splitting</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates whether the sample splitting should be drawn during
initialization of the object. Default is <code>TRUE</code>.</p>
</dd>
<dt><code>learner</code></dt>
<dd>
<p>(named <code>list()</code>) <br>
The machine learners for the nuisance functions.</p>
</dd>
<dt><code>n_folds</code></dt>
<dd>
<p>(<code>integer(1)</code>) <br>
Number of folds. Default is <code>5</code>.</p>
</dd>
<dt><code>n_rep</code></dt>
<dd>
<p>(<code>integer(1)</code>) <br>
Number of repetitions for the sample splitting. Default is <code>1</code>.</p>
</dd>
<dt><code>params</code></dt>
<dd>
<p>(named <code>list()</code>) <br>
The hyperparameters of the learners.</p>
</dd>
<dt><code>psi</code></dt>
<dd>
<p>(<code>array()</code>) <br>
Value of the score function
<code class="reqn">\psi(W;\theta, \eta)=\psi_a(W;\eta) \theta + \psi_b (W; \eta)</code>
after calling <code>fit()</code>.</p>
</dd>
<dt><code>psi_a</code></dt>
<dd>
<p>(<code>array()</code>) <br>
Value of the score function component <code class="reqn">\psi_a(W;\eta)</code> after
calling <code>fit()</code>.</p>
</dd>
<dt><code>psi_b</code></dt>
<dd>
<p>(<code>array()</code>) <br>
Value of the score function component <code class="reqn">\psi_b(W;\eta)</code> after
calling <code>fit()</code>.</p>
</dd>
<dt><code>predictions</code></dt>
<dd>
<p>(<code>array()</code>) <br>
Predictions of the nuisance models after calling
<code>fit(store_predictions=TRUE)</code>.</p>
</dd>
<dt><code>models</code></dt>
<dd>
<p>(<code>array()</code>) <br>
The fitted nuisance models after calling
<code>fit(store_models=TRUE)</code>.</p>
</dd>
<dt><code>pval</code></dt>
<dd>
<p>(<code>numeric()</code>) <br>
p-values for the causal parameter(s) after calling <code>fit()</code>.</p>
</dd>
<dt><code>score</code></dt>
<dd>
<p>(<code>character(1)</code>, <code style="white-space: pre;">⁠function()⁠</code>) <br>
A <code>character(1)</code> or <code style="white-space: pre;">⁠function()⁠</code> specifying the score function.</p>
</dd>
<dt><code>se</code></dt>
<dd>
<p>(<code>numeric()</code>) <br>
Standard errors for the causal parameter(s) after calling <code>fit()</code>.</p>
</dd>
<dt><code>smpls</code></dt>
<dd>
<p>(<code>list()</code>) <br>
The partition used for cross-fitting.</p>
</dd>
<dt><code>smpls_cluster</code></dt>
<dd>
<p>(<code>list()</code>) <br>
The partition of clusters used for cross-fitting.</p>
</dd>
<dt><code>t_stat</code></dt>
<dd>
<p>(<code>numeric()</code>) <br>
t-statistics for the causal parameter(s) after calling <code>fit()</code>.</p>
</dd>
<dt><code>tuning_res</code></dt>
<dd>
<p>(named <code>list()</code>) <br>
Results from hyperparameter tuning.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-DoubleML-new"><code>DoubleML$new()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-print"><code>DoubleML$print()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-fit"><code>DoubleML$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-bootstrap"><code>DoubleML$bootstrap()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-split_samples"><code>DoubleML$split_samples()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-set_sample_splitting"><code>DoubleML$set_sample_splitting()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-tune"><code>DoubleML$tune()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-summary"><code>DoubleML$summary()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-confint"><code>DoubleML$confint()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-learner_names"><code>DoubleML$learner_names()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-params_names"><code>DoubleML$params_names()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-set_ml_nuisance_params"><code>DoubleML$set_ml_nuisance_params()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-p_adjust"><code>DoubleML$p_adjust()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-get_params"><code>DoubleML$get_params()</code></a>
</p>
</li>
<li> <p><a href="#method-DoubleML-clone"><code>DoubleML$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-DoubleML-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>DoubleML is an abstract class that can't be initialized.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$new()</pre></div>


<hr>
<a id="method-DoubleML-print"></a>



<h4>Method <code>print()</code>
</h4>

<p>Print DoubleML objects.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$print()</pre></div>


<hr>
<a id="method-DoubleML-fit"></a>



<h4>Method <code>fit()</code>
</h4>

<p>Estimate DoubleML models.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$fit(store_predictions = FALSE, store_models = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>store_predictions</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates whether the predictions for the nuisance functions should be
stored in field <code>predictions</code>. Default is <code>FALSE</code>.</p>
</dd>
<dt><code>store_models</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates whether the fitted models for the nuisance functions should be
stored in field <code>models</code> if you want to analyze the models or extract
information like variable importance. Default is <code>FALSE</code>.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-DoubleML-bootstrap"></a>



<h4>Method <code>bootstrap()</code>
</h4>

<p>Multiplier bootstrap for DoubleML models.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$bootstrap(method = "normal", n_rep_boot = 500)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>method</code></dt>
<dd>
<p>(<code>character(1)</code>) <br>
A <code>character(1)</code> (<code>"Bayes"</code>, <code>"normal"</code> or <code>"wild"</code>) specifying the
multiplier bootstrap method.</p>
</dd>
<dt><code>n_rep_boot</code></dt>
<dd>
<p>(<code>integer(1)</code>) <br>
The number of bootstrap replications.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-DoubleML-split_samples"></a>



<h4>Method <code>split_samples()</code>
</h4>

<p>Draw sample splitting for DoubleML models.
</p>
<p>The samples are drawn according to the attributes <code>n_folds</code>, <code>n_rep</code>
and <code>apply_cross_fitting</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$split_samples()</pre></div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-DoubleML-set_sample_splitting"></a>



<h4>Method <code>set_sample_splitting()</code>
</h4>

<p>Set the sample splitting for DoubleML models.
</p>
<p>The attributes <code>n_folds</code> and <code>n_rep</code> are derived from the provided
partition.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$set_sample_splitting(smpls)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>smpls</code></dt>
<dd>
<p>(<code>list()</code>) <br>
A nested <code>list()</code>. The outer lists needs to provide an entry per
repeated sample splitting (length of the list is set as <code>n_rep</code>).
The inner list is a named <code>list()</code> with names <code>train_ids</code> and <code>test_ids</code>.
The entries in <code>train_ids</code> and <code>test_ids</code> must be partitions per fold
(length of <code>train_ids</code> and <code>test_ids</code> is set as <code>n_folds</code>).</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>self
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>library(DoubleML)
library(mlr3)
set.seed(2)
obj_dml_data = make_plr_CCDDHNR2018(n_obs=10)
dml_plr_obj = DoubleMLPLR$new(obj_dml_data,
                              lrn("regr.rpart"), lrn("regr.rpart"))

# simple sample splitting with two folds and without cross-fitting
smpls = list(list(train_ids = list(c(1, 2, 3, 4, 5)),
                  test_ids = list(c(6, 7, 8, 9, 10))))
dml_plr_obj$set_sample_splitting(smpls)

# sample splitting with two folds and cross-fitting but no repeated cross-fitting
smpls = list(list(train_ids = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)),
                  test_ids = list(c(6, 7, 8, 9, 10), c(1, 2, 3, 4, 5))))
dml_plr_obj$set_sample_splitting(smpls)

# sample splitting with two folds and repeated cross-fitting with n_rep = 2
smpls = list(list(train_ids = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)),
                  test_ids = list(c(6, 7, 8, 9, 10), c(1, 2, 3, 4, 5))),
             list(train_ids = list(c(1, 3, 5, 7, 9), c(2, 4, 6, 8, 10)),
                  test_ids = list(c(2, 4, 6, 8, 10), c(1, 3, 5, 7, 9))))
dml_plr_obj$set_sample_splitting(smpls)
</pre>
</div>


<hr>
<a id="method-DoubleML-tune"></a>



<h4>Method <code>tune()</code>
</h4>

<p>Hyperparameter-tuning for DoubleML models.
</p>
<p>The hyperparameter-tuning is performed using the tuning methods provided
in the <a href="https://mlr3tuning.mlr-org.com/">mlr3tuning</a> package. For more
information on tuning in <a href="https://mlr3.mlr-org.com/">mlr3</a>, we refer to
the section on parameter tuning in the
<a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html">mlr3 book</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$tune(
  param_set,
  tune_settings = list(n_folds_tune = 5, rsmp_tune = mlr3::rsmp("cv", folds = 5), measure
    = NULL, terminator = mlr3tuning::trm("evals", n_evals = 20), algorithm =
    mlr3tuning::tnr("grid_search"), resolution = 5),
  tune_on_folds = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>param_set</code></dt>
<dd>
<p>(named <code>list()</code>) <br>
A named <code>list</code> with a parameter grid for each nuisance model/learner
(see method <code>learner_names()</code>). The parameter grid must be an object of
class ParamSet.</p>
</dd>
<dt><code>tune_settings</code></dt>
<dd>
<p>(named <code>list()</code>) <br>
A named <code>list()</code> with arguments passed to the hyperparameter-tuning with
<a href="https://mlr3tuning.mlr-org.com/">mlr3tuning</a> to set up
TuningInstance objects.
<code>tune_settings</code> has entries
</p>

<ul>
<li> <p><code>terminator</code> (Terminator) <br>
A Terminator object. Specification of <code>terminator</code>
is required to perform tuning.
</p>
</li>
<li> <p><code>algorithm</code> (Tuner or <code>character(1)</code>) <br>
A Tuner object (recommended) or key passed to the
respective dictionary to specify the tuning algorithm used in
tnr(). <code>algorithm</code> is passed as an argument to
tnr(). If <code>algorithm</code> is not specified by the users,
default is set to <code>"grid_search"</code>. If set to <code>"grid_search"</code>, then
additional argument <code>"resolution"</code> is required.
</p>
</li>
<li> <p><code>rsmp_tune</code> (Resampling or <code>character(1)</code>)<br>
A Resampling object (recommended) or option passed
to rsmp() to initialize a
Resampling for parameter tuning in <code>mlr3</code>.
If not specified by the user, default is set to <code>"cv"</code>
(cross-validation).
</p>
</li>
<li> <p><code>n_folds_tune</code> (<code>integer(1)</code>, optional) <br>
If <code>rsmp_tune = "cv"</code>, number of folds used for cross-validation.
If not specified by the user, default is set to <code>5</code>.
</p>
</li>
<li> <p><code>measure</code> (<code>NULL</code>, named <code>list()</code>, optional) <br>
Named list containing the measures used for parameter tuning. Entries in
list must either be Measure objects or keys to be
passed to passed to msr(). The names of the entries must
match the learner names (see method <code>learner_names()</code>). If set to <code>NULL</code>,
default measures are used, i.e., <code>"regr.mse"</code> for continuous outcome
variables and <code>"classif.ce"</code> for binary outcomes.
</p>
</li>
<li> <p><code>resolution</code> (<code>character(1)</code>) <br> The key passed to the respective
dictionary to specify  the tuning algorithm used in
tnr(). <code>resolution</code> is passed as an argument to
tnr().
</p>
</li>
</ul>
</dd>
<dt><code>tune_on_folds</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates whether the tuning should be done fold-specific or globally.
Default is <code>FALSE</code>.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-DoubleML-summary"></a>



<h4>Method <code>summary()</code>
</h4>

<p>Summary for DoubleML models after calling <code>fit()</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$summary(digits = max(3L, getOption("digits") - 3L))</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>digits</code></dt>
<dd>
<p>(<code>integer(1)</code>) <br>
The number of significant digits to use when printing.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-DoubleML-confint"></a>



<h4>Method <code>confint()</code>
</h4>

<p>Confidence intervals for DoubleML models.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$confint(parm, joint = FALSE, level = 0.95)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>parm</code></dt>
<dd>
<p>(<code>numeric()</code> or <code>character()</code>) <br>
A specification of which parameters are to be given confidence intervals
among the variables for which inference was done, either a vector of
numbers or a vector of names. If missing, all parameters are considered
(default).</p>
</dd>
<dt><code>joint</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates whether joint confidence intervals are computed.
Default is <code>FALSE</code>.</p>
</dd>
<dt><code>level</code></dt>
<dd>
<p>(<code>numeric(1)</code>) <br>
The confidence level. Default is <code>0.95</code>.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>A <code>matrix()</code> with the confidence interval(s).
</p>


<hr>
<a id="method-DoubleML-learner_names"></a>



<h4>Method <code>learner_names()</code>
</h4>

<p>Returns the names of the learners.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$learner_names()</pre></div>



<h5>Returns</h5>

<p><code>character()</code> with names of learners.
</p>


<hr>
<a id="method-DoubleML-params_names"></a>



<h4>Method <code>params_names()</code>
</h4>

<p>Returns the names of the nuisance models with hyperparameters.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$params_names()</pre></div>



<h5>Returns</h5>

<p><code>character()</code> with names of nuisance models with hyperparameters.
</p>


<hr>
<a id="method-DoubleML-set_ml_nuisance_params"></a>



<h4>Method <code>set_ml_nuisance_params()</code>
</h4>

<p>Set hyperparameters for the nuisance models of DoubleML models.
</p>
<p>Note that in the current implementation, either all parameters have to
be set globally or all parameters have to be provided fold-specific.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$set_ml_nuisance_params(
  learner = NULL,
  treat_var = NULL,
  params,
  set_fold_specific = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learner</code></dt>
<dd>
<p>(<code>character(1)</code>) <br>
The nuisance model/learner (see method <code>params_names</code>).</p>
</dd>
<dt><code>treat_var</code></dt>
<dd>
<p>(<code>character(1)</code>) <br>
The treatment varaible (hyperparameters can be set treatment-variable
specific).</p>
</dd>
<dt><code>params</code></dt>
<dd>
<p>(named <code>list()</code>) <br>
A named <code>list()</code> with estimator parameters. Parameters are used for all
folds by default. Alternatively, parameters can be passed in a
fold-specific way if option  <code>fold_specific</code>is <code>TRUE</code>. In this case, the
outer list needs to be of length <code>n_rep</code> and the inner list of length
<code>n_folds</code>.</p>
</dd>
<dt><code>set_fold_specific</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates if the parameters passed in <code>params</code> should be passed in
fold-specific way. Default is <code>FALSE</code>. If <code>TRUE</code>, the outer list needs
to be of length <code>n_rep</code> and the inner list of length <code>n_folds</code>.
Note that in the current implementation, either all parameters have to
be set globally or all parameters have to be provided fold-specific.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>self
</p>


<hr>
<a id="method-DoubleML-p_adjust"></a>



<h4>Method <code>p_adjust()</code>
</h4>

<p>Multiple testing adjustment for DoubleML models.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$p_adjust(method = "romano-wolf", return_matrix = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>method</code></dt>
<dd>
<p>(<code>character(1)</code>) <br>
A <code>character(1)</code>(<code>"romano-wolf"</code>, <code>"bonferroni"</code>, <code>"holm"</code>, etc)
specifying the adjustment method. In addition to <code>"romano-wolf"</code>,
all methods implemented in p.adjust() can be
applied. Default is <code>"romano-wolf"</code>.</p>
</dd>
<dt><code>return_matrix</code></dt>
<dd>
<p>(<code>logical(1)</code>) <br>
Indicates if the output is returned as a matrix with corresponding
coefficient names.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>numeric()</code> with adjusted p-values. If <code>return_matrix = TRUE</code>,
a <code>matrix()</code> with adjusted p_values.
</p>


<hr>
<a id="method-DoubleML-get_params"></a>



<h4>Method <code>get_params()</code>
</h4>

<p>Get hyperparameters for the nuisance model of DoubleML models.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$get_params(learner)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learner</code></dt>
<dd>
<p>(<code>character(1)</code>) <br>
The nuisance model/learner (see method <code>params_names()</code>)</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>named <code>list()</code>with paramers for the nuisance model/learner.
</p>


<hr>
<a id="method-DoubleML-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DoubleML$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>See Also</h3>

<p>Other DoubleML: 
<code>DoubleMLIIVM</code>,
<code>DoubleMLIRM</code>,
<code>DoubleMLPLIV</code>,
<code>DoubleMLPLR</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## ------------------------------------------------
## Method `DoubleML$set_sample_splitting`
## ------------------------------------------------

library(DoubleML)
library(mlr3)
set.seed(2)
obj_dml_data = make_plr_CCDDHNR2018(n_obs=10)
dml_plr_obj = DoubleMLPLR$new(obj_dml_data,
                              lrn("regr.rpart"), lrn("regr.rpart"))

# simple sample splitting with two folds and without cross-fitting
smpls = list(list(train_ids = list(c(1, 2, 3, 4, 5)),
                  test_ids = list(c(6, 7, 8, 9, 10))))
dml_plr_obj$set_sample_splitting(smpls)

# sample splitting with two folds and cross-fitting but no repeated cross-fitting
smpls = list(list(train_ids = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)),
                  test_ids = list(c(6, 7, 8, 9, 10), c(1, 2, 3, 4, 5))))
dml_plr_obj$set_sample_splitting(smpls)

# sample splitting with two folds and repeated cross-fitting with n_rep = 2
smpls = list(list(train_ids = list(c(1, 2, 3, 4, 5), c(6, 7, 8, 9, 10)),
                  test_ids = list(c(6, 7, 8, 9, 10), c(1, 2, 3, 4, 5))),
             list(train_ids = list(c(1, 3, 5, 7, 9), c(2, 4, 6, 8, 10)),
                  test_ids = list(c(2, 4, 6, 8, 10), c(1, 3, 5, 7, 9))))
dml_plr_obj$set_sample_splitting(smpls)
</code></pre>


</div>