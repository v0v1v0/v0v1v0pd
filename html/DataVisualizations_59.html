<div class="container">

<table style="width: 100%;"><tr>
<td>RobustNormalization</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
RobustNormalization
</h2>

<h3>Description</h3>

<p>RobustNormalization as described in [Milligan/Cooper, 1988].
</p>


<h3>Usage</h3>

<pre><code class="language-R">RobustNormalization(Data,Centered=FALSE,Capped=FALSE,

na.rm=TRUE,WithBackTransformation=FALSE,

pmin=0.01,pmax=0.99) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Data</code></td>
<td>
<p>[1:n,1:d] data matrix of n cases and d features</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Centered</code></td>
<td>
<p> centered data around zero by median if TRUE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Capped</code></td>
<td>
<p>TRUE: outliers are capped above 1 or below -1 and set to 1 or -1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.rm</code></td>
<td>
<p>If TRUE, infinite vlaues are disregarded</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>WithBackTransformation</code></td>
<td>
<p>If in the case for forecasting with neural
networks a backtransformation is required, this parameter can be set to 'TRUE'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmin</code></td>
<td>
<p>defines outliers on the lower end of scale</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmax</code></td>
<td>
<p>defines outliers on the higher end of scale</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Normalizes features either between -1 to 1 (Centered=TRUE) or 0-1
(Centered=TRUE) without changing the distribution of a feature itself. For a
more precise description please read [Thrun, 2018, p.17].
</p>
<p>"[The] scaling of the inputs determines the effective scaling of the weights in
the last layer of a MLP with BP neural netowrk, it can have a large effect on
the quality of the final solution. At the outset it is best to standardize all
inputs to have mean zero and standard deviation 1
[(or at least the range under 1)]. This ensures all inputs are treated equally
in the regularization prozess, and allows to choose a meaningful range for the
random starting weights."[Friedman et al., 2012]
</p>


<h3>Value</h3>

<p>if <code>WithBackTransformation=FALSE</code>: TransformedData[1:n,1:d] i.e.,
normalized data matrix of n cases and d features
</p>
<p>if <code>WithBackTransformation=TRUE</code>: List with
</p>
<table>
<tr style="vertical-align: top;">
<td><code>TransformedData</code></td>
<td>
<p>[1:n,1:d]  normalized data matrix of n cases and d
features</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MinX</code></td>
<td>
<p>[1:d] numerical vector used for manual back-transformation of each
feature</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MaxX</code></td>
<td>
<p>[1:d] numerical vector used for manual back-transformation of each
feature</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Denom</code></td>
<td>
<p>[1:d] numerical vector used for manual back-transformation of each
feature</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Center</code></td>
<td>
<p>[1:d] numerical vector used for manual back-transformation of each
feature</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michael Thrun
</p>


<h3>References</h3>

<p>[Milligan/Cooper, 1988]  Milligan, G. W., &amp; Cooper, M. C.: A study of
standardization of variables in cluster analysis, Journal of Classification,
Vol. 5(2), pp. 181-204. 1988.
</p>
<p>[Friedman et al., 2012]  Friedman, J., Hastie, T., &amp; Tibshirani, R.: The
Elements of Statistical Learning, (Second ed. Vol. 1), Springer series in
statistics New York, NY, USA:, ISBN, 2012.
</p>
<p>[Thrun, 2018]  Thrun, M. C.: Projection Based Clustering through
Self-Organization and Swarm Intelligence, doctoral dissertation 2017, Springer,
Heidelberg, ISBN: 978-3-658-20539-3, <a href="https://doi.org/10.1007/978-3-658-20540-9">doi:10.1007/978-3-658-20540-9</a>, 2018. 
</p>


<h3>See Also</h3>

<p><code>RobustNorm_BackTrafo</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">Scaled = RobustNormalization(rnorm(1000, 2, 100), Capped = TRUE)
hist(Scaled)

m = cbind(c(1, 2, 3), c(2, 6, 4))
List = RobustNormalization(m, FALSE, FALSE, FALSE, TRUE)
TransformedData = List$TransformedData

mback = RobustNorm_BackTrafo(TransformedData, List$MinX, List$Denom, List$Center)

sum(m - mback)
</code></pre>


</div>