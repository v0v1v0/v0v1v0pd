<div class="container">

<table style="width: 100%;"><tr>
<td>KLFDA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Kernel Local Fisher Discriminant Analysis (KLFDA)

</h2>

<h3>Description</h3>

<p>Kernel Local Fisher Discriminant Analysis (KLFDA). This function implements the Kernel Local Fisher Discriminant Analysis with an unified Kernel function. Different from KLFDA function, which adopts the Multinomial Kernel as an example, this function empolys the kernel function that allows you to choose various types of kernels. See the kernel function from "kernelMatriax" (kernlab).

</p>


<h3>Usage</h3>

<pre><code class="language-R">KLFDA(x, y, kernel = kernlab::polydot(degree = 1, scale = 1, offset = 1), 
r = 20, tol, prior, CV = FALSE, usekernel = TRUE, 
fL = 0.5, metric = c("weighted", "orthonormalized", "plain"), 
knn = 6, reg = 0.001, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p> The input training data

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p> The training labels

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel</code></td>
<td>
<p> The kernel function used to calculate kernel matrix. Choose the corresponding kernel you want, see details.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>r</code></td>
<td>
<p> The number of reduced features you want to keep.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p> The tolerance used to reject the uni-variance. This is important when the variance between classes is small, and setting the large tolerance will avoid the data distortion.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p> The weight of each class, or the proportion of each class.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CV</code></td>
<td>
<p> Whether to do cross validation.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>usekernel</code></td>
<td>
<p>whether to use kernel classifier, if TRUE, pass to Naive Bayes classifier.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fL</code></td>
<td>
<p> If usekernel is TRUE, pass to the kernel function.

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>type of metric in the embedding space (default: 'weighted') 'weighted' - weighted eigenvectors 'orthonormalized' - orthonormalized 'plain' - raw eigenvectors

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knn</code></td>
<td>
<p> The number of nearest neighbours

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reg</code></td>
<td>
<p> The regularization parameter

</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p> additional arguments for the classifier

</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function empolys three different classifiers, the basic linear classifier, the Mabayes (Bayes rule and the Mahalanobis distance), and Niave Bayes classifier.
The argeument "kernel" in the klfda function is the kernel function used to calculate the kernel matrix. If usekernel is TRUE, the corresponding kernel parameters will pass the the Naive Bayes kernel classifier.
The kernel parameter can be set to any function, of class kernel, which computes the inner product in feature space between two vector arguments. kernlab provides the most popular kernel functions which can be initialized by using the following functions:
</p>
<p>rbfdot Radial Basis kernel function
</p>
<p>polydot Polynomial kernel function
</p>
<p>vanilladot Linear kernel function
</p>
<p>tanhdot Hyperbolic tangent kernel function
</p>
<p>laplacedot Laplacian kernel function
</p>
<p>besseldot Bessel kernel function
</p>
<p>anovadot ANOVA RBF kernel function
</p>
<p>splinedot the Spline kernel
</p>
<p>(see example.)
</p>
<p>kernelFast is mainly used in situations where columns of the kernel matrix are computed per invocation. In these cases, evaluating the norm of each row-entry over and over again would cause significant computational overhead.

</p>


<h3>Value</h3>

<p> The results give the classified classes and the posterior possibility of each class using different classifier. 


</p>
<table>
<tr style="vertical-align: top;">
<td><code>class</code></td>
<td>
<p>The class labels from linear classifier</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>posterior</code></td>
<td>
<p>The posterior possibility of each class from linear classifier</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bayes_judgement</code></td>
<td>
<p>Discrimintion results using the Mabayes classifier</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bayes_assigment</code></td>
<td>
<p>Discrimintion results using the Naive bayes classifier</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Z </code></td>
<td>
<p>The reduced features</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>qinxinghu@gmail.com

</p>


<h3>References</h3>

<p>Sugiyama, M (2007).Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis. Journal of Machine Learning Research, vol.8, 1027-1061.
</p>
<p>Sugiyama, M (2006). Local Fisher discriminant analysis for supervised dimensionality reduction. In W. W. Cohen and A. Moore (Eds.), Proceedings of 23rd International Conference on Machine Learning (ICML2006), 905-912.
</p>
<p>Original Matlab Implementation: http://www.ms.k.u-tokyo.ac.jp/software.html#LFDA
</p>
<p>Tang, Y., &amp; Li, W. (2019). lfda: Local Fisher Discriminant Analysis inR. Journal of Open Source Software, 4(39), 1572.
</p>
<p>Moore, A. W. (2004). Naive Bayes Classifiers. In School of Computer Science. Carnegie Mellon University.
</p>
<p>Pierre Enel (2020). Kernel Fisher Discriminant Analysis (https://www.github.com/p-enel/MatlabKFDA), GitHub. Retrieved March 30, 2020.
</p>
<p>Karatzoglou, A., Smola, A., Hornik, K., &amp; Zeileis, A. (2004). kernlab-an S4 package for kernel methods in R. Journal of statistical software, 11(9), 1-20.

</p>


<h3>See Also</h3>

<p>predict.KLFDA, KLFDAM

</p>


<h3>Examples</h3>

<pre><code class="language-R">require(kernlab)
btest=KLFDA(as.matrix(iris[,1:4]),as.matrix(as.data.frame(iris[,5])),
kernel=kernlab::rbfdot(sigma = 0.1),
r=3,prior=NULL,tol=1e-90,
reg=0.01,metric =  'plain')
pred=predict.KLFDA(btest,testData=as.matrix(iris[1:10,1:4]),prior=NULL)
</code></pre>


</div>