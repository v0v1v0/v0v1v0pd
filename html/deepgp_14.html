<div class="container">

<table style="width: 100%;"><tr>
<td>fit_one_layer</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>MCMC sampling for one layer GP</h2>

<h3>Description</h3>

<p>Conducts MCMC sampling of hyperparameters for a one layer 
GP.  Length scale parameter <code>theta</code> governs 
the strength of the correlation and nugget parameter <code>g</code> 
governs noise.  In Matern covariance, <code>v</code> governs smoothness.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fit_one_layer(
  x,
  y,
  nmcmc = 10000,
  sep = FALSE,
  verb = TRUE,
  g_0 = 0.001,
  theta_0 = 0.1,
  true_g = NULL,
  settings = NULL,
  cov = c("matern", "exp2"),
  v = 2.5,
  vecchia = FALSE,
  m = min(25, length(y) - 1),
  ordering = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>vector or matrix of input locations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>vector of response values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nmcmc</code></td>
<td>
<p>number of MCMC iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sep</code></td>
<td>
<p>logical indicating whether to use separable (<code>sep = TRUE</code>)
or isotropic (<code>sep = FALSE</code>) lengthscales</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>
<p>logical indicating whether to print iteration progress</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>g_0</code></td>
<td>
<p>initial value for <code>g</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta_0</code></td>
<td>
<p>initial value for <code>theta</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>true_g</code></td>
<td>
<p>if true nugget is known it may be specified here (set to a 
small value to make fit deterministic).  Note - values that are too 
small may cause numerical issues in matrix inversions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>settings</code></td>
<td>
<p>hyperparameters for proposals and priors (see details)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cov</code></td>
<td>
<p>covariance kernel, either Matern or squared exponential 
(<code>"exp2"</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>Matern smoothness parameter (only used if <code>cov = "matern"</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vecchia</code></td>
<td>
<p>logical indicating whether to use Vecchia approximation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>
<p>size of Vecchia conditioning sets (only used if 
<code>vecchia = TRUE</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ordering</code></td>
<td>
<p>optional ordering for Vecchia approximation, must correspond
to rows of <code>x</code>, defaults to random</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Utilizes Metropolis Hastings sampling of the length scale and
nugget parameters with proposals and priors controlled by 
<code>settings</code>.  When <code>true_g</code> is set to a specific value, the 
nugget is not estimated.  When <code>vecchia = TRUE</code>, all calculations 
leverage the Vecchia approximation with specified conditioning set size 
<code>m</code>.  Vecchia approximation is only implemented for 
<code>cov = "matern"</code>.
</p>
<p>NOTE on OpenMP: The Vecchia implementation relies on OpenMP parallelization
for efficient computation.  This function will produce a warning message 
if the package was installed without OpenMP (this is the default for 
CRAN packages installed on Apple machines).  To set up OpenMP 
parallelization, download the package source code and install 
using the gcc/g++ compiler.  
</p>
<p>Proposals for <code>g</code> and <code>theta</code> follow a uniform sliding window 
scheme, e.g. 
</p>
<p><code>g_star &lt;- runif(1, l * g_t / u, u * g_t / l)</code>, 
</p>
<p>with defaults <code>l = 1</code> and <code>u = 2</code> provided in <code>settings</code>.
To adjust these, set <code>settings = list(l = new_l, u = new_u)</code>.
Priors on <code>g</code> and <code>theta</code> follow Gamma distributions with 
shape parameters (<code>alpha</code>) and rate parameters (<code>beta</code>) 
controlled within the <code>settings</code> list object.  
Defaults have been updated with package version 1.1.3.  Default priors differ
for noisy/deterministic settings and depend on whether <code>monowarp = TRUE</code>.  
All default values are visible in the internal
<code>deepgp:::check_settings</code> function.
These priors are designed for <code>x</code> scaled 
to [0, 1] and <code>y</code> scaled to have mean 0 and variance 1.  These may
be adjusted using the <code>settings</code> input.
</p>
<p>The output object of class <code>gp</code> is designed for use with 
<code>continue</code>, <code>trim</code>, and <code>predict</code>.
</p>


<h3>Value</h3>

<p>a list of the S3 class <code>gp</code> or <code>gpvec</code> with elements:
</p>

<ul>
<li> <p><code>x</code>: copy of input matrix
</p>
</li>
<li> <p><code>y</code>: copy of response vector
</p>
</li>
<li> <p><code>nmcmc</code>: number of MCMC iterations
</p>
</li>
<li> <p><code>settings</code>: copy of proposal/prior settings
</p>
</li>
<li> <p><code>v</code>: copy of Matern smoothness parameter (<code>v = 999</code> 
indicates <code>cov = "exp2"</code>)
</p>
</li>
<li> <p><code>g</code>: vector of MCMC samples for <code>g</code>
</p>
</li>
<li> <p><code>theta</code>: vector of MCMC samples for <code>theta</code>
</p>
</li>
<li> <p><code>tau2</code>: vector of MLE estimates for <code>tau2</code> 
(scale parameter)
</p>
</li>
<li> <p><code>ll</code>: vector of MVN log likelihood for each Gibbs iteration
</p>
</li>
<li> <p><code>time</code>: computation time in seconds
</p>
</li>
</ul>
<h3>References</h3>

<p>Sauer, A. (2023). Deep Gaussian process surrogates for computer experiments. 
*Ph.D. Dissertation, Department of Statistics, Virginia Polytechnic Institute and State University.*
<br><br>
Sauer, A., Gramacy, R.B., &amp; Higdon, D. (2023). Active learning for deep 
Gaussian process surrogates. *Technometrics, 65,* 4-18.  arXiv:2012.08015
<br><br>
Sauer, A., Cooper, A., &amp; Gramacy, R. B. (2023). Vecchia-approximated deep Gaussian 
processes for computer experiments. 
*Journal of Computational and Graphical Statistics, 32*(3), 824-837.  arXiv:2204.02904
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Additional examples including real-world computer experiments are available at: 
# https://bitbucket.org/gramacylab/deepgp-ex/

# Booth function (inspired by the Higdon function)
f &lt;- function(x) {
  i &lt;- which(x &lt;= 0.58)
  x[i] &lt;- sin(pi * x[i] * 6) + cos(pi * x[i] * 12)
  x[-i] &lt;- 5 * x[-i] - 4.9
  return(x)
}

# Training data
x &lt;- seq(0, 1, length = 25)
y &lt;- f(x)

# Testing data
xx &lt;- seq(0, 1, length = 200)
yy &lt;- f(xx)

plot(xx, yy, type = "l")
points(x, y, col = 2)

# Example 1: full model (nugget fixed)
fit &lt;- fit_one_layer(x, y, nmcmc = 2000, true_g = 1e-6)
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit)

# Example 2: full model (nugget estimated, EI calculated)
fit &lt;- fit_one_layer(x, y, nmcmc = 2000)
plot(fit) 
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1, EI = TRUE)
plot(fit)
par(new = TRUE) # overlay EI
plot(xx[order(xx)], fit$EI[order(xx)], type = 'l', lty = 2, 
axes = FALSE, xlab = '', ylab = '')

# Example 3: Vecchia approximated model (nugget estimated)
fit &lt;- fit_one_layer(x, y, nmcmc = 2000, vecchia = TRUE, m = 10)
plot(fit)
fit &lt;- trim(fit, 1000, 2)
fit &lt;- predict(fit, xx, cores = 1)
plot(fit)


</code></pre>


</div>