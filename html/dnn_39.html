<div class="container">

<table style="width: 100%;"><tr>
<td>optimizerSGD</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Functions to optimize the gradient descent of a cost function
</h2>

<h3>Description</h3>

<p>Different type of optimizer functions such as SGD, Momentum, AdamG and NAG.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  optimizerMomentum(V, dW, W, alpha = 0.63, lr = 1e-4, lambda = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>V</code></td>
<td>
<p>Momentum V = alpha*V - lr*(dW + lambda*W); W = W + V. 
NAG V = alpha*(V - lr*(dW + lambda*W); W = W + V - lr*(dW + lambda*W)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dW</code></td>
<td>
<p>derivative of cost with respect to W, can be founde by dW = bwdNN2(dy, cache, model), </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W</code></td>
<td>
<p>weights for DNN model, optimizerd by W = W + V</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Momentum rate 0 &lt; alpha &lt; 1, default is alpah = 0.5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>learning rate, default is lr = 0.001.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>regulation rate for cost + 0.5*lambda*||W||, default is lambda = 1.0.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For SGD with momentum, use
</p>
<p>V = 0; obj = optimizerMomentum(V, dW, W); V = obj$V; W = obj$W
</p>
<p>For SDG with MAG
</p>
<p>V = 0; obj = optimizerNAG(V, dW, W); V = obj$V; W = obj$W
</p>


<h3>Value</h3>


<p>return and updated W and other parameters such as V, V1 and V2 that will be used on SGD.
</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p><code>activation</code>, 
<code>bwdNN</code>, 
<code>fwdNN</code>,
<code>dNNmodel</code>, 
<code>dnnFit</code> 
</p>


</div>