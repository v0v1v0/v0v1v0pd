<div class="container">

<table style="width: 100%;"><tr>
<td>rca</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Relevant Component Analysis</h2>

<h3>Description</h3>

<p>Performs relevant component analysis on the given data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">rca(x, chunks)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>matrix or data frame of original data.
Each row is a feature vector of a data instance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chunks</code></td>
<td>
<p>list of <code>k</code> numerical vectors.
Each vector represents a chunklet, the elements
in the vectors indicate where the samples locate
in <code>x</code>. See examples for more information.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The RCA function takes a data set and a set of positive constraints
as arguments and returns a linear transformation of the data space
into better representation, alternatively, a Mahalanobis metric
over the data space.
</p>
<p>Relevant component analysis consists of three steps:
</p>
<ol>
<li>
<p> locate the test point
</p>
</li>
<li>
<p> compute the distances between the test points
</p>
</li>
<li>
<p> find <code class="reqn">k</code> shortest distances and the bla</p>
</li>
</ol>
<p>The new representation is known to be optimal in an information
theoretic sense under a constraint of keeping equivalent data
points close to each other.
</p>


<h3>Value</h3>

<p>list of the RCA results:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>The RCA suggested Mahalanobis matrix.
Distances between data points x1, x2 should be
computed by (x2 - x1)' * B * (x2 - x1)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>A</code></td>
<td>
<p>The RCA suggested transformation of the data.
The data should be transformed by A * data</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newX</code></td>
<td>
<p>The data after the RCA transformation (A).
newData = A * data</p>
</td>
</tr>
</table>
<p>The three returned argument are just different forms of the same output.
If one is interested in a Mahalanobis metric over the original data space,
the first argument is all she/he needs. If a transformation into another
space (where one can use the Euclidean metric) is preferred, the second
returned argument is sufficient. Using A and B is equivalent in the
following sense:
</p>
<p>if y1 = A * x1, y2 = A * y2  then
(x2 - x1)' * B * (x2 - x1) = (y2 - y1)' * (y2 - y1)
</p>


<h3>Note</h3>

<p>Note that any different sets of instances (chunklets),
e.g. 1, 3, 7 and 4, 6, might belong to the
same class and might belong to different classes.
</p>


<h3>Author(s)</h3>

<p>Xiao Nan &lt;<a href="http://www.road2stat.com">http://www.road2stat.com</a>&gt;
</p>


<h3>References</h3>

<p>Aharon Bar-Hillel, Tomer Hertz, Noam Shental, and Daphna Weinshall (2003).
Learning Distance Functions using Equivalence Relations.
<em>Proceedings of 20th International Conference on
Machine Learning (ICML2003)</em>.
</p>


<h3>See Also</h3>

<p>See <code>dca</code> for exploiting negative constrains.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
set.seed(1234)
require(MASS)  # generate synthetic Gaussian data
k = 100        # sample size of each class
n = 3          # specify how many class
N = k * n      # total sample number
x1 = mvrnorm(k, mu = c(-10, 6), matrix(c(10, 4, 4, 10), ncol = 2))
x2 = mvrnorm(k, mu = c(0, 0), matrix(c(10, 4, 4, 10), ncol = 2))
x3 = mvrnorm(k, mu = c(10, -6), matrix(c(10, 4, 4, 10), ncol = 2))
x = as.data.frame(rbind(x1, x2, x3))
x$V3 = gl(n, k)

# The fully labeled data set with 3 classes
plot(x$V1, x$V2, bg = c("#E41A1C", "#377EB8", "#4DAF4A")[x$V3],
     pch = c(rep(22, k), rep(21, k), rep(25, k)))
Sys.sleep(3)

# Same data unlabeled; clearly the classes' structure is less evident
plot(x$V1, x$V2)
Sys.sleep(3)

chunk1 = sample(1:100, 5)
chunk2 = sample(setdiff(1:100, chunk1), 5)
chunk3 = sample(101:200, 5)
chunk4 = sample(setdiff(101:200, chunk3), 5)
chunk5 = sample(201:300, 5)
chks = x[c(chunk1, chunk2, chunk3, chunk4, chunk5), ]
chunks = list(chunk1, chunk2, chunk3, chunk4, chunk5)

# The chunklets provided to the RCA algorithm
plot(chks$V1, chks$V2, col = rep(c("#E41A1C", "#377EB8",
     "#4DAF4A", "#984EA3", "#FF7F00"), each = 5),
     pch = rep(0:4, each = 5), ylim = c(-15, 15))
Sys.sleep(3)

# Whitening transformation applied to the  chunklets
chkTransformed = as.matrix(chks[ , 1:2]) %*% rca(x[ , 1:2], chunks)$A

plot(chkTransformed[ , 1], chkTransformed[ , 2], col = rep(c(
     "#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00"), each = 5),
     pch = rep(0:4, each = 5), ylim = c(-15, 15))
Sys.sleep(3)

# The origin data after applying the RCA transformation
plot(rca(x[ , 1:2], chunks)$newX[, 1], rca(x[ , 1:2], chunks)$newX[, 2],
         bg = c("#E41A1C", "#377EB8", "#4DAF4A")[gl(n, k)],
         pch = c(rep(22, k), rep(21, k), rep(25, k)))

# The RCA suggested transformation of the data, dimensionality reduced
rca(x[ , 1:2], chunks)$A

# The RCA suggested Mahalanobis matrix
rca(x[ , 1:2], chunks)$B

## End(Not run)
</code></pre>


</div>