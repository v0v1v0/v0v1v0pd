<div class="container">

<table style="width: 100%;"><tr>
<td>Normal</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Create a Normal distribution</h2>

<h3>Description</h3>

<p>The Normal distribution is ubiquitous in statistics, partially because
of the central limit theorem, which states that sums of i.i.d. random
variables eventually become Normal. Linear transformations of Normal
random variables result in new random variables that are also Normal. If
you are taking an intro stats course, you'll likely use the Normal
distribution for Z-tests and in simple linear regression. Under
regularity conditions, maximum likelihood estimators are
asymptotically Normal. The Normal distribution is also called the
gaussian distribution.
</p>


<h3>Usage</h3>

<pre><code class="language-R">Normal(mu = 0, sigma = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>The location parameter, written <code class="reqn">\mu</code> in textbooks,
which is also the mean of the distribution. Can be any real number.
Defaults to <code>0</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>The scale parameter, written <code class="reqn">\sigma</code> in textbooks,
which is also the <strong>standard deviation</strong> of the distribution. Can be any
positive number. Defaults to <code>1</code>. If you would like a Normal
distribution with <strong>variance</strong> <code class="reqn">\sigma^2</code>, be sure to take the
square root, as this is a common source of errors.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>We recommend reading this documentation on
<a href="https://alexpghayes.github.io/distributions3/">https://alexpghayes.github.io/distributions3/</a>, where the math
will render with additional detail and much greater clarity.
</p>
<p>In the following, let <code class="reqn">X</code> be a Normal random variable with mean
<code>mu</code> = <code class="reqn">\mu</code> and standard deviation <code>sigma</code> = <code class="reqn">\sigma</code>.
</p>
<p><strong>Support</strong>: <code class="reqn">R</code>, the set of all real numbers
</p>
<p><strong>Mean</strong>: <code class="reqn">\mu</code>
</p>
<p><strong>Variance</strong>: <code class="reqn">\sigma^2</code>
</p>
<p><strong>Probability density function (p.d.f)</strong>:
</p>
<p style="text-align: center;"><code class="reqn">
    f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-(x - \mu)^2 / 2 \sigma^2}
  </code>
</p>

<p><strong>Cumulative distribution function (c.d.f)</strong>:
</p>
<p>The cumulative distribution function has the form
</p>
<p style="text-align: center;"><code class="reqn">
    F(t) = \int_{-\infty}^t \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-(x - \mu)^2 / 2 \sigma^2} dx
  </code>
</p>

<p>but this integral does not have a closed form solution and must be
approximated numerically. The c.d.f. of a standard Normal is sometimes
called the "error function". The notation <code class="reqn">\Phi(t)</code> also stands
for the c.d.f. of a standard Normal evaluated at <code class="reqn">t</code>. Z-tables
list the value of <code class="reqn">\Phi(t)</code> for various <code class="reqn">t</code>.
</p>
<p><strong>Moment generating function (m.g.f)</strong>:
</p>
<p style="text-align: center;"><code class="reqn">
    E(e^{tX}) = e^{\mu t + \sigma^2 t^2 / 2}
  </code>
</p>



<h3>Value</h3>

<p>A <code>Normal</code> object.
</p>


<h3>See Also</h3>

<p>Other continuous distributions: 
<code>Beta()</code>,
<code>Cauchy()</code>,
<code>ChiSquare()</code>,
<code>Erlang()</code>,
<code>Exponential()</code>,
<code>FisherF()</code>,
<code>Frechet()</code>,
<code>GEV()</code>,
<code>GP()</code>,
<code>Gamma()</code>,
<code>Gumbel()</code>,
<code>LogNormal()</code>,
<code>Logistic()</code>,
<code>RevWeibull()</code>,
<code>StudentsT()</code>,
<code>Tukey()</code>,
<code>Uniform()</code>,
<code>Weibull()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
set.seed(27)

X &lt;- Normal(5, 2)
X

mean(X)
variance(X)
skewness(X)
kurtosis(X)

random(X, 10)

pdf(X, 2)
log_pdf(X, 2)

cdf(X, 4)
quantile(X, 0.7)

### example: calculating p-values for two-sided Z-test

# here the null hypothesis is H_0: mu = 3
# and we assume sigma = 2

# exactly the same as: Z &lt;- Normal(0, 1)
Z &lt;- Normal()

# data to test
x &lt;- c(3, 7, 11, 0, 7, 0, 4, 5, 6, 2)
nx &lt;- length(x)

# calculate the z-statistic
z_stat &lt;- (mean(x) - 3) / (2 / sqrt(nx))
z_stat

# calculate the two-sided p-value
1 - cdf(Z, abs(z_stat)) + cdf(Z, -abs(z_stat))

# exactly equivalent to the above
2 * cdf(Z, -abs(z_stat))

# p-value for one-sided test
# H_0: mu &lt;= 3   vs   H_A: mu &gt; 3
1 - cdf(Z, z_stat)

# p-value for one-sided test
# H_0: mu &gt;= 3   vs   H_A: mu &lt; 3
cdf(Z, z_stat)

### example: calculating a 88 percent Z CI for a mean

# same `x` as before, still assume `sigma = 2`

# lower-bound
mean(x) - quantile(Z, 1 - 0.12 / 2) * 2 / sqrt(nx)

# upper-bound
mean(x) + quantile(Z, 1 - 0.12 / 2) * 2 / sqrt(nx)

# equivalent to
mean(x) + c(-1, 1) * quantile(Z, 1 - 0.12 / 2) * 2 / sqrt(nx)

# also equivalent to
mean(x) + quantile(Z, 0.12 / 2) * 2 / sqrt(nx)
mean(x) + quantile(Z, 1 - 0.12 / 2) * 2 / sqrt(nx)

### generating random samples and plugging in ks.test()

set.seed(27)

# generate a random sample
ns &lt;- random(Normal(3, 7), 26)

# test if sample is Normal(3, 7)
ks.test(ns, pnorm, mean = 3, sd = 7)

# test if sample is gamma(8, 3) using base R pgamma()
ks.test(ns, pgamma, shape = 8, rate = 3)

### MISC

# note that the cdf() and quantile() functions are inverses
cdf(X, quantile(X, 0.7))
quantile(X, cdf(X, 7))
</code></pre>


</div>