<div class="container">

<table style="width: 100%;"><tr>
<td>dai_async</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>OCR documents asynchronously</h2>

<h3>Description</h3>

<p>Sends files from a Google Cloud Services (GCS) Storage
bucket to the GCS Document AI v1 API for asynchronous (offline) processing.
The output is delivered to the same bucket as JSON files containing
the OCRed text and additional data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">dai_async(
  files,
  dest_folder = NULL,
  bucket = Sys.getenv("GCS_DEFAULT_BUCKET"),
  proj_id = get_project_id(),
  proc_id = Sys.getenv("DAI_PROCESSOR_ID"),
  proc_v = NA,
  skip_rev = "true",
  loc = "eu",
  token = dai_token()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>files</code></td>
<td>
<p>a vector or list of pdf filepaths in a GCS Storage bucket
Filepaths must include all parent bucket folder(s) except the bucket name</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dest_folder</code></td>
<td>
<p>the name of the GCS Storage bucket subfolder where
you want the json output</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bucket</code></td>
<td>
<p>the name of the GCS Storage bucket where the files
to be processed are located</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>proj_id</code></td>
<td>
<p>a GCS project id</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>proc_id</code></td>
<td>
<p>a Document AI processor id</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>proc_v</code></td>
<td>
<p>one of 1) a processor version name, 2) "stable" for the
latest processor from the stable channel, or 3) "rc" for the latest
processor from the release candidate channel.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>skip_rev</code></td>
<td>
<p>whether to skip human review; "true" or "false"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loc</code></td>
<td>
<p>a two-letter region code; "eu" or "us"</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>token</code></td>
<td>
<p>an access token generated by <code>dai_auth()</code> or another
auth function</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Requires a GCS access token and some configuration of the
.Renviron file; see package vignettes for details. Currently, a
<code>dai_async()</code> call can contain a maximum of 50 files (but a
multi-page pdf counts as one file). You can not have more than
5 batch requests and 10,000 pages undergoing processing at any one time.
Maximum pdf document length is 2,000 pages. With long pdf documents,
Document AI divides the JSON output into separate files ('shards') of
20 pages each. If you want longer shards, use <code>dai_tab_async()</code>,
which accesses another API endpoint that allows for shards of up to
100 pages.
</p>


<h3>Value</h3>

<p>A list of HTTP responses
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# with daiR configured on your system, several parameters are automatically provided,
# and you can pass simple calls, such as:
dai_async("my_document.pdf")

# NB: Include all parent bucket folders (but not the bucket name) in the filepath:
dai_async("for_processing/pdfs/my_document.pdf")

# Bulk process by passing a vector of filepaths in the files argument:
dai_async(my_files)

# Specify a bucket subfolder for the json output:
dai_async(my_files, dest_folder = "processed")

## End(Not run)
</code></pre>


</div>