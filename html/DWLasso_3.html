<div class="container">

<table style="width: 100%;"><tr>
<td>DWLasso</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Degree weighted lasso
</h2>

<h3>Description</h3>

<p>Infers undirected networks with hubs using weighted nodewise regression approach. The method contains two parameters that control hub and overall sparsity, respectively.
</p>


<h3>Usage</h3>

<pre><code class="language-R">DWLasso(X, lambda1 = 0.4, lambda2 = 2, a = 1, tol = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>An input matrix. The columns represent variables and the rows indicate observations.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda1</code></td>
<td>

<p>A penalty parameter that controls degree sparsity of the inferred network
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>

<p>A penalty parameter that controls overall sparsity of the inferred network
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>

<p>A parameter of the update equation that controls the convergence of weights
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>Tolerance
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This implements weighted degree lasso using coordinate descent algorithm (implemented in Glmnet package) described in Sulaimanov et al.. The method is based on the weighted nodewise regression approach and infers large undirected networks with hubs in iterative manner in the setting more variables than samples (p&gt;n). Given p variables, the network is inferred by regressing each variable against the remaining (p-1) variables.
The penalty parameter, lambda1 controls the degree sparsity of the network, whereas the penalty parameter, lambda2 controls the overall sparsity.The method uses a fast Lasso solver Glmnet (Friedman et al. (2010)) with default settings.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mat</code></td>
<td>

<p>The estimated matrix corresponding to the inferred network. The diagonal elements of the matrix are zero</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>

<p>The estimated weights used to estimate the network. These weights are computed from the degree of estimated networks</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda1</code></td>
<td>

<p>The value of the penalty parameter controlling degree sparsity of the inferred network.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2</code></td>
<td>

<p>The value of the penalty parameter controlling the overall sparsity</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Nurgazy Sulaimanov, Sunil Kumar, Frederic Burdet, Mark Ibberson, Marco Pagni, Heinz Koeppl.
</p>
<p>Maintainer: Nurgazy Sulaimanov, nurgazy.sulaimanov@bcs.tu-darmstadt.de
</p>


<h3>References</h3>

<p>1. Nurgazy Sulaimanov, Sunil Kumar, Frederic Burdet, Mark Ibberson, Marco Pagni, Heinz Koeppl. Inferring hub networks using weighted degree Lasso. http://arxiv.org/abs/1710.01912.
</p>
<p>2. Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.
</p>
<p>3. Tan, KM., London, P., Mohan, K., Lee, S-I., Fazel, M., and Witten, D. (2014). Learning graphical models with hubs. Journal of Machine Learning Research. 5.1 (2014): 3297-3331.
</p>
<p>4. Meinshausen, Nicolai, and Peter BÃ¼hlmann. "High-dimensional graphs and variable selection with the lasso." The annals of statistics (2006): 1436-1462.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(DWLasso)
library(glmnet)
library(hglasso)


# Generate inverse covariance matrix with 3 hubs
# 20 % of the elements within a hub are zero
# 97 % of the elements that are not within hub nodes are zero
p &lt;- 60 # Number of variables
n &lt;- 40 # Number of samples

hub_number = 3  # Number of hubs

# Generate the adjacency matrix
Theta &lt;- HubNetwork(p,0.97,hub_number,0.2)$Theta

# Generate a data matrix
out &lt;- rmvnorm(n,rep(0,p),solve(Theta))

# Standardize the data
dat &lt;- scale(out)

# Run DWLasso
out.p &lt;- DWLasso(dat, lambda1 = 0.6, lambda2 = 10)

# print out a summary of the output
summary(out.p)
</code></pre>


</div>