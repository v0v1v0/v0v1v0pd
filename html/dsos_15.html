<div class="container">

<table style="width: 100%;"><tr>
<td>pt_refit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Permutation Test By Refitting</h2>

<h3>Description</h3>

<p>Test for no adverse shift with outlier scores. Like goodness-of-fit testing,
this two-sample comparison takes the training set, <code>x_train</code> as the
as the reference. The method checks whether the test set, <code>x_test</code>, is
worse off relative to this reference set. The function <code>scorer</code> assigns
an outlier score to each instance/observation in both training and test set.
</p>


<h3>Usage</h3>

<pre><code class="language-R">pt_refit(x_train, x_test, scorer, n_pt = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x_train</code></td>
<td>
<p>Training (reference/validation) sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x_test</code></td>
<td>
<p>Test sample.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scorer</code></td>
<td>
<p>Function which returns a named list with outlier scores from
the training and test sample. The first argument to <code>scorer</code> must be
<code>x_train</code>; the second, <code>x_test</code>. The returned named list contains
two elements: <em>train</em> and <em>test</em>, each of which is a vector of
corresponding (outlier) scores. See notes below for more information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_pt</code></td>
<td>
<p>The number of permutations.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The null distribution of the test statistic is based on <code>n_pt</code>
permutations. For speed, this is implemented as a sequential Monte Carlo test
with the <span class="pkg">simctest</span> package. See Gandy (2009) for details. The prefix
<em>pt</em> refers to permutation test. This approach does not use the
asymptotic null distribution for the test statistic. This is the recommended
approach for small samples. The test statistic is the weighted AUC (WAUC).
</p>


<h3>Value</h3>

<p>A named list of class <code>outlier.test</code> containing:
</p>

<ul>
<li> <p><code>statistic</code>: observed WAUC statistic
</p>
</li>
<li> <p><code>seq_mct</code>: sequential Monte Carlo test, when applicable
</p>
</li>
<li> <p><code>p_value</code>: p-value
</p>
</li>
<li> <p><code>outlier_scores</code>: outlier scores from training and test set
</p>
</li>
</ul>
<h3>Notes</h3>

<p>The scoring function, <code>scorer</code>, predicts out-of-sample scores by
refitting the underlying algorithm from <code>scorer</code> at every permutation
The suffix <em>refit</em> emphasizes this point. This is in contrast to the
out-of-bag variant, <code>pt_oob</code>, which only fits once. This method can be
be computationally expensive.
</p>


<h3>References</h3>

<p>Kamulete, V. M. (2022).
<em>Test for non-negligible adverse shifts</em>.
In The 38th Conference on Uncertainty in Artificial Intelligence. PMLR.
</p>
<p>Gandy, A. (2009).
<em>Sequential implementation of Monte Carlo tests with uniformly bounded resampling risk</em>.
Journal of the American Statistical Association, 104(488), 1504-1511.
</p>


<h3>See Also</h3>

<p>[pt_oob()] for (faster) p-value approximation via out-of-bag predictions.
[at_oob()] for p-value approximation from asymptotic null distribution.
</p>
<p>Other permutation-test: 
<code>pt_from_os()</code>,
<code>pt_oob()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(dsos)
set.seed(12345)
data(iris)
setosa &lt;- iris[1:50, 1:4] # Training sample: Species == 'setosa'
versicolor &lt;- iris[51:100, 1:4] # Test sample: Species == 'versicolor'
scorer &lt;- function(tr, te) list(train=runif(nrow(tr)), test=runif(nrow(te)))
pt_test &lt;- pt_refit(setosa, versicolor, scorer = scorer)
pt_test


</code></pre>


</div>