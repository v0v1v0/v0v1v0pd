<div class="container">

<table style="width: 100%;"><tr>
<td>WeightedERMDP.CMS</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Privacy-preserving Weighted Empirical Risk Minimization</h2>

<h3>Description</h3>

<p>This class implements differentially private empirical risk
minimization in the case where weighted observation-level losses are
desired (such as weighted SVM (Yang et al. 2005)). Currently,
only the output perturbation method is implemented.
</p>


<h3>Details</h3>

<p>To use this class for weighted empirical risk minimization, first
use the <code>new</code> method to construct an object of this class with the
desired function values and hyperparameters. After constructing the object,
the <code>fit</code> method can be applied with a provided dataset, data bounds,
weights, and weight bounds to fit the model. In fitting, the model stores a
vector of coefficients <code>coeff</code> which satisfy differential privacy.
These can be released directly, or used in conjunction with the
<code>predict</code> method to privately predict the outcomes of new datapoints.
</p>
<p>Note that in order to guarantee differential privacy for weighted empirical
risk minimization, certain constraints must be satisfied for the values
used to construct the object, as well as for the data used to fit. These
conditions depend on the chosen perturbation method, though currently only
output perturbation is implemented. Specifically, the provided loss
function must be convex and differentiable with respect to <code>y.hat</code>,
and the absolute value of the first derivative of the loss function must be
at most 1. If objective perturbation is chosen (not currently implemented),
the loss function must also be doubly differentiable and the absolute value
of the second derivative of the loss function must be bounded above by a
constant c for all possible values of <code>y.hat</code> and <code>y</code>, where
<code>y.hat</code> is the predicted label and <code>y</code> is the true label. The
regularizer must be 1-strongly convex and differentiable. It also must be
doubly differentiable if objective perturbation is chosen. For the data x,
it is assumed that if x represents a single row of the dataset X, then the
l2-norm of x is at most 1 for all x. Note that because of this, a bias term
cannot be included without appropriate scaling/preprocessing of the
dataset. To ensure privacy, the add.bias argument in the <code>fit</code> and
<code>predict</code> methods should only be utilized in subclasses within this
package where appropriate preprocessing is implemented, not in this class.
Finally, if weights are provided, they should be nonnegative, of the same
length as y, and be upper bounded by a global or public bound which must
also be provided.
</p>


<h3>Super class</h3>

<p><code>DPpack::EmpiricalRiskMinimizationDP.CMS</code> -&gt; <code>WeightedERMDP.CMS</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-WeightedERMDP.CMS-new"><code>WeightedERMDP.CMS$new()</code></a>
</p>
</li>
<li> <p><a href="#method-WeightedERMDP.CMS-fit"><code>WeightedERMDP.CMS$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-WeightedERMDP.CMS-predict"><code>WeightedERMDP.CMS$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-WeightedERMDP.CMS-clone"><code>WeightedERMDP.CMS$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-WeightedERMDP.CMS-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new <code>WeightedERMDP.CMS</code> object.
</p>


<h5>Usage</h5>

<div class="r"><pre>WeightedERMDP.CMS$new(
  mapXy,
  loss,
  regularizer,
  eps,
  gamma,
  perturbation.method = "objective",
  c = NULL,
  mapXy.gr = NULL,
  loss.gr = NULL,
  regularizer.gr = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>mapXy</code></dt>
<dd>
<p>Map function of the form <code>mapXy(X, coeff)</code> mapping input
data matrix <code>X</code> and coefficient vector or matrix <code>coeff</code> to
output labels <code>y</code>. Should return a column matrix of predicted labels
for each row of <code>X</code>. See <code>mapXy.sigmoid</code> for an example.</p>
</dd>
<dt><code>loss</code></dt>
<dd>
<p>Loss function of the form <code>loss(y.hat, y, w)</code>, where
<code>y.hat</code> and <code>y</code> are matrices and <code>w</code> is a matrix or vector
of weights of the same length as <code>y</code>. Should be defined such that it
returns a matrix of weighted loss values for each element of <code>y.hat</code>
and <code>y</code>. If <code>w</code> is not given, the function should operate as if
uniform weights were given. See <code>generate.loss.huber</code> for an
example. It must be convex and differentiable, and the absolute value of
the first derivative of the loss function must be at most 1.
Additionally, if the objective perturbation method is chosen, it must be
doubly differentiable and the absolute value of the second derivative of
the loss function must be bounded above by a constant c for all possible
values of <code>y.hat</code> and <code>y</code>.</p>
</dd>
<dt><code>regularizer</code></dt>
<dd>
<p>String or regularization function. If a string, must be
'l2', indicating to use l2 regularization. If a function, must have form
<code>regularizer(coeff)</code>, where <code>coeff</code> is a vector or matrix, and
return the value of the regularizer at <code>coeff</code>. See
<code>regularizer.l2</code> for an example. Additionally, in order to
ensure differential privacy, the function must be 1-strongly convex and
differentiable. If the objective perturbation method is chosen, it must
also be doubly differentiable.</p>
</dd>
<dt><code>eps</code></dt>
<dd>
<p>Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.</p>
</dd>
<dt><code>gamma</code></dt>
<dd>
<p>Nonnegative real number representing the regularization
constant.</p>
</dd>
<dt><code>perturbation.method</code></dt>
<dd>
<p>String indicating whether to use the 'output' or
the 'objective' perturbation methods (Chaudhuri et al. 2011).
Defaults to 'objective'. Currently, only the output perturbation method
is supported.</p>
</dd>
<dt><code>c</code></dt>
<dd>
<p>Positive real number denoting the upper bound on the absolute
value of the second derivative of the loss function, as required to
ensure differential privacy for the objective perturbation method. This
input is unnecessary if perturbation.method is 'output', but is required
if perturbation.method is 'objective'. Defaults to NULL.</p>
</dd>
<dt><code>mapXy.gr</code></dt>
<dd>
<p>Optional function representing the gradient of the map
function with respect to the values in <code>coeff</code>. If given, must be of
the form <code>mapXy.gr(X, coeff)</code>, where <code>X</code> is a matrix and
<code>coeff</code> is a matrix or numeric vector. Should be defined such that
the ith row of the output represents the gradient with respect to the ith
coefficient. See <code>mapXy.gr.sigmoid</code> for an example. If not
given, non-gradient based optimization methods are used to compute the
coefficient values in fitting the model.</p>
</dd>
<dt><code>loss.gr</code></dt>
<dd>
<p>Optional function representing the gradient of the loss
function with respect to <code>y.hat</code> and of the form
<code>loss.gr(y.hat, y, w)</code>, where <code>y.hat</code> and <code>y</code> are matrices
and <code>w</code> is a matrix or vector of weights. Should be defined such
that the ith row of the output represents the gradient of the (possibly
weighted) loss function at the ith set of input values. See
<code>generate.loss.gr.huber</code> for an example. If not given,
non-gradient based optimization methods are used to compute the
coefficient values in fitting the model.</p>
</dd>
<dt><code>regularizer.gr</code></dt>
<dd>
<p>Optional function representing the gradient of the
regularization function with respect to <code>coeff</code> and of the form
<code>regularizer.gr(coeff)</code>. Should return a vector. See
<code>regularizer.gr.l2</code> for an example. If <code>regularizer</code> is
given as a string, this value is ignored. If not given and
<code>regularizer</code> is a function, non-gradient based optimization methods
are used to compute the coefficient values in fitting the model.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>A new <code>WeightedERMDP.CMS</code> object.
</p>


<hr>
<a id="method-WeightedERMDP.CMS-fit"></a>



<h4>Method <code>fit()</code>
</h4>

<p>Fit the differentially private weighted empirical risk
minimization model. This method runs either the output perturbation or
the objective perturbation algorithm (Chaudhuri et al. 2011)
(only output is currently implemented), depending on the value of
perturbation.method used to construct the object, to generate an
objective function. A numerical optimization method is then run to find
optimal coefficients for fitting the model given the training data,
weights, and hyperparameters. The built-in <code>optim</code> function
using the "BFGS" optimization method is used. If <code>mapXy.gr</code>,
<code>loss.gr</code>, and <code>regularizer.gr</code> are all given in the
construction of the object, the gradient of the objective function is
utilized by <code>optim</code> as well. Otherwise, non-gradient based
optimization methods are used. The resulting privacy-preserving
coefficients are stored in <code>coeff</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>WeightedERMDP.CMS$fit(
  X,
  y,
  upper.bounds,
  lower.bounds,
  add.bias = FALSE,
  weights = NULL,
  weights.upper.bound = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt>
<dd>
<p>Dataframe of data to be fit.</p>
</dd>
<dt><code>y</code></dt>
<dd>
<p>Vector or matrix of true labels for each row of <code>X</code>.</p>
</dd>
<dt><code>upper.bounds</code></dt>
<dd>
<p>Numeric vector of length <code>ncol(X)</code> giving upper
bounds on the values in each column of X. The <code>ncol(X)</code> values are
assumed to be in the same order as the corresponding columns of <code>X</code>.
Any value in the columns of <code>X</code> larger than the corresponding upper
bound is clipped at the bound.</p>
</dd>
<dt><code>lower.bounds</code></dt>
<dd>
<p>Numeric vector of length <code>ncol(X)</code> giving lower
bounds on the values in each column of <code>X</code>. The <code>ncol(X)</code>
values are assumed to be in the same order as the corresponding columns
of <code>X</code>. Any value in the columns of <code>X</code> larger than the
corresponding upper bound is clipped at the bound.</p>
</dd>
<dt><code>add.bias</code></dt>
<dd>
<p>Boolean indicating whether to add a bias term to <code>X</code>.
Defaults to FALSE.</p>
</dd>
<dt><code>weights</code></dt>
<dd>
<p>Numeric vector of observation weights of the same length as
<code>y</code>.</p>
</dd>
<dt><code>weights.upper.bound</code></dt>
<dd>
<p>Numeric value representing the global or public
upper bound on the weights.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-WeightedERMDP.CMS-predict"></a>



<h4>Method <code>predict()</code>
</h4>

<p>Predict label(s) for given <code>X</code> using the fitted
coefficients.
</p>


<h5>Usage</h5>

<div class="r"><pre>WeightedERMDP.CMS$predict(X, add.bias = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt>
<dd>
<p>Dataframe of data on which to make predictions. Must be of same
form as <code>X</code> used to fit coefficients.</p>
</dd>
<dt><code>add.bias</code></dt>
<dd>
<p>Boolean indicating whether to add a bias term to <code>X</code>.
Defaults to FALSE. If add.bias was set to TRUE when fitting the
coefficients, add.bias should be set to TRUE for predictions.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Matrix of predicted labels corresponding to each row of <code>X</code>.
</p>


<hr>
<a id="method-WeightedERMDP.CMS-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>WeightedERMDP.CMS$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>References</h3>

<p>Chaudhuri K, Monteleoni C, Sarwate AD (2011).
“Differentially Private Empirical Risk Minimization.”
<em>Journal of Machine Learning Research</em>, <b>12</b>(29), 1069-1109.
<a href="https://jmlr.org/papers/v12/chaudhuri11a.html">https://jmlr.org/papers/v12/chaudhuri11a.html</a>.
</p>
<p>Yang X, Song Q, Cao A (2005).
“Weighted support vector machine for data classification.”
In <em>Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.</em>, volume 2, 859-864 vol. 2.
<a href="https://doi.org/10.1109/IJCNN.2005.1555965">doi:10.1109/IJCNN.2005.1555965</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Build train dataset X and y, and test dataset Xtest and ytest
N &lt;- 200
K &lt;- 2
X &lt;- data.frame()
y &lt;- data.frame()
for (j in (1:K)){
  t &lt;- seq(-.25, .25, length.out = N)
  if (j==1) m &lt;- stats::rnorm(N,-.2, .1)
  if (j==2) m &lt;- stats::rnorm(N, .2, .1)
  Xtemp &lt;- data.frame(x1 = 3*t , x2 = m - t)
  ytemp &lt;- data.frame(matrix(j-1, N, 1))
  X &lt;- rbind(X, Xtemp)
  y &lt;- rbind(y, ytemp)
}
Xtest &lt;- X[seq(1,(N*K),10),]
ytest &lt;- y[seq(1,(N*K),10),,drop=FALSE]
X &lt;- X[-seq(1,(N*K),10),]
y &lt;- y[-seq(1,(N*K),10),,drop=FALSE]

# Construct object for weighted linear SVM
mapXy &lt;- function(X, coeff) X%*%coeff
# Huber loss from DPpack
huber.h &lt;- 0.5
loss &lt;- generate.loss.huber(huber.h)
regularizer &lt;- 'l2' # Alternatively, function(coeff) coeff%*%coeff/2
eps &lt;- 1
gamma &lt;- 1
perturbation.method &lt;- 'output'
c &lt;- 1/(2*huber.h) # Required value for SVM
mapXy.gr &lt;- function(X, coeff) t(X)
loss.gr &lt;- generate.loss.gr.huber(huber.h)
regularizer.gr &lt;- function(coeff) coeff
wermdp &lt;- WeightedERMDP.CMS$new(mapXy, loss, regularizer, eps,
                                gamma, perturbation.method, c,
                                mapXy.gr, loss.gr,
                                regularizer.gr)

# Fit with data
# Bounds for X based on construction
upper.bounds &lt;- c( 1, 1)
lower.bounds &lt;- c(-1,-1)
weights &lt;- rep(1, nrow(y)) # Uniform weighting
weights[nrow(y)] &lt;- 0.5 # half weight for last observation
wub &lt;- 1 # Public upper bound for weights
wermdp$fit(X, y, upper.bounds, lower.bounds, weights=weights,
           weights.upper.bound=wub)
wermdp$coeff # Gets private coefficients

# Predict new data points
predicted.y &lt;- wermdp$predict(Xtest)
n.errors &lt;- sum(round(predicted.y)!=ytest)

</code></pre>


</div>