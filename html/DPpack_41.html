<div class="container">

<table style="width: 100%;"><tr>
<td>tune_linear_regression_model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Privacy-preserving Hyperparameter Tuning for Linear Regression Models</h2>

<h3>Description</h3>

<p>This function implements the privacy-preserving hyperparameter tuning
function for linear regression (Kifer et al. 2012) using the
exponential mechanism. It accepts a list of models with various chosen
hyperparameters, a dataset X with corresponding values y, upper and lower
bounds on the columns of X and the values of y, and a boolean indicating
whether to add bias in the construction of each of the models. The data are
split into m+1 equal groups, where m is the number of models being compared.
One group is set aside as the validation group, and each of the other m
groups are used to train each of the given m models. The negative of the sum
of the squared error for each model on the validation set is used as the
utility values in the exponential mechanism
(<code>ExponentialMechanism</code>) to select a tuned model in a
privacy-preserving way.
</p>


<h3>Usage</h3>

<pre><code class="language-R">tune_linear_regression_model(
  models,
  X,
  y,
  upper.bounds,
  lower.bounds,
  add.bias = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>models</code></td>
<td>
<p>Vector of linear regression model objects, each initialized
with a different combination of hyperparameter values from the search space
for tuning. Each model should be initialized with the same epsilon privacy
parameter value eps. The tuned model satisfies eps-level differential
privacy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Dataframe of data to be used in tuning the model. Note it is assumed
the data rows and corresponding labels are randomly shuffled.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Vector or matrix of true values for each row of X.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper.bounds</code></td>
<td>
<p>Numeric vector giving upper bounds on the values in each
column of X and the values in y. Should be length ncol(X)+1. The first
ncol(X) values are assumed to be in the same order as the corresponding
columns of X, while the last value in the vector is assumed to be the upper
bound on y. Any value in the columns of X and y larger than the
corresponding upper bound is clipped at the bound.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower.bounds</code></td>
<td>
<p>Numeric vector giving lower bounds on the values in each
column of X and the values in y. Should be length ncol(X)+1. The first
ncol(X) values are assumed to be in the same order as the corresponding
columns of X, while the last value in the vector is assumed to be the lower
bound on y. Any value in the columns of X and y smaller than the
corresponding lower bound is clipped at the bound.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>add.bias</code></td>
<td>
<p>Boolean indicating whether to add a bias term to X. Defaults
to FALSE.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Single model object selected from the input list models with tuned
parameters.
</p>


<h3>References</h3>

<p>Kifer D, Smith A, Thakurta A (2012).
“Private Convex Empirical Risk Minimization and High-dimensional Regression.”
In Mannor S, Srebro N, Williamson RC (eds.), <em>Proceedings of the 25th Annual Conference on Learning Theory</em>, volume 23 of <em>Proceedings of Machine Learning Research</em>, 25.1–25.40.
<a href="https://proceedings.mlr.press/v23/kifer12.html">https://proceedings.mlr.press/v23/kifer12.html</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Build example dataset
n &lt;- 500
X &lt;- data.frame(X=seq(-1,1,length.out = n))
true.theta &lt;- c(-.3,.5) # First element is bias term
p &lt;- length(true.theta)
y &lt;- true.theta[1] + as.matrix(X)%*%true.theta[2:p] + stats::rnorm(n=n,sd=.1)

# Grid of possible gamma values for tuning linear regression model
grid.search &lt;- c(100, 1, .0001)

# Construct objects for logistic regression parameter tuning
# Privacy budget should be the same for all models
eps &lt;- 1
delta &lt;- 0.01
linrdp1 &lt;- LinearRegressionDP$new("l2", eps, delta, grid.search[1])
linrdp2 &lt;- LinearRegressionDP$new("l2", eps, delta, grid.search[2])
linrdp3 &lt;- LinearRegressionDP$new("l2", eps, delta, grid.search[3])
models &lt;- c(linrdp1, linrdp2, linrdp3)

# Tune using data and bounds for X and y based on their construction
upper.bounds &lt;- c( 1, 2) # Bounds for X and y
lower.bounds &lt;- c(-1,-2) # Bounds for X and y
tuned.model &lt;- tune_linear_regression_model(models, X, y, upper.bounds,
                                            lower.bounds, add.bias=TRUE)
tuned.model$gamma # Gives resulting selected hyperparameter

# tuned.model result can be used the same as a trained LogisticRegressionDP model
tuned.model$coeff # Gives coefficients for tuned model

# Build a test dataset for prediction
Xtest &lt;- data.frame(X=c(-.5, -.25, .1, .4))
predicted.y &lt;- tuned.model$predict(Xtest, add.bias=TRUE)

</code></pre>


</div>