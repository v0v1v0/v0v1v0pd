<div class="container">

<table style="width: 100%;"><tr>
<td>performance</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Measure Model Performance</h2>

<h3>Description</h3>

<p><code>performance</code> measures difference between predictions and data
</p>


<h3>Usage</h3>

<pre><code class="language-R">performance(results, outcome, measure)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>results</code></td>
<td>
<p>Numeric vector with predictions</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outcome</code></td>
<td>
<p>Numeric vector same length as results with real data to compare to.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measure</code></td>
<td>
<p>Optional length one character vector that is either:
"accuracy", "sens", "spec", or "ppv". This specifies what measure of
predictive performance to use for training and evaluating the model. The
default measure is <code>"accuracy"</code>. However, accuracy can be a problematic
measure when the classes are imbalanced in the samples, i.e. if a class the
model is trying to predict is very rare. Alternatives to accuracy are
available that illuminate different aspects of predictive power. Sensitivity
answers the question, “ given that a result is truly an event, what is the
probability that the model will predict an event?” Specificity answers the
question, “given that a result is truly not an event, what is the
probability that the model will predict a negative?” Positive predictive
value answers, “what is the percent of predicted positives that are
actually positive?”</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This is the function of the <strong>datafsm</strong> package used to measure the fsm model performance. It uses the caret package.
</p>


<h3>Value</h3>

<p>Returns a numeric vector length one.
</p>


</div>