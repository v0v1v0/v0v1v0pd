<div class="container">

<table style="width: 100%;"><tr>
<td>dnnControl</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Auxiliary function for <code>dnnFit</code> dnnFit
</h2>

<h3>Description</h3>


<p>dnnControl is an auxiliary function for <code>dnnFit</code>. Typically only used internally by the dnn package, may be used to construct a control argument for the deep learning neural network model to specify parameters such as a loss function. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">  dnnControl(loss = c("mse", "cox", "bin", "log", "mae"), epochs = 300, 
	     batch_size = 64, verbose = 0, lr_rate = 0.0001,  
	     alpha = 0.5, lambda = 1.0, epsilon = 0.01, max.iter = 100, 
	     censor.group = NULL, weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>loss</code></td>
<td>
<p>loss function for the neural network model, "mse" for mean square error (guassian glm model), 
"mae" for mean absolute error, 
"cox" for the Cox partial likelihood (proportional hazards model), 
"bin" for cross-entropy (binomial glm model), 
"log" for log-linear (poisson glm model).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>number of deep learning epochs, default is 30.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p>batch size, default is 64. 'NaN' may be generated if batch size is too small and there is not event in a batch.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr_rate</code></td>
<td>
<p>learning rate, default is 0.0001.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>an optional vector of 'prior weights' to be used in the
fitting process. Should be NULL or a numeric vector, default is NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>alpha decay rate for momentum gradient descent, default is 0.5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>regularization term for dnn weighting parameters, 0.5*lambda*W*W), default is 1.0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>verbose = 1 for print out verbose during the model fit, 0 for not print.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>epsilon for convergence check, default is epsilon = 0.01.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max.iter</code></td>
<td>
<p>number of maximum iteration, default is max.iter = 100. This is used in the deepAFT function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>censor.group</code></td>
<td>
<p>a vector for censoring groups. A KM curve for censoring will be fit for each group. If a matrix is provided, then a Cox model will be used to predict the censoring probability. Used only in the deepAFT function.</p>
</td>
</tr>
</table>
<h3>Details</h3>


<p>dnnControl is used in model fitting of "dnnFit". Additional loss functions will be added to the library in the future. 
</p>


<h3>Value</h3>


<p>This function checks the internal consistency and returns a list of values as input to control model fitting of "dnnFit".
</p>




<h3>Note</h3>

<p>For right censored survival time only</p>


<h3>Author(s)</h3>

<p>Chen, B. E.
</p>


<h3>References</h3>


<p>Norman, P. and Chen, B. E. (2023). DeepAFAT: A nonparametric accelerated failure time model with artificial neural network. Manuscript to be submitted. 
</p>


<h3>See Also</h3>


<p><code>deepAFT</code>, <code>deepGLM</code>, <code>deepSurv</code>, <code>dnnFit</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Example for dnnControl
##
# model = dNNmodel()

  control = dnnControl(loss='mse')
  
# can also be used in   
# fit = dnnFit(y ~ x, model, control) 
# print(fit)
</code></pre>


</div>