<div class="container">

<table style="width: 100%;"><tr>
<td>EmpiricalRiskMinimizationDP.KST</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Privacy-preserving Empirical Risk Minimization for Regression</h2>

<h3>Description</h3>

<p>This class implements differentially private empirical risk
minimization using the objective perturbation technique
(Kifer et al. 2012). It is intended to be a framework for
building more specific models via inheritance. See
<code>LinearRegressionDP</code> for an example of this type of structure.
</p>


<h3>Details</h3>

<p>To use this class for empirical risk minimization, first use the
<code>new</code> method to construct an object of this class with the desired
function values and hyperparameters. After constructing the object, the
<code>fit</code> method can be applied with a provided dataset and data bounds to
fit the model. In fitting, the model stores a vector of coefficients
<code>coeff</code> which satisfy differential privacy. These can be released
directly, or used in conjunction with the <code>predict</code> method to privately
predict the outcomes of new datapoints.
</p>
<p>Note that in order to guarantee differential privacy for the empirical risk
minimization model, certain constraints must be satisfied for the values
used to construct the object, as well as for the data used to fit.
Specifically, the following constraints must be met. Let <code class="reqn">l</code> represent
the loss function for an individual dataset row x and output value y and
<code class="reqn">L</code> represent the average loss over all rows and output values. First,
<code class="reqn">L</code> must be convex with a continuous Hessian. Second, the l2-norm of the
gradient of <code class="reqn">l</code> must be bounded above by some constant zeta for all
possible input values in the domain. Third, for all possible inputs to
<code class="reqn">l</code>, the Hessian of <code class="reqn">l</code> must be of rank at most one and its
Eigenvalues must be bounded above by some constant lambda. Fourth, the
regularizer must be convex. Finally, the provided domain of <code class="reqn">l</code> must be
a closed convex subset of the set of all real-valued vectors of dimension p,
where p is the number of columns of X. Note that because of this, a bias
term cannot be included without appropriate scaling/preprocessing of the
dataset. To ensure privacy, the add.bias argument in the <code>fit</code> and
<code>predict</code> methods should only be utilized in subclasses within this
package where appropriate preprocessing is implemented, not in this class.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>mapXy</code></dt>
<dd>
<p>Map function of the form <code>mapXy(X, coeff)</code> mapping input
data matrix <code>X</code> and coefficient vector or matrix <code>coeff</code> to
output labels <code>y</code>.</p>
</dd>
<dt><code>mapXy.gr</code></dt>
<dd>
<p>Function representing the gradient of the map function with
respect to the values in <code>coeff</code> and of the form <code>mapXy.gr(X,
  coeff)</code>, where <code>X</code> is a matrix and <code>coeff</code> is a matrix or
numeric vector.</p>
</dd>
<dt><code>loss</code></dt>
<dd>
<p>Loss function of the form <code>loss(y.hat, y)</code>, where
<code>y.hat</code> and <code>y</code> are matrices.</p>
</dd>
<dt><code>loss.gr</code></dt>
<dd>
<p>Function representing the gradient of the loss function with
respect to <code>y.hat</code> and of the form <code>loss.gr(y.hat, y)</code>, where
<code>y.hat</code> and <code>y</code> are matrices.</p>
</dd>
<dt><code>regularizer</code></dt>
<dd>
<p>Regularization function of the form
<code>regularizer(coeff)</code>, where <code>coeff</code> is a vector or matrix.</p>
</dd>
<dt><code>regularizer.gr</code></dt>
<dd>
<p>Function representing the gradient of the
regularization function with respect to <code>coeff</code> and of the form
<code>regularizer.gr(coeff)</code>.</p>
</dd>
<dt><code>gamma</code></dt>
<dd>
<p>Nonnegative real number representing the regularization
constant.</p>
</dd>
<dt><code>eps</code></dt>
<dd>
<p>Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.</p>
</dd>
<dt><code>delta</code></dt>
<dd>
<p>Nonnegative real number defining the delta privacy parameter.
If 0, reduces to pure eps-DP.</p>
</dd>
<dt><code>domain</code></dt>
<dd>
<p>List of constraint and jacobian functions representing the
constraints on the search space for the objective perturbation algorithm
used in Kifer et al. (2012).</p>
</dd>
<dt><code>zeta</code></dt>
<dd>
<p>Positive real number denoting the upper bound on the l2-norm
value of the gradient of the loss function, as required to ensure
differential privacy.</p>
</dd>
<dt><code>lambda</code></dt>
<dd>
<p>Positive real number corresponding to the upper bound of the
Eigenvalues of the Hessian of the loss function for all possible inputs.</p>
</dd>
<dt><code>coeff</code></dt>
<dd>
<p>Numeric vector of coefficients for the model.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-EmpiricalRiskMinimizationDP.KST-new"><code>EmpiricalRiskMinimizationDP.KST$new()</code></a>
</p>
</li>
<li> <p><a href="#method-EmpiricalRiskMinimizationDP.KST-fit"><code>EmpiricalRiskMinimizationDP.KST$fit()</code></a>
</p>
</li>
<li> <p><a href="#method-EmpiricalRiskMinimizationDP.KST-predict"><code>EmpiricalRiskMinimizationDP.KST$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-EmpiricalRiskMinimizationDP.KST-clone"><code>EmpiricalRiskMinimizationDP.KST$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-EmpiricalRiskMinimizationDP.KST-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new <code>EmpiricalRiskMinimizationDP.KST</code> object.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmpiricalRiskMinimizationDP.KST$new(
  mapXy,
  loss,
  regularizer,
  eps,
  delta,
  domain,
  zeta,
  lambda,
  gamma,
  mapXy.gr = NULL,
  loss.gr = NULL,
  regularizer.gr = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>mapXy</code></dt>
<dd>
<p>Map function of the form <code>mapXy(X, coeff)</code> mapping input
data matrix <code>X</code> and coefficient vector or matrix <code>coeff</code> to
output labels <code>y</code>. Should return a column matrix of predicted labels
for each row of <code>X</code>. See <code>mapXy.linear</code> for an example.</p>
</dd>
<dt><code>loss</code></dt>
<dd>
<p>Loss function of the form <code>loss(y.hat, y)</code>, where
<code>y.hat</code> and <code>y</code> are matrices. Should be defined such that it
returns a matrix of loss values for each element of <code>y.hat</code> and
<code>y</code>. See <code>loss.squared.error</code> for an example. This
function must be convex and the l2-norm of its gradient must be bounded
above by zeta for some constant zeta for all possible inputs within the
given domain. Additionally, for all possible inputs within the given
domain, the Hessian of the loss function must be of rank at most one and
its Eigenvalues must be bounded above by some constant lambda.</p>
</dd>
<dt><code>regularizer</code></dt>
<dd>
<p>String or regularization function. If a string, must be
'l2', indicating to use l2 regularization. If a function, must have form
<code>regularizer(coeff)</code>, where <code>coeff</code> is a vector or matrix, and
return the value of the regularizer at <code>coeff</code>. See
<code>regularizer.l2</code> for an example. Additionally, in order to
ensure differential privacy, the function must be convex.</p>
</dd>
<dt><code>eps</code></dt>
<dd>
<p>Positive real number defining the epsilon privacy budget. If set
to Inf, runs algorithm without differential privacy.</p>
</dd>
<dt><code>delta</code></dt>
<dd>
<p>Nonnegative real number defining the delta privacy parameter.
If 0, reduces to pure eps-DP.</p>
</dd>
<dt><code>domain</code></dt>
<dd>
<p>List of functions representing the constraints on the search
space for the objective perturbation algorithm. Must contain two
functions, labeled "constraints" and "jacobian", respectively. The
"constraints" function accepts a vector of coefficients from the search
space and returns a value such that the value is nonpositive if and only
if the input coefficient vector is within the constrained search space.
The "jacobian" function also accepts a vector of coefficients and returns
the Jacobian of the constraint function. For example, in linear
regression, the square of the l2-norm of the coefficient vector
<code class="reqn">\theta</code> is assumed to be bounded above by p, where p is the length
of <code class="reqn">\theta</code> (Kifer et al. 2012). So, domain could be
defined as <code>domain &lt;- list("constraints"=function(coeff) coeff%*%coeff-length(coeff), "jacobian"=function(coeff) 2*coeff)</code>.</p>
</dd>
<dt><code>zeta</code></dt>
<dd>
<p>Positive real number denoting the upper bound on the l2-norm
value of the gradient of the loss function, as required to ensure
differential privacy.</p>
</dd>
<dt><code>lambda</code></dt>
<dd>
<p>Positive real number corresponding to the upper bound of the
Eigenvalues of the Hessian of the loss function for all possible inputs.</p>
</dd>
<dt><code>gamma</code></dt>
<dd>
<p>Nonnegative real number representing the regularization
constant.</p>
</dd>
<dt><code>mapXy.gr</code></dt>
<dd>
<p>Optional function representing the gradient of the map
function with respect to the values in <code>coeff</code>. If given, must be of
the form <code>mapXy.gr(X, coeff)</code>, where <code>X</code> is a matrix and
<code>coeff</code> is a matrix or numeric vector. Should be defined such that
the ith row of the output represents the gradient with respect to the ith
coefficient. See <code>mapXy.gr.linear</code> for an example. If not
given, non-gradient based optimization methods are used to compute the
coefficient values in fitting the model.</p>
</dd>
<dt><code>loss.gr</code></dt>
<dd>
<p>Optional function representing the gradient of the loss
function with respect to <code>y.hat</code> and of the form
<code>loss.gr(y.hat, y)</code>, where <code>y.hat</code> and <code>y</code> are matrices.
Should be defined such that the ith row of the output represents the
gradient of the loss function at the ith set of input values. See
<code>loss.gr.squared.error</code> for an example. If not given,
non-gradient based optimization methods are used to compute the
coefficient values in fitting the model.</p>
</dd>
<dt><code>regularizer.gr</code></dt>
<dd>
<p>Optional function representing the gradient of the
regularization function with respect to <code>coeff</code> and of the form
<code>regularizer.gr(coeff)</code>. Should return a vector. See
<code>regularizer.gr.l2</code> for an example. If <code>regularizer</code> is
given as a string, this value is ignored. If not given and
<code>regularizer</code> is a function, non-gradient based optimization methods
are used to compute the coefficient values in fitting the model.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>A new EmpiricalRiskMinimizationDP.KST object.
</p>


<hr>
<a id="method-EmpiricalRiskMinimizationDP.KST-fit"></a>



<h4>Method <code>fit()</code>
</h4>

<p>Fit the differentially private emprirical risk minimization
model. The function runs the objective perturbation algorithm
(Kifer et al. 2012) to generate an objective function. A
numerical optimization method is then run to find optimal coefficients
for fitting the model given the training data and hyperparameters. The
<code>nloptr</code> function is used. If mapXy.gr, loss.gr, and
regularizer.gr are all given in the construction of the object, the
gradient of the objective function and the Jacobian of the constraint
function are utilized for the algorithm, and the NLOPT_LD_MMA method is
used. If one or more of these gradient functions are not given, the
NLOPT_LN_COBYLA method is used. The resulting privacy-preserving
coefficients are stored in coeff.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmpiricalRiskMinimizationDP.KST$fit(
  X,
  y,
  upper.bounds,
  lower.bounds,
  add.bias = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt>
<dd>
<p>Dataframe of data to be fit.</p>
</dd>
<dt><code>y</code></dt>
<dd>
<p>Vector or matrix of true values for each row of <code>X</code>.</p>
</dd>
<dt><code>upper.bounds</code></dt>
<dd>
<p>Numeric vector of length <code>ncol(X)+1</code> giving upper
bounds on the values in each column of <code>X</code> and the values of
<code>y</code>. The last value in the vector is assumed to be the upper bound
on <code>y</code>, while the first <code>ncol(X)</code> values are assumed to be in
the same order as the corresponding columns of <code>X</code>. Any value in the
columns of <code>X</code> and in <code>y</code> larger than the corresponding upper
bound is clipped at the bound.</p>
</dd>
<dt><code>lower.bounds</code></dt>
<dd>
<p>Numeric vector of length <code>ncol(X)+1</code> giving lower
bounds on the values in each column of <code>X</code> and the values of
<code>y</code>. The last value in the vector is assumed to be the lower bound
on <code>y</code>, while the first <code>ncol(X)</code> values are assumed to be in
the same order as the corresponding columns of <code>X</code>. Any value in the
columns of <code>X</code> and in <code>y</code> larger than the corresponding lower
bound is clipped at the bound.</p>
</dd>
<dt><code>add.bias</code></dt>
<dd>
<p>Boolean indicating whether to add a bias term to <code>X</code>.
Defaults to FALSE.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-EmpiricalRiskMinimizationDP.KST-predict"></a>



<h4>Method <code>predict()</code>
</h4>

<p>Predict y values for given X using the fitted coefficients.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmpiricalRiskMinimizationDP.KST$predict(X, add.bias = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>X</code></dt>
<dd>
<p>Dataframe of data on which to make predictions. Must be of same
form as <code>X</code> used to fit coefficients.</p>
</dd>
<dt><code>add.bias</code></dt>
<dd>
<p>Boolean indicating whether to add a bias term to <code>X</code>.
Defaults to FALSE. If add.bias was set to TRUE when fitting the
coefficients, add.bias should be set to TRUE for predictions.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Matrix of predicted y values corresponding to each row of X.
</p>


<hr>
<a id="method-EmpiricalRiskMinimizationDP.KST-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmpiricalRiskMinimizationDP.KST$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>References</h3>

<p>Kifer D, Smith A, Thakurta A (2012).
“Private Convex Empirical Risk Minimization and High-dimensional Regression.”
In Mannor S, Srebro N, Williamson RC (eds.), <em>Proceedings of the 25th Annual Conference on Learning Theory</em>, volume 23 of <em>Proceedings of Machine Learning Research</em>, 25.1–25.40.
<a href="https://proceedings.mlr.press/v23/kifer12.html">https://proceedings.mlr.press/v23/kifer12.html</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Build example dataset
n &lt;- 500
X &lt;- data.frame(X=seq(-1,1,length.out = n))
true.theta &lt;- c(-.3,.5) # First element is bias term
p &lt;- length(true.theta)
y &lt;- true.theta[1] + as.matrix(X)%*%true.theta[2:p] + stats::rnorm(n=n,sd=.1)

# Construct object for linear regression
mapXy &lt;- function(X, coeff) X%*%coeff
loss &lt;- function(y.hat, y) (y.hat-y)^2/2
regularizer &lt;- 'l2' # Alternatively, function(coeff) coeff%*%coeff/2
eps &lt;- 1
delta &lt;- 1
domain &lt;- list("constraints"=function(coeff) coeff%*%coeff-length(coeff),
  "jacobian"=function(coeff) 2*coeff)
# Set p to be the number of predictors desired including intercept term (length of coeff)
zeta &lt;- 2*p^(3/2) # Proper bound for linear regression
lambda &lt;- p # Proper bound for linear regression
gamma &lt;- 1
mapXy.gr &lt;- function(X, coeff) t(X)
loss.gr &lt;- function(y.hat, y) y.hat-y
regularizer.gr &lt;- function(coeff) coeff

ermdp &lt;- EmpiricalRiskMinimizationDP.KST$new(mapXy, loss, 'l2', eps, delta,
                                             domain, zeta, lambda,
                                             gamma, mapXy.gr, loss.gr,
                                             regularizer.gr)

# Fit with data
# We must assume y is a matrix with values between -p and p (-2 and 2
#   for this example)
upper.bounds &lt;- c(1, 2) # Bounds for X and y
lower.bounds &lt;- c(-1,-2) # Bounds for X and y
ermdp$fit(X, y, upper.bounds, lower.bounds, add.bias=TRUE)
ermdp$coeff # Gets private coefficients

# Predict new data points
# Build a test dataset
Xtest &lt;- data.frame(X=c(-.5, -.25, .1, .4))
predicted.y &lt;- ermdp$predict(Xtest, add.bias=TRUE)

</code></pre>


</div>